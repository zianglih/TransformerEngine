+ : /root/TransformerEngine
+ : /data/home/ziangli/te_pr_logs_v210/
+ mkdir -p /data/home/ziangli/te_pr_logs_v210/
+ pip3 install pytest==8.2.1
Requirement already satisfied: pytest==8.2.1 in /usr/local/lib/python3.12/dist-packages (8.2.1)
Requirement already satisfied: iniconfig in /usr/local/lib/python3.12/dist-packages (from pytest==8.2.1) (2.3.0)
Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pytest==8.2.1) (25.0)
Requirement already satisfied: pluggy<2.0,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest==8.2.1) (1.6.0)
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_sanity.xml /root/TransformerEngine/tests/pytorch/test_sanity.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 16773 items

tests/pytorch/test_sanity.py ssssss..................ssssss............. [  0%]
.....ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [  0%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [  1%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [  1%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [  1%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [  2%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [  2%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [  3%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [  3%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [  4%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [  4%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [  4%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [  5%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [  5%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [  6%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [  6%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [  7%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [  7%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [  7%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [  8%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [  8%]
ss............sssssssss.s..s..s.sssssssss.........sssssssss.........ssss [  9%]
sssss.........sssssssss...........................sssssssss.s..s..s.ssss [  9%]
sssss.........sssssssss.........sssssssss.........sssssssss............. [ 10%]
..............sssssssss.s..s..s.sssssssss.........sssssssss.........ssss [ 10%]
sssss.........sssssssss...........................sssssssss.s..s..s.ssss [ 10%]
sssss.........sssssssss.........sssssssss.........sssssssss............. [ 11%]
..............sssssssssssssssssssssssssss.........sssssssss.........ssss [ 11%]
sssss.........sssssssss...........................ssssssssssssssssssssss [ 12%]
sssss.........sssssssss.........sssssssss.........sssssssss............. [ 12%]
..............sssssssssssssssssssssssssss.........sssssssss.........ssss [ 13%]
sssss.........sssssssss...........................ssssssssssssssssssssss [ 13%]
sssss.........sssssssss.........sssssssss.........sssssssss............. [ 13%]
..............sssssssssssssssssssssssssss.........sssssssss.........ssss [ 14%]
sssss.........sssssssss...........................ssssssssssssssssssssss [ 14%]
sssss.........sssssssss.........sssssssss.........sssssssss............. [ 15%]
..............sssssssssssssssssssssssssss.........sssssssss.........ssss [ 15%]
sssss.........sssssssss...........................ssssssssssssssssssssss [ 16%]
sssss.........sssssssss.........sssssssss.........sssssssss............. [ 16%]
..............sssssssssssssssssssssssssss.........sssssssss.........ssss [ 16%]
sssss.........sssssssss...........................ssssssssssssssssssssss [ 17%]
sssss.........sssssssss.........sssssssss.........sssssssss............. [ 17%]
..............sssssssssssssssssssssssssss.........sssssssss.........ssss [ 18%]
sssss.........sssssssss...........................ssssssssssssssssssssss [ 18%]
sssss.........sssssssss.........sssssssss.........sssssssss............. [ 19%]
.....ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 19%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 20%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 20%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 20%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 21%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 21%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 22%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 22%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 23%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 23%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 23%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 24%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 24%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 25%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 25%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 26%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 26%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 26%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 27%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 27%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 28%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 28%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 29%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 29%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 29%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 30%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 30%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 31%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 31%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 32%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 32%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 32%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 33%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 33%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 34%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 34%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 35%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 35%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 35%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 36%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 36%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 37%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 37%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 38%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 38%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 38%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 39%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 39%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 40%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 40%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 41%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 41%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 41%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 42%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 42%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 43%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 43%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 44%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 44%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 44%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 45%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 45%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 46%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 46%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 47%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 47%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 47%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 48%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 48%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 49%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 49%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 50%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 50%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 50%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 51%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 51%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 52%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 52%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 53%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 53%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 53%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 54%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 54%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 55%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 55%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 56%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 56%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 56%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 57%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 57%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 58%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 58%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 59%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 59%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 59%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 60%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 60%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 61%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 61%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 62%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 62%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 62%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 63%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 63%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 64%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 64%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 65%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 65%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 65%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 66%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 66%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 67%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 67%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 68%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 68%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 68%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 69%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 69%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 70%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 70%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 71%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 71%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 71%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 72%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 72%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 73%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 73%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 74%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 74%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 74%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 75%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 75%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 76%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 76%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 77%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 77%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 77%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 78%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 78%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 79%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 79%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 80%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 80%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 80%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 81%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 81%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 82%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 82%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 83%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 83%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 83%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 84%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 84%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 85%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 85%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 86%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 86%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 86%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 87%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 87%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 88%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 88%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 89%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 89%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 89%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 90%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 90%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 91%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 91%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 92%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 92%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 92%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 93%]
ss...ssssssssssssssssssssssssssssssssssss....s.............sssssssssssss [ 93%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 94%]
ss.......s.............sssssssssssssss.......s.............sssssssssssss [ 94%]
ss.......s.................s.................s.................s........ [ 95%]
.........s.................s.................s.................s........ [ 95%]
.........s.................s.................s.................s........ [ 95%]
.........s.................s.................s.................s........ [ 96%]
.........s.................s.................s.................s........ [ 96%]
.........s.................s.................s.................s........ [ 97%]
.........s.................s.................s.................s........ [ 97%]
.........s.................s.................s.................s........ [ 98%]
..........s.................s.................s.................s....... [ 98%]
...........s.................s.................s.................s...... [ 98%]
............s.................s.................s.................s..... [ 99%]
............s.................s.................s....................... [ 99%]
..........................                                               [100%]

=============================== warnings summary ===============================
tests/pytorch/test_sanity.py: 69 warnings
  /root/TransformerEngine/transformer_engine/pytorch/attention/dot_product_attention/utils.py:2070: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask
    warnings.warn(

tests/pytorch/test_sanity.py::test_inference_mode[fp8_delayed_scaling-ops.Linear]
tests/pytorch/test_sanity.py::test_inference_mode[fp8_current_scaling-ops.Linear]
tests/pytorch/test_sanity.py::test_inference_mode[mxfp8-ops.Linear]
  /root/TransformerEngine/transformer_engine/pytorch/quantized_tensor.py:120: UserWarning: Quantizer is being updated, this may affect model behavior
    warnings.warn("Quantizer is being updated, this may affect model behavior")

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_sanity.xml -
=============== 7642 passed, 9131 skipped, 72 warnings in 44.10s ===============
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_recipe.xml /root/TransformerEngine/tests/pytorch/test_recipe.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 58 items

tests/pytorch/test_recipe.py ........................................... [ 74%]
...............                                                          [100%]

=============================== warnings summary ===============================
tests/pytorch/test_recipe.py::TestFP8Recipe::test_check_for_weight_tensor_and_recipe_correspondence[model_init_recipe0]
  /root/TransformerEngine/transformer_engine/pytorch/module/base.py:1019: UserWarning: Recipe type changed from MXFP8BlockScaling to DelayedScaling. This may affect model behavior.
    warnings.warn(

tests/pytorch/test_recipe.py::TestFP8Recipe::test_check_for_weight_tensor_and_recipe_correspondence[model_init_recipe1]
  /root/TransformerEngine/transformer_engine/pytorch/module/base.py:1019: UserWarning: Recipe type changed from Float8BlockScaling to DelayedScaling. This may affect model behavior.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_recipe.xml -
======================== 58 passed, 2 warnings in 9.63s ========================
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_deferred_init.xml /root/TransformerEngine/tests/pytorch/test_deferred_init.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 12 items

tests/pytorch/test_deferred_init.py ............                         [100%]

- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_deferred_init.xml -
============================== 12 passed in 6.37s ==============================
+ PYTORCH_JIT=0
+ NVTE_TORCH_COMPILE=0
+ NVTE_ALLOW_NONDETERMINISTIC_ALGO=0
+ NVTE_FUSED_ATTN=0
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_numerics.xml /root/TransformerEngine/tests/pytorch/test_numerics.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 2734 items

tests/pytorch/test_numerics.py ......................................... [  1%]
.......ss.ss.......................................................ss.ss [  4%]
.......................................................ss.ss............ [  6%]
...........................................ss.ss........................ [  9%]
...............................ss.ss.................................... [ 12%]
...................ss.ss..............FF.FF.FF.FF.FF.FF.FF.FF......F..F. [ 14%]
.F..F..F..F..F..F...................................sssss............... [ 17%]
........................................................................ [ 19%]
.............F..F..F..F..F..F..F..F..F..F..F..F..F..F..F..F..F..F..F..F. [ 22%]
.F..F..F..F..F..F..F..F..F..F..F..F..................................... [ 25%]
........................................................................ [ 27%]
.............F..F..F..F................................................. [ 30%]
.F..F..F..F..................................................F..F..F..F. [ 33%]
.................................................F..F..F..F............. [ 35%]
.....................................F..F..F..F......................... [ 38%]
.........................F..F..F..F..................................... [ 41%]
.............F..F..F..F................................................. [ 43%]
.F..F..F..F............................................................. [ 46%]
.........................ss.ss.ss.ss.................................... [ 48%]
.........................ss.ss.ss.ss.................................... [ 51%]
.........................ss.ss.ss.ss.................................... [ 54%]
.........................ss.ss.ss.ss.................................... [ 56%]
.........................ss.ss.ss.ss.................................... [ 59%]
.........................ss.ss.ss.ss.................................... [ 62%]
.........................ss.ss.ss.ss.................................... [ 64%]
.........................ss.ss.ss.ss.................................... [ 67%]
.........................ss.ss.ss.ss.................................... [ 69%]
.........................ss.ss.ss.ss.................................... [ 72%]
.........................ss.ss.ss.ss.................................... [ 75%]
.........................ss.ss.ss.ss.................................... [ 77%]
.........................ss.ss.ss.ss.................................... [ 80%]
.........................ss.ss.ss.ss.................................... [ 83%]
.........................ss.ss.ss.ss.................................... [ 85%]
.........................ss.ss.ss.ss.............sssssssssssssssssssssss [ 88%]
sssssssssssssssssssssssss.........sssss........s........................ [ 91%]
.........................ss.ss.ss.ss.................................... [ 93%]
.............ss.ss.ss.ss..........sssss...............................ss [ 96%]
.ss..................................................................... [ 98%]
.............................                                            [100%]

=================================== FAILURES ===================================
____________________ test_gpt_accuracy[True-small-1-dtype1] ____________________

dtype = torch.float16, bs = 1, model = 'small', parallel_attention_mlp = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("parallel_attention_mlp", all_boolean)
    def test_gpt_accuracy(dtype, bs, model, parallel_attention_mlp):
        config = model_configs[model]
    
        te_gpt = TransformerLayer(
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            num_attention_heads=config.num_heads,
            layernorm_epsilon=config.eps,
            attention_dropout=0.1,
            hidden_dropout=0.1,
            params_dtype=dtype,
            fuse_qkv_params=True,
            qkv_weight_interleaved=False,
            parallel_attention_mlp=parallel_attention_mlp,
            device="cuda",
        ).eval()
    
        torch_gpt = (
            TorchGPT(
                config.hidden_size,
                config.eps,
                config.num_heads,
                parallel_attention_mlp=parallel_attention_mlp,
            )
            .to(dtype=dtype)
            .cuda()
            .eval()
        )
    
        # Share params
        with torch.no_grad():
            torch_gpt.ln.weight = Parameter(
                te_gpt.self_attention.layernorm_qkv.layer_norm_weight.clone()
            )
            torch_gpt.ln.bias = Parameter(te_gpt.self_attention.layernorm_qkv.layer_norm_bias.clone())
            torch_gpt.causal_attn.mhsa.in_proj_weight = Parameter(
                te_gpt.self_attention.layernorm_qkv.weight.clone()
            )
            torch_gpt.causal_attn.mhsa.in_proj_bias = Parameter(
                te_gpt.self_attention.layernorm_qkv.bias.clone()
            )
            torch_gpt.causal_attn.mhsa.out_proj.weight = Parameter(
                te_gpt.self_attention.proj.weight.clone()
            )
            torch_gpt.causal_attn.mhsa.out_proj.bias = Parameter(
                te_gpt.self_attention.proj.bias.clone()
            )
            torch_gpt.ln_mlp.ln.weight = Parameter(te_gpt.layernorm_mlp.layer_norm_weight.clone())
            torch_gpt.ln_mlp.ln.bias = Parameter(te_gpt.layernorm_mlp.layer_norm_bias.clone())
            torch_gpt.ln_mlp.fc1.weight = Parameter(te_gpt.layernorm_mlp.fc1_weight.clone())
            torch_gpt.ln_mlp.fc1.bias = Parameter(te_gpt.layernorm_mlp.fc1_bias.clone())
            torch_gpt.ln_mlp.fc2.weight = Parameter(te_gpt.layernorm_mlp.fc2_weight.clone())
            torch_gpt.ln_mlp.fc2.bias = Parameter(te_gpt.layernorm_mlp.fc2_bias.clone())
    
        te_outputs = _test_e2e_gpt_accuracy(te_gpt, bs, dtype, config)
>       torch_outputs = _test_e2e_gpt_accuracy(torch_gpt, bs, dtype, config)

tests/pytorch/test_numerics.py:987: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/test_numerics.py:916: in _test_e2e_gpt_accuracy
    out = block(inp_hidden_states, attention_mask=inp_attn_mask)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:551: in forward
    b = self.causal_attn(a, attention_mask)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:426: in forward
    output = self.mhsa(x, x, x, attn_mask=attention_mask, need_weights=False)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1488: in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = tensor([[[-1.6729,  0.5493, -0.5479,  ..., -1.4033,  2.7266, -0.1997]],

        [[-0.2756,  1.3506,  0.1406,  ..., -1...3,  ..., -1.1631, -0.6611, -1.6035]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)
key = tensor([[[-1.6729,  0.5493, -0.5479,  ..., -1.4033,  2.7266, -0.1997]],

        [[-0.2756,  1.3506,  0.1406,  ..., -1...3,  ..., -1.1631, -0.6611, -1.6035]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)
value = tensor([[[-1.6729,  0.5493, -0.5479,  ..., -1.4033,  2.7266, -0.1997]],

        [[-0.2756,  1.3506,  0.1406,  ..., -1...3,  ..., -1.1631, -0.6611, -1.6035]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)
embed_dim_to_check = 128, num_heads = 8
in_proj_weight = Parameter containing:
tensor([[ 1.4565e-02,  3.7628e-02, -7.9575e-03,  ..., -6.2141e-03,
          7.5221e-05,  1.5306......, -3.4058e-02,
         -1.2466e-02,  3.1300e-03]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
in_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
bias_k = None, bias_v = None, add_zero_attn = False, dropout_p = 0.0
out_proj_weight = Parameter containing:
tensor([[-0.0037,  0.0416, -0.0242,  ...,  0.0051, -0.0132, -0.0170],
        [ 0.0056,  0.0353,..., -0.0457,  0.0080,  ..., -0.0051, -0.0340,  0.0095]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
out_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ..., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
training = False, key_padding_mask = None, need_weights = False
attn_mask = tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],
          [0., 0., -inf,  ..., -inf, -inf, -inf],
          [0., 0... 0., 0.,  ..., 0., 0., -inf],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
use_separate_proj_weight = False, q_proj_weight = None, k_proj_weight = None
v_proj_weight = None, static_k = None, static_v = None
average_attn_weights = True, is_causal = False

    def multi_head_attention_forward(
        query: Tensor,
        key: Tensor,
        value: Tensor,
        embed_dim_to_check: int,
        num_heads: int,
        in_proj_weight: Optional[Tensor],
        in_proj_bias: Optional[Tensor],
        bias_k: Optional[Tensor],
        bias_v: Optional[Tensor],
        add_zero_attn: bool,
        dropout_p: float,
        out_proj_weight: Tensor,
        out_proj_bias: Optional[Tensor],
        training: bool = True,
        key_padding_mask: Optional[Tensor] = None,
        need_weights: bool = True,
        attn_mask: Optional[Tensor] = None,
        use_separate_proj_weight: bool = False,
        q_proj_weight: Optional[Tensor] = None,
        k_proj_weight: Optional[Tensor] = None,
        v_proj_weight: Optional[Tensor] = None,
        static_k: Optional[Tensor] = None,
        static_v: Optional[Tensor] = None,
        average_attn_weights: bool = True,
        is_causal: bool = False,
    ) -> tuple[Tensor, Optional[Tensor]]:
        r"""Forward method for MultiHeadAttention.
    
        See :class:`torch.nn.MultiheadAttention` for details.
    
        Args:
            query, key, value: map a query and a set of key-value pairs to an output.
                See "Attention Is All You Need" for more details.
            embed_dim_to_check: total dimension of the model.
            num_heads: parallel attention heads.
            in_proj_weight, in_proj_bias: input projection weight and bias.
            bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
            add_zero_attn: add a new batch of zeros to the key and
                           value sequences at dim=1.
            dropout_p: probability of an element to be zeroed.
            out_proj_weight, out_proj_bias: the output projection weight and bias.
            training: apply dropout if is ``True``.
            key_padding_mask: if provided, specified padding elements in the key will
                be ignored by the attention. This is an binary mask. When the value is True,
                the corresponding value on the attention layer will be filled with -inf.
            need_weights: output attn_output_weights.
                Default: `True`
                Note: `needs_weight` defaults to `True`, but should be set to `False`
                For best performance when attention weights are not needed.
                *Setting needs_weights to `True`
                leads to a significant performance degradation.*
            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
                the batches while a 3D mask allows to specify a different mask for the entries of each batch.
            is_causal: If specified, applies a causal mask as attention mask, and ignores
                attn_mask for computing scaled dot product attention.
                Default: ``False``.
                .. warning::
                    is_causal is provides a hint that the attn_mask is the
                    causal mask.Providing incorrect hints can result in
                    incorrect execution, including forward and backward
                    compatibility.
            use_separate_proj_weight: the function accept the proj. weights for query, key,
                and value in different forms. If false, in_proj_weight will be used, which is
                a combination of q_proj_weight, k_proj_weight, v_proj_weight.
            q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
            static_k, static_v: static key and value used for attention operators.
            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.
                Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect
                when ``need_weights=True.``. Default: True
    
    
        Shape:
            Inputs:
            - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
              the embedding dimension.
            - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.
              If a FloatTensor is provided, it will be directly added to the value.
              If a BoolTensor is provided, the positions with the
              value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
            - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
              3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
              S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked
              positions. If a BoolTensor is provided, positions with ``True``
              are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
              is provided, it will be added to the attention weight.
            - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
            - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
    
            Outputs:
            - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
              E is the embedding dimension.
            - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns
              attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or
              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and
              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per
              head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.
        """
        tens_ops = (
            query,
            key,
            value,
            in_proj_weight,
            in_proj_bias,
            bias_k,
            bias_v,
            out_proj_weight,
            out_proj_bias,
        )
        if has_torch_function(tens_ops):
            return handle_torch_function(
                multi_head_attention_forward,
                tens_ops,
                query,
                key,
                value,
                embed_dim_to_check,
                num_heads,
                in_proj_weight,
                in_proj_bias,
                bias_k,
                bias_v,
                add_zero_attn,
                dropout_p,
                out_proj_weight,
                out_proj_bias,
                training=training,
                key_padding_mask=key_padding_mask,
                need_weights=need_weights,
                attn_mask=attn_mask,
                is_causal=is_causal,
                use_separate_proj_weight=use_separate_proj_weight,
                q_proj_weight=q_proj_weight,
                k_proj_weight=k_proj_weight,
                v_proj_weight=v_proj_weight,
                static_k=static_k,
                static_v=static_v,
                average_attn_weights=average_attn_weights,
            )
    
        is_batched = _mha_shape_check(
            query, key, value, key_padding_mask, attn_mask, num_heads
        )
    
        # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input
        # is batched, run the computation and before returning squeeze the
        # batch dimension so that the output doesn't carry this temporary batch dimension.
        if not is_batched:
            # unsqueeze if the input is unbatched
            query = query.unsqueeze(1)
            key = key.unsqueeze(1)
            value = value.unsqueeze(1)
            if key_padding_mask is not None:
                key_padding_mask = key_padding_mask.unsqueeze(0)
    
        # set up shape vars
        tgt_len, bsz, embed_dim = query.shape
        src_len, _, _ = key.shape
    
        key_padding_mask = _canonical_mask(
            mask=key_padding_mask,
            mask_name="key_padding_mask",
            other_type=_none_or_dtype(attn_mask),
            other_name="attn_mask",
            target_type=query.dtype,
        )
    
        if is_causal and attn_mask is None:
            raise RuntimeError(
                "Need attn_mask if specifying the is_causal hint. "
                "You may use the Transformer module method "
                "`generate_square_subsequent_mask` to create this mask."
            )
    
        if is_causal and key_padding_mask is None and not need_weights:
            # when we have a kpm or need weights, we need attn_mask
            # Otherwise, we use the is_causal hint go as is_causal
            # indicator to SDPA.
            attn_mask = None
        else:
            attn_mask = _canonical_mask(
                mask=attn_mask,
                mask_name="attn_mask",
                other_type=None,
                other_name="",
                target_type=query.dtype,
                check_other=False,
            )
    
            if key_padding_mask is not None:
                # We have the attn_mask, and use that to merge kpm into it.
                # Turn off use of is_causal hint, as the merged mask is no
                # longer causal.
                is_causal = False
    
        assert embed_dim == embed_dim_to_check, (
            f"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}"
        )
        if isinstance(embed_dim, torch.Tensor):
            # embed_dim can be a tensor when JIT tracing
            head_dim = embed_dim.div(num_heads, rounding_mode="trunc")
        else:
            head_dim = embed_dim // num_heads
        assert head_dim * num_heads == embed_dim, (
            f"embed_dim {embed_dim} not divisible by num_heads {num_heads}"
        )
        if use_separate_proj_weight:
            # allow MHA to have different embedding dimensions when separate projection weights are used
            assert key.shape[:2] == value.shape[:2], (
                f"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}"
            )
        else:
            assert key.shape == value.shape, (
                f"key shape {key.shape} does not match value shape {value.shape}"
            )
    
        #
        # compute in-projection
        #
        if not use_separate_proj_weight:
            assert in_proj_weight is not None, (
                "use_separate_proj_weight is False but in_proj_weight is None"
            )
            q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
        else:
            assert q_proj_weight is not None, (
                "use_separate_proj_weight is True but q_proj_weight is None"
            )
            assert k_proj_weight is not None, (
                "use_separate_proj_weight is True but k_proj_weight is None"
            )
            assert v_proj_weight is not None, (
                "use_separate_proj_weight is True but v_proj_weight is None"
            )
            if in_proj_bias is None:
                b_q = b_k = b_v = None
            else:
                b_q, b_k, b_v = in_proj_bias.chunk(3)
            q, k, v = _in_projection(
                query,
                key,
                value,
                q_proj_weight,
                k_proj_weight,
                v_proj_weight,
                b_q,
                b_k,
                b_v,
            )
    
        # prep attention mask
    
        if attn_mask is not None:
            # ensure attn_mask's dim is 3
            if attn_mask.dim() == 2:
                correct_2d_size = (tgt_len, src_len)
                if attn_mask.shape != correct_2d_size:
                    raise RuntimeError(
                        f"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}."
                    )
                attn_mask = attn_mask.unsqueeze(0)
            elif attn_mask.dim() == 3:
                correct_3d_size = (bsz * num_heads, tgt_len, src_len)
                if attn_mask.shape != correct_3d_size:
                    raise RuntimeError(
                        f"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}."
                    )
            else:
                raise RuntimeError(
                    f"attn_mask's dimension {attn_mask.dim()} is not supported"
                )
    
        # add bias along batch dimension (currently second)
        if bias_k is not None and bias_v is not None:
            assert static_k is None, "bias cannot be added to static key."
            assert static_v is None, "bias cannot be added to static value."
            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
        else:
            assert bias_k is None
            assert bias_v is None
    
        #
        # reshape q, k, v for multihead attention and make them batch first
        #
        q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
        if static_k is None:
            k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_k.size(0) == bsz * num_heads, (
                f"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}"
            )
            assert static_k.size(2) == head_dim, (
                f"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}"
            )
            k = static_k
        if static_v is None:
            v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_v.size(0) == bsz * num_heads, (
                f"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}"
            )
            assert static_v.size(2) == head_dim, (
                f"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}"
            )
            v = static_v
    
        # add zero attention along batch dimension (now first)
        if add_zero_attn:
            zero_attn_shape = (bsz * num_heads, 1, head_dim)
            k = torch.cat(
                [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1
            )
            v = torch.cat(
                [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1
            )
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
    
        # update source sequence length after adjustments
        src_len = k.size(1)
    
        # merge key padding and attention masks
        if key_padding_mask is not None:
            if not torch.jit.is_scripting() and not torch.jit.is_tracing():
                _check_key_padding_mask(key_padding_mask, src_len, bsz)
    
            key_padding_mask = (
                key_padding_mask.view(bsz, 1, 1, src_len)
                .expand(-1, num_heads, -1, -1)
                .reshape(bsz * num_heads, 1, src_len)
            )
            if attn_mask is None:
                attn_mask = key_padding_mask
            else:
                attn_mask = attn_mask + key_padding_mask
    
        # adjust dropout probability
        if not training:
            dropout_p = 0.0
    
        #
        # (deep breath) calculate attention and out projection
        #
    
        if need_weights:
            _B, _Nt, E = q.shape
            q_scaled = q * math.sqrt(1.0 / float(E))
    
            assert not (is_causal and attn_mask is None), (
                "FIXME: is_causal not implemented for need_weights"
            )
    
            if attn_mask is not None:
                attn_output_weights = torch.baddbmm(
                    attn_mask, q_scaled, k.transpose(-2, -1)
                )
            else:
                attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
            attn_output_weights = softmax(attn_output_weights, dim=-1)
            if dropout_p > 0.0:
                attn_output_weights = dropout(attn_output_weights, p=dropout_p)
    
            attn_output = torch.bmm(attn_output_weights, v)
    
            attn_output = (
                attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
            )
            attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
            attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
    
            # optionally average attention weights over heads
            attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
            if average_attn_weights:
                attn_output_weights = attn_output_weights.mean(dim=1)
    
            if not is_batched:
                # squeeze the output if input was unbatched
                attn_output = attn_output.squeeze(1)
                attn_output_weights = attn_output_weights.squeeze(0)
            return attn_output, attn_output_weights
        else:
            # attn_mask can be either (L,S) or (N*num_heads, L, S)
            # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)
            # in order to match the input for SDPA of (N, num_heads, L, S)
            if attn_mask is not None:
                if attn_mask.size(0) == 1 and attn_mask.dim() == 3:
                    attn_mask = attn_mask.unsqueeze(0)
                else:
                    attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)
    
            q = q.view(bsz, num_heads, tgt_len, head_dim)
            k = k.view(bsz, num_heads, src_len, head_dim)
            v = v.view(bsz, num_heads, src_len, head_dim)
    
>           attn_output = scaled_dot_product_attention(
                q, k, v, attn_mask, dropout_p, is_causal
            )
E           RuntimeError: cuDNN Frontend error: CUDNN_BACKEND_TENSOR_DESCRIPTOR cudnnFinalize failedptrDesc->finalize() cudnn_status: CUDNN_STATUS_SUBLIBRARY_LOADING_FAILED

/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6487: RuntimeError
____________________ test_gpt_accuracy[True-small-1-dtype2] ____________________

dtype = torch.bfloat16, bs = 1, model = 'small', parallel_attention_mlp = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("parallel_attention_mlp", all_boolean)
    def test_gpt_accuracy(dtype, bs, model, parallel_attention_mlp):
        config = model_configs[model]
    
        te_gpt = TransformerLayer(
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            num_attention_heads=config.num_heads,
            layernorm_epsilon=config.eps,
            attention_dropout=0.1,
            hidden_dropout=0.1,
            params_dtype=dtype,
            fuse_qkv_params=True,
            qkv_weight_interleaved=False,
            parallel_attention_mlp=parallel_attention_mlp,
            device="cuda",
        ).eval()
    
        torch_gpt = (
            TorchGPT(
                config.hidden_size,
                config.eps,
                config.num_heads,
                parallel_attention_mlp=parallel_attention_mlp,
            )
            .to(dtype=dtype)
            .cuda()
            .eval()
        )
    
        # Share params
        with torch.no_grad():
            torch_gpt.ln.weight = Parameter(
                te_gpt.self_attention.layernorm_qkv.layer_norm_weight.clone()
            )
            torch_gpt.ln.bias = Parameter(te_gpt.self_attention.layernorm_qkv.layer_norm_bias.clone())
            torch_gpt.causal_attn.mhsa.in_proj_weight = Parameter(
                te_gpt.self_attention.layernorm_qkv.weight.clone()
            )
            torch_gpt.causal_attn.mhsa.in_proj_bias = Parameter(
                te_gpt.self_attention.layernorm_qkv.bias.clone()
            )
            torch_gpt.causal_attn.mhsa.out_proj.weight = Parameter(
                te_gpt.self_attention.proj.weight.clone()
            )
            torch_gpt.causal_attn.mhsa.out_proj.bias = Parameter(
                te_gpt.self_attention.proj.bias.clone()
            )
            torch_gpt.ln_mlp.ln.weight = Parameter(te_gpt.layernorm_mlp.layer_norm_weight.clone())
            torch_gpt.ln_mlp.ln.bias = Parameter(te_gpt.layernorm_mlp.layer_norm_bias.clone())
            torch_gpt.ln_mlp.fc1.weight = Parameter(te_gpt.layernorm_mlp.fc1_weight.clone())
            torch_gpt.ln_mlp.fc1.bias = Parameter(te_gpt.layernorm_mlp.fc1_bias.clone())
            torch_gpt.ln_mlp.fc2.weight = Parameter(te_gpt.layernorm_mlp.fc2_weight.clone())
            torch_gpt.ln_mlp.fc2.bias = Parameter(te_gpt.layernorm_mlp.fc2_bias.clone())
    
        te_outputs = _test_e2e_gpt_accuracy(te_gpt, bs, dtype, config)
>       torch_outputs = _test_e2e_gpt_accuracy(torch_gpt, bs, dtype, config)

tests/pytorch/test_numerics.py:987: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/test_numerics.py:916: in _test_e2e_gpt_accuracy
    out = block(inp_hidden_states, attention_mask=inp_attn_mask)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:551: in forward
    b = self.causal_attn(a, attention_mask)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:426: in forward
    output = self.mhsa(x, x, x, attn_mask=attention_mask, need_weights=False)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1488: in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = tensor([[[-1.6719,  0.5508, -0.5508,  ..., -1.4062,  2.7188, -0.1992]],

        [[-0.2754,  1.3516,  0.1406,  ..., -1...,  ..., -1.1641, -0.6641, -1.6016]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<NativeLayerNormBackward0>)
key = tensor([[[-1.6719,  0.5508, -0.5508,  ..., -1.4062,  2.7188, -0.1992]],

        [[-0.2754,  1.3516,  0.1406,  ..., -1...,  ..., -1.1641, -0.6641, -1.6016]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<NativeLayerNormBackward0>)
value = tensor([[[-1.6719,  0.5508, -0.5508,  ..., -1.4062,  2.7188, -0.1992]],

        [[-0.2754,  1.3516,  0.1406,  ..., -1...,  ..., -1.1641, -0.6641, -1.6016]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<NativeLayerNormBackward0>)
embed_dim_to_check = 128, num_heads = 8
in_proj_weight = Parameter containing:
tensor([[ 1.4587e-02,  3.7598e-02, -7.9346e-03,  ..., -6.2256e-03,
          7.5340e-05,  1.5335....., -3.3936e-02,
         -1.2451e-02,  3.1281e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
in_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
bias_k = None, bias_v = None, add_zero_attn = False, dropout_p = 0.0
out_proj_weight = Parameter containing:
tensor([[-0.0037,  0.0415, -0.0242,  ...,  0.0052, -0.0132, -0.0171],
        [ 0.0056,  0.0354,... -0.0457,  0.0081,  ..., -0.0051, -0.0339,  0.0095]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
out_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ... 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
training = False, key_padding_mask = None, need_weights = False
attn_mask = tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],
          [0., 0., -inf,  ..., -inf, -inf, -inf],
          [0., 0...0., 0.,  ..., 0., 0., -inf],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
use_separate_proj_weight = False, q_proj_weight = None, k_proj_weight = None
v_proj_weight = None, static_k = None, static_v = None
average_attn_weights = True, is_causal = False

    def multi_head_attention_forward(
        query: Tensor,
        key: Tensor,
        value: Tensor,
        embed_dim_to_check: int,
        num_heads: int,
        in_proj_weight: Optional[Tensor],
        in_proj_bias: Optional[Tensor],
        bias_k: Optional[Tensor],
        bias_v: Optional[Tensor],
        add_zero_attn: bool,
        dropout_p: float,
        out_proj_weight: Tensor,
        out_proj_bias: Optional[Tensor],
        training: bool = True,
        key_padding_mask: Optional[Tensor] = None,
        need_weights: bool = True,
        attn_mask: Optional[Tensor] = None,
        use_separate_proj_weight: bool = False,
        q_proj_weight: Optional[Tensor] = None,
        k_proj_weight: Optional[Tensor] = None,
        v_proj_weight: Optional[Tensor] = None,
        static_k: Optional[Tensor] = None,
        static_v: Optional[Tensor] = None,
        average_attn_weights: bool = True,
        is_causal: bool = False,
    ) -> tuple[Tensor, Optional[Tensor]]:
        r"""Forward method for MultiHeadAttention.
    
        See :class:`torch.nn.MultiheadAttention` for details.
    
        Args:
            query, key, value: map a query and a set of key-value pairs to an output.
                See "Attention Is All You Need" for more details.
            embed_dim_to_check: total dimension of the model.
            num_heads: parallel attention heads.
            in_proj_weight, in_proj_bias: input projection weight and bias.
            bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
            add_zero_attn: add a new batch of zeros to the key and
                           value sequences at dim=1.
            dropout_p: probability of an element to be zeroed.
            out_proj_weight, out_proj_bias: the output projection weight and bias.
            training: apply dropout if is ``True``.
            key_padding_mask: if provided, specified padding elements in the key will
                be ignored by the attention. This is an binary mask. When the value is True,
                the corresponding value on the attention layer will be filled with -inf.
            need_weights: output attn_output_weights.
                Default: `True`
                Note: `needs_weight` defaults to `True`, but should be set to `False`
                For best performance when attention weights are not needed.
                *Setting needs_weights to `True`
                leads to a significant performance degradation.*
            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
                the batches while a 3D mask allows to specify a different mask for the entries of each batch.
            is_causal: If specified, applies a causal mask as attention mask, and ignores
                attn_mask for computing scaled dot product attention.
                Default: ``False``.
                .. warning::
                    is_causal is provides a hint that the attn_mask is the
                    causal mask.Providing incorrect hints can result in
                    incorrect execution, including forward and backward
                    compatibility.
            use_separate_proj_weight: the function accept the proj. weights for query, key,
                and value in different forms. If false, in_proj_weight will be used, which is
                a combination of q_proj_weight, k_proj_weight, v_proj_weight.
            q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
            static_k, static_v: static key and value used for attention operators.
            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.
                Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect
                when ``need_weights=True.``. Default: True
    
    
        Shape:
            Inputs:
            - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
              the embedding dimension.
            - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.
              If a FloatTensor is provided, it will be directly added to the value.
              If a BoolTensor is provided, the positions with the
              value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
            - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
              3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
              S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked
              positions. If a BoolTensor is provided, positions with ``True``
              are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
              is provided, it will be added to the attention weight.
            - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
            - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
    
            Outputs:
            - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
              E is the embedding dimension.
            - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns
              attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or
              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and
              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per
              head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.
        """
        tens_ops = (
            query,
            key,
            value,
            in_proj_weight,
            in_proj_bias,
            bias_k,
            bias_v,
            out_proj_weight,
            out_proj_bias,
        )
        if has_torch_function(tens_ops):
            return handle_torch_function(
                multi_head_attention_forward,
                tens_ops,
                query,
                key,
                value,
                embed_dim_to_check,
                num_heads,
                in_proj_weight,
                in_proj_bias,
                bias_k,
                bias_v,
                add_zero_attn,
                dropout_p,
                out_proj_weight,
                out_proj_bias,
                training=training,
                key_padding_mask=key_padding_mask,
                need_weights=need_weights,
                attn_mask=attn_mask,
                is_causal=is_causal,
                use_separate_proj_weight=use_separate_proj_weight,
                q_proj_weight=q_proj_weight,
                k_proj_weight=k_proj_weight,
                v_proj_weight=v_proj_weight,
                static_k=static_k,
                static_v=static_v,
                average_attn_weights=average_attn_weights,
            )
    
        is_batched = _mha_shape_check(
            query, key, value, key_padding_mask, attn_mask, num_heads
        )
    
        # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input
        # is batched, run the computation and before returning squeeze the
        # batch dimension so that the output doesn't carry this temporary batch dimension.
        if not is_batched:
            # unsqueeze if the input is unbatched
            query = query.unsqueeze(1)
            key = key.unsqueeze(1)
            value = value.unsqueeze(1)
            if key_padding_mask is not None:
                key_padding_mask = key_padding_mask.unsqueeze(0)
    
        # set up shape vars
        tgt_len, bsz, embed_dim = query.shape
        src_len, _, _ = key.shape
    
        key_padding_mask = _canonical_mask(
            mask=key_padding_mask,
            mask_name="key_padding_mask",
            other_type=_none_or_dtype(attn_mask),
            other_name="attn_mask",
            target_type=query.dtype,
        )
    
        if is_causal and attn_mask is None:
            raise RuntimeError(
                "Need attn_mask if specifying the is_causal hint. "
                "You may use the Transformer module method "
                "`generate_square_subsequent_mask` to create this mask."
            )
    
        if is_causal and key_padding_mask is None and not need_weights:
            # when we have a kpm or need weights, we need attn_mask
            # Otherwise, we use the is_causal hint go as is_causal
            # indicator to SDPA.
            attn_mask = None
        else:
            attn_mask = _canonical_mask(
                mask=attn_mask,
                mask_name="attn_mask",
                other_type=None,
                other_name="",
                target_type=query.dtype,
                check_other=False,
            )
    
            if key_padding_mask is not None:
                # We have the attn_mask, and use that to merge kpm into it.
                # Turn off use of is_causal hint, as the merged mask is no
                # longer causal.
                is_causal = False
    
        assert embed_dim == embed_dim_to_check, (
            f"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}"
        )
        if isinstance(embed_dim, torch.Tensor):
            # embed_dim can be a tensor when JIT tracing
            head_dim = embed_dim.div(num_heads, rounding_mode="trunc")
        else:
            head_dim = embed_dim // num_heads
        assert head_dim * num_heads == embed_dim, (
            f"embed_dim {embed_dim} not divisible by num_heads {num_heads}"
        )
        if use_separate_proj_weight:
            # allow MHA to have different embedding dimensions when separate projection weights are used
            assert key.shape[:2] == value.shape[:2], (
                f"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}"
            )
        else:
            assert key.shape == value.shape, (
                f"key shape {key.shape} does not match value shape {value.shape}"
            )
    
        #
        # compute in-projection
        #
        if not use_separate_proj_weight:
            assert in_proj_weight is not None, (
                "use_separate_proj_weight is False but in_proj_weight is None"
            )
            q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
        else:
            assert q_proj_weight is not None, (
                "use_separate_proj_weight is True but q_proj_weight is None"
            )
            assert k_proj_weight is not None, (
                "use_separate_proj_weight is True but k_proj_weight is None"
            )
            assert v_proj_weight is not None, (
                "use_separate_proj_weight is True but v_proj_weight is None"
            )
            if in_proj_bias is None:
                b_q = b_k = b_v = None
            else:
                b_q, b_k, b_v = in_proj_bias.chunk(3)
            q, k, v = _in_projection(
                query,
                key,
                value,
                q_proj_weight,
                k_proj_weight,
                v_proj_weight,
                b_q,
                b_k,
                b_v,
            )
    
        # prep attention mask
    
        if attn_mask is not None:
            # ensure attn_mask's dim is 3
            if attn_mask.dim() == 2:
                correct_2d_size = (tgt_len, src_len)
                if attn_mask.shape != correct_2d_size:
                    raise RuntimeError(
                        f"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}."
                    )
                attn_mask = attn_mask.unsqueeze(0)
            elif attn_mask.dim() == 3:
                correct_3d_size = (bsz * num_heads, tgt_len, src_len)
                if attn_mask.shape != correct_3d_size:
                    raise RuntimeError(
                        f"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}."
                    )
            else:
                raise RuntimeError(
                    f"attn_mask's dimension {attn_mask.dim()} is not supported"
                )
    
        # add bias along batch dimension (currently second)
        if bias_k is not None and bias_v is not None:
            assert static_k is None, "bias cannot be added to static key."
            assert static_v is None, "bias cannot be added to static value."
            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
        else:
            assert bias_k is None
            assert bias_v is None
    
        #
        # reshape q, k, v for multihead attention and make them batch first
        #
        q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
        if static_k is None:
            k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_k.size(0) == bsz * num_heads, (
                f"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}"
            )
            assert static_k.size(2) == head_dim, (
                f"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}"
            )
            k = static_k
        if static_v is None:
            v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_v.size(0) == bsz * num_heads, (
                f"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}"
            )
            assert static_v.size(2) == head_dim, (
                f"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}"
            )
            v = static_v
    
        # add zero attention along batch dimension (now first)
        if add_zero_attn:
            zero_attn_shape = (bsz * num_heads, 1, head_dim)
            k = torch.cat(
                [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1
            )
            v = torch.cat(
                [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1
            )
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
    
        # update source sequence length after adjustments
        src_len = k.size(1)
    
        # merge key padding and attention masks
        if key_padding_mask is not None:
            if not torch.jit.is_scripting() and not torch.jit.is_tracing():
                _check_key_padding_mask(key_padding_mask, src_len, bsz)
    
            key_padding_mask = (
                key_padding_mask.view(bsz, 1, 1, src_len)
                .expand(-1, num_heads, -1, -1)
                .reshape(bsz * num_heads, 1, src_len)
            )
            if attn_mask is None:
                attn_mask = key_padding_mask
            else:
                attn_mask = attn_mask + key_padding_mask
    
        # adjust dropout probability
        if not training:
            dropout_p = 0.0
    
        #
        # (deep breath) calculate attention and out projection
        #
    
        if need_weights:
            _B, _Nt, E = q.shape
            q_scaled = q * math.sqrt(1.0 / float(E))
    
            assert not (is_causal and attn_mask is None), (
                "FIXME: is_causal not implemented for need_weights"
            )
    
            if attn_mask is not None:
                attn_output_weights = torch.baddbmm(
                    attn_mask, q_scaled, k.transpose(-2, -1)
                )
            else:
                attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
            attn_output_weights = softmax(attn_output_weights, dim=-1)
            if dropout_p > 0.0:
                attn_output_weights = dropout(attn_output_weights, p=dropout_p)
    
            attn_output = torch.bmm(attn_output_weights, v)
    
            attn_output = (
                attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
            )
            attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
            attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
    
            # optionally average attention weights over heads
            attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
            if average_attn_weights:
                attn_output_weights = attn_output_weights.mean(dim=1)
    
            if not is_batched:
                # squeeze the output if input was unbatched
                attn_output = attn_output.squeeze(1)
                attn_output_weights = attn_output_weights.squeeze(0)
            return attn_output, attn_output_weights
        else:
            # attn_mask can be either (L,S) or (N*num_heads, L, S)
            # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)
            # in order to match the input for SDPA of (N, num_heads, L, S)
            if attn_mask is not None:
                if attn_mask.size(0) == 1 and attn_mask.dim() == 3:
                    attn_mask = attn_mask.unsqueeze(0)
                else:
                    attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)
    
            q = q.view(bsz, num_heads, tgt_len, head_dim)
            k = k.view(bsz, num_heads, src_len, head_dim)
            v = v.view(bsz, num_heads, src_len, head_dim)
    
>           attn_output = scaled_dot_product_attention(
                q, k, v, attn_mask, dropout_p, is_causal
            )
E           RuntimeError: cuDNN Frontend error: CUDNN_BACKEND_TENSOR_DESCRIPTOR cudnnFinalize failedptrDesc->finalize() cudnn_status: CUDNN_STATUS_SUBLIBRARY_LOADING_FAILED

/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6487: RuntimeError
____________________ test_gpt_accuracy[True-small-2-dtype1] ____________________

dtype = torch.float16, bs = 2, model = 'small', parallel_attention_mlp = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("parallel_attention_mlp", all_boolean)
    def test_gpt_accuracy(dtype, bs, model, parallel_attention_mlp):
        config = model_configs[model]
    
        te_gpt = TransformerLayer(
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            num_attention_heads=config.num_heads,
            layernorm_epsilon=config.eps,
            attention_dropout=0.1,
            hidden_dropout=0.1,
            params_dtype=dtype,
            fuse_qkv_params=True,
            qkv_weight_interleaved=False,
            parallel_attention_mlp=parallel_attention_mlp,
            device="cuda",
        ).eval()
    
        torch_gpt = (
            TorchGPT(
                config.hidden_size,
                config.eps,
                config.num_heads,
                parallel_attention_mlp=parallel_attention_mlp,
            )
            .to(dtype=dtype)
            .cuda()
            .eval()
        )
    
        # Share params
        with torch.no_grad():
            torch_gpt.ln.weight = Parameter(
                te_gpt.self_attention.layernorm_qkv.layer_norm_weight.clone()
            )
            torch_gpt.ln.bias = Parameter(te_gpt.self_attention.layernorm_qkv.layer_norm_bias.clone())
            torch_gpt.causal_attn.mhsa.in_proj_weight = Parameter(
                te_gpt.self_attention.layernorm_qkv.weight.clone()
            )
            torch_gpt.causal_attn.mhsa.in_proj_bias = Parameter(
                te_gpt.self_attention.layernorm_qkv.bias.clone()
            )
            torch_gpt.causal_attn.mhsa.out_proj.weight = Parameter(
                te_gpt.self_attention.proj.weight.clone()
            )
            torch_gpt.causal_attn.mhsa.out_proj.bias = Parameter(
                te_gpt.self_attention.proj.bias.clone()
            )
            torch_gpt.ln_mlp.ln.weight = Parameter(te_gpt.layernorm_mlp.layer_norm_weight.clone())
            torch_gpt.ln_mlp.ln.bias = Parameter(te_gpt.layernorm_mlp.layer_norm_bias.clone())
            torch_gpt.ln_mlp.fc1.weight = Parameter(te_gpt.layernorm_mlp.fc1_weight.clone())
            torch_gpt.ln_mlp.fc1.bias = Parameter(te_gpt.layernorm_mlp.fc1_bias.clone())
            torch_gpt.ln_mlp.fc2.weight = Parameter(te_gpt.layernorm_mlp.fc2_weight.clone())
            torch_gpt.ln_mlp.fc2.bias = Parameter(te_gpt.layernorm_mlp.fc2_bias.clone())
    
        te_outputs = _test_e2e_gpt_accuracy(te_gpt, bs, dtype, config)
>       torch_outputs = _test_e2e_gpt_accuracy(torch_gpt, bs, dtype, config)

tests/pytorch/test_numerics.py:987: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/test_numerics.py:916: in _test_e2e_gpt_accuracy
    out = block(inp_hidden_states, attention_mask=inp_attn_mask)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:551: in forward
    b = self.causal_attn(a, attention_mask)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:426: in forward
    output = self.mhsa(x, x, x, attn_mask=attention_mask, need_weights=False)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1488: in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = tensor([[[-1.6729,  0.5493, -0.5479,  ..., -1.4033,  2.7266, -0.1997],
         [-0.2756,  1.3506,  0.1406,  ..., -1.5...0,  ...,  0.1979,  0.7422, -0.3604]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)
key = tensor([[[-1.6729,  0.5493, -0.5479,  ..., -1.4033,  2.7266, -0.1997],
         [-0.2756,  1.3506,  0.1406,  ..., -1.5...0,  ...,  0.1979,  0.7422, -0.3604]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)
value = tensor([[[-1.6729,  0.5493, -0.5479,  ..., -1.4033,  2.7266, -0.1997],
         [-0.2756,  1.3506,  0.1406,  ..., -1.5...0,  ...,  0.1979,  0.7422, -0.3604]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)
embed_dim_to_check = 128, num_heads = 8
in_proj_weight = Parameter containing:
tensor([[ 1.4565e-02,  3.7628e-02, -7.9575e-03,  ..., -6.2141e-03,
          7.5221e-05,  1.5306......, -3.4058e-02,
         -1.2466e-02,  3.1300e-03]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
in_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
bias_k = None, bias_v = None, add_zero_attn = False, dropout_p = 0.0
out_proj_weight = Parameter containing:
tensor([[-0.0037,  0.0416, -0.0242,  ...,  0.0051, -0.0132, -0.0170],
        [ 0.0056,  0.0353,..., -0.0457,  0.0080,  ..., -0.0051, -0.0340,  0.0095]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
out_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ..., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
training = False, key_padding_mask = None, need_weights = False
attn_mask = tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],
          [0., 0., -inf,  ..., -inf, -inf, -inf],
          [0., 0... 0., 0.,  ..., 0., 0., -inf],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
use_separate_proj_weight = False, q_proj_weight = None, k_proj_weight = None
v_proj_weight = None, static_k = None, static_v = None
average_attn_weights = True, is_causal = False

    def multi_head_attention_forward(
        query: Tensor,
        key: Tensor,
        value: Tensor,
        embed_dim_to_check: int,
        num_heads: int,
        in_proj_weight: Optional[Tensor],
        in_proj_bias: Optional[Tensor],
        bias_k: Optional[Tensor],
        bias_v: Optional[Tensor],
        add_zero_attn: bool,
        dropout_p: float,
        out_proj_weight: Tensor,
        out_proj_bias: Optional[Tensor],
        training: bool = True,
        key_padding_mask: Optional[Tensor] = None,
        need_weights: bool = True,
        attn_mask: Optional[Tensor] = None,
        use_separate_proj_weight: bool = False,
        q_proj_weight: Optional[Tensor] = None,
        k_proj_weight: Optional[Tensor] = None,
        v_proj_weight: Optional[Tensor] = None,
        static_k: Optional[Tensor] = None,
        static_v: Optional[Tensor] = None,
        average_attn_weights: bool = True,
        is_causal: bool = False,
    ) -> tuple[Tensor, Optional[Tensor]]:
        r"""Forward method for MultiHeadAttention.
    
        See :class:`torch.nn.MultiheadAttention` for details.
    
        Args:
            query, key, value: map a query and a set of key-value pairs to an output.
                See "Attention Is All You Need" for more details.
            embed_dim_to_check: total dimension of the model.
            num_heads: parallel attention heads.
            in_proj_weight, in_proj_bias: input projection weight and bias.
            bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
            add_zero_attn: add a new batch of zeros to the key and
                           value sequences at dim=1.
            dropout_p: probability of an element to be zeroed.
            out_proj_weight, out_proj_bias: the output projection weight and bias.
            training: apply dropout if is ``True``.
            key_padding_mask: if provided, specified padding elements in the key will
                be ignored by the attention. This is an binary mask. When the value is True,
                the corresponding value on the attention layer will be filled with -inf.
            need_weights: output attn_output_weights.
                Default: `True`
                Note: `needs_weight` defaults to `True`, but should be set to `False`
                For best performance when attention weights are not needed.
                *Setting needs_weights to `True`
                leads to a significant performance degradation.*
            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
                the batches while a 3D mask allows to specify a different mask for the entries of each batch.
            is_causal: If specified, applies a causal mask as attention mask, and ignores
                attn_mask for computing scaled dot product attention.
                Default: ``False``.
                .. warning::
                    is_causal is provides a hint that the attn_mask is the
                    causal mask.Providing incorrect hints can result in
                    incorrect execution, including forward and backward
                    compatibility.
            use_separate_proj_weight: the function accept the proj. weights for query, key,
                and value in different forms. If false, in_proj_weight will be used, which is
                a combination of q_proj_weight, k_proj_weight, v_proj_weight.
            q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
            static_k, static_v: static key and value used for attention operators.
            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.
                Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect
                when ``need_weights=True.``. Default: True
    
    
        Shape:
            Inputs:
            - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
              the embedding dimension.
            - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.
              If a FloatTensor is provided, it will be directly added to the value.
              If a BoolTensor is provided, the positions with the
              value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
            - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
              3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
              S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked
              positions. If a BoolTensor is provided, positions with ``True``
              are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
              is provided, it will be added to the attention weight.
            - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
            - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
    
            Outputs:
            - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
              E is the embedding dimension.
            - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns
              attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or
              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and
              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per
              head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.
        """
        tens_ops = (
            query,
            key,
            value,
            in_proj_weight,
            in_proj_bias,
            bias_k,
            bias_v,
            out_proj_weight,
            out_proj_bias,
        )
        if has_torch_function(tens_ops):
            return handle_torch_function(
                multi_head_attention_forward,
                tens_ops,
                query,
                key,
                value,
                embed_dim_to_check,
                num_heads,
                in_proj_weight,
                in_proj_bias,
                bias_k,
                bias_v,
                add_zero_attn,
                dropout_p,
                out_proj_weight,
                out_proj_bias,
                training=training,
                key_padding_mask=key_padding_mask,
                need_weights=need_weights,
                attn_mask=attn_mask,
                is_causal=is_causal,
                use_separate_proj_weight=use_separate_proj_weight,
                q_proj_weight=q_proj_weight,
                k_proj_weight=k_proj_weight,
                v_proj_weight=v_proj_weight,
                static_k=static_k,
                static_v=static_v,
                average_attn_weights=average_attn_weights,
            )
    
        is_batched = _mha_shape_check(
            query, key, value, key_padding_mask, attn_mask, num_heads
        )
    
        # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input
        # is batched, run the computation and before returning squeeze the
        # batch dimension so that the output doesn't carry this temporary batch dimension.
        if not is_batched:
            # unsqueeze if the input is unbatched
            query = query.unsqueeze(1)
            key = key.unsqueeze(1)
            value = value.unsqueeze(1)
            if key_padding_mask is not None:
                key_padding_mask = key_padding_mask.unsqueeze(0)
    
        # set up shape vars
        tgt_len, bsz, embed_dim = query.shape
        src_len, _, _ = key.shape
    
        key_padding_mask = _canonical_mask(
            mask=key_padding_mask,
            mask_name="key_padding_mask",
            other_type=_none_or_dtype(attn_mask),
            other_name="attn_mask",
            target_type=query.dtype,
        )
    
        if is_causal and attn_mask is None:
            raise RuntimeError(
                "Need attn_mask if specifying the is_causal hint. "
                "You may use the Transformer module method "
                "`generate_square_subsequent_mask` to create this mask."
            )
    
        if is_causal and key_padding_mask is None and not need_weights:
            # when we have a kpm or need weights, we need attn_mask
            # Otherwise, we use the is_causal hint go as is_causal
            # indicator to SDPA.
            attn_mask = None
        else:
            attn_mask = _canonical_mask(
                mask=attn_mask,
                mask_name="attn_mask",
                other_type=None,
                other_name="",
                target_type=query.dtype,
                check_other=False,
            )
    
            if key_padding_mask is not None:
                # We have the attn_mask, and use that to merge kpm into it.
                # Turn off use of is_causal hint, as the merged mask is no
                # longer causal.
                is_causal = False
    
        assert embed_dim == embed_dim_to_check, (
            f"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}"
        )
        if isinstance(embed_dim, torch.Tensor):
            # embed_dim can be a tensor when JIT tracing
            head_dim = embed_dim.div(num_heads, rounding_mode="trunc")
        else:
            head_dim = embed_dim // num_heads
        assert head_dim * num_heads == embed_dim, (
            f"embed_dim {embed_dim} not divisible by num_heads {num_heads}"
        )
        if use_separate_proj_weight:
            # allow MHA to have different embedding dimensions when separate projection weights are used
            assert key.shape[:2] == value.shape[:2], (
                f"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}"
            )
        else:
            assert key.shape == value.shape, (
                f"key shape {key.shape} does not match value shape {value.shape}"
            )
    
        #
        # compute in-projection
        #
        if not use_separate_proj_weight:
            assert in_proj_weight is not None, (
                "use_separate_proj_weight is False but in_proj_weight is None"
            )
            q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
        else:
            assert q_proj_weight is not None, (
                "use_separate_proj_weight is True but q_proj_weight is None"
            )
            assert k_proj_weight is not None, (
                "use_separate_proj_weight is True but k_proj_weight is None"
            )
            assert v_proj_weight is not None, (
                "use_separate_proj_weight is True but v_proj_weight is None"
            )
            if in_proj_bias is None:
                b_q = b_k = b_v = None
            else:
                b_q, b_k, b_v = in_proj_bias.chunk(3)
            q, k, v = _in_projection(
                query,
                key,
                value,
                q_proj_weight,
                k_proj_weight,
                v_proj_weight,
                b_q,
                b_k,
                b_v,
            )
    
        # prep attention mask
    
        if attn_mask is not None:
            # ensure attn_mask's dim is 3
            if attn_mask.dim() == 2:
                correct_2d_size = (tgt_len, src_len)
                if attn_mask.shape != correct_2d_size:
                    raise RuntimeError(
                        f"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}."
                    )
                attn_mask = attn_mask.unsqueeze(0)
            elif attn_mask.dim() == 3:
                correct_3d_size = (bsz * num_heads, tgt_len, src_len)
                if attn_mask.shape != correct_3d_size:
                    raise RuntimeError(
                        f"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}."
                    )
            else:
                raise RuntimeError(
                    f"attn_mask's dimension {attn_mask.dim()} is not supported"
                )
    
        # add bias along batch dimension (currently second)
        if bias_k is not None and bias_v is not None:
            assert static_k is None, "bias cannot be added to static key."
            assert static_v is None, "bias cannot be added to static value."
            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
        else:
            assert bias_k is None
            assert bias_v is None
    
        #
        # reshape q, k, v for multihead attention and make them batch first
        #
        q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
        if static_k is None:
            k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_k.size(0) == bsz * num_heads, (
                f"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}"
            )
            assert static_k.size(2) == head_dim, (
                f"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}"
            )
            k = static_k
        if static_v is None:
            v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_v.size(0) == bsz * num_heads, (
                f"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}"
            )
            assert static_v.size(2) == head_dim, (
                f"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}"
            )
            v = static_v
    
        # add zero attention along batch dimension (now first)
        if add_zero_attn:
            zero_attn_shape = (bsz * num_heads, 1, head_dim)
            k = torch.cat(
                [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1
            )
            v = torch.cat(
                [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1
            )
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
    
        # update source sequence length after adjustments
        src_len = k.size(1)
    
        # merge key padding and attention masks
        if key_padding_mask is not None:
            if not torch.jit.is_scripting() and not torch.jit.is_tracing():
                _check_key_padding_mask(key_padding_mask, src_len, bsz)
    
            key_padding_mask = (
                key_padding_mask.view(bsz, 1, 1, src_len)
                .expand(-1, num_heads, -1, -1)
                .reshape(bsz * num_heads, 1, src_len)
            )
            if attn_mask is None:
                attn_mask = key_padding_mask
            else:
                attn_mask = attn_mask + key_padding_mask
    
        # adjust dropout probability
        if not training:
            dropout_p = 0.0
    
        #
        # (deep breath) calculate attention and out projection
        #
    
        if need_weights:
            _B, _Nt, E = q.shape
            q_scaled = q * math.sqrt(1.0 / float(E))
    
            assert not (is_causal and attn_mask is None), (
                "FIXME: is_causal not implemented for need_weights"
            )
    
            if attn_mask is not None:
                attn_output_weights = torch.baddbmm(
                    attn_mask, q_scaled, k.transpose(-2, -1)
                )
            else:
                attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
            attn_output_weights = softmax(attn_output_weights, dim=-1)
            if dropout_p > 0.0:
                attn_output_weights = dropout(attn_output_weights, p=dropout_p)
    
            attn_output = torch.bmm(attn_output_weights, v)
    
            attn_output = (
                attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
            )
            attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
            attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
    
            # optionally average attention weights over heads
            attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
            if average_attn_weights:
                attn_output_weights = attn_output_weights.mean(dim=1)
    
            if not is_batched:
                # squeeze the output if input was unbatched
                attn_output = attn_output.squeeze(1)
                attn_output_weights = attn_output_weights.squeeze(0)
            return attn_output, attn_output_weights
        else:
            # attn_mask can be either (L,S) or (N*num_heads, L, S)
            # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)
            # in order to match the input for SDPA of (N, num_heads, L, S)
            if attn_mask is not None:
                if attn_mask.size(0) == 1 and attn_mask.dim() == 3:
                    attn_mask = attn_mask.unsqueeze(0)
                else:
                    attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)
    
            q = q.view(bsz, num_heads, tgt_len, head_dim)
            k = k.view(bsz, num_heads, src_len, head_dim)
            v = v.view(bsz, num_heads, src_len, head_dim)
    
>           attn_output = scaled_dot_product_attention(
                q, k, v, attn_mask, dropout_p, is_causal
            )
E           RuntimeError: cuDNN Frontend error: CUDNN_BACKEND_TENSOR_DESCRIPTOR cudnnFinalize failedptrDesc->finalize() cudnn_status: CUDNN_STATUS_SUBLIBRARY_LOADING_FAILED

/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6487: RuntimeError
____________________ test_gpt_accuracy[True-small-2-dtype2] ____________________

dtype = torch.bfloat16, bs = 2, model = 'small', parallel_attention_mlp = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("parallel_attention_mlp", all_boolean)
    def test_gpt_accuracy(dtype, bs, model, parallel_attention_mlp):
        config = model_configs[model]
    
        te_gpt = TransformerLayer(
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            num_attention_heads=config.num_heads,
            layernorm_epsilon=config.eps,
            attention_dropout=0.1,
            hidden_dropout=0.1,
            params_dtype=dtype,
            fuse_qkv_params=True,
            qkv_weight_interleaved=False,
            parallel_attention_mlp=parallel_attention_mlp,
            device="cuda",
        ).eval()
    
        torch_gpt = (
            TorchGPT(
                config.hidden_size,
                config.eps,
                config.num_heads,
                parallel_attention_mlp=parallel_attention_mlp,
            )
            .to(dtype=dtype)
            .cuda()
            .eval()
        )
    
        # Share params
        with torch.no_grad():
            torch_gpt.ln.weight = Parameter(
                te_gpt.self_attention.layernorm_qkv.layer_norm_weight.clone()
            )
            torch_gpt.ln.bias = Parameter(te_gpt.self_attention.layernorm_qkv.layer_norm_bias.clone())
            torch_gpt.causal_attn.mhsa.in_proj_weight = Parameter(
                te_gpt.self_attention.layernorm_qkv.weight.clone()
            )
            torch_gpt.causal_attn.mhsa.in_proj_bias = Parameter(
                te_gpt.self_attention.layernorm_qkv.bias.clone()
            )
            torch_gpt.causal_attn.mhsa.out_proj.weight = Parameter(
                te_gpt.self_attention.proj.weight.clone()
            )
            torch_gpt.causal_attn.mhsa.out_proj.bias = Parameter(
                te_gpt.self_attention.proj.bias.clone()
            )
            torch_gpt.ln_mlp.ln.weight = Parameter(te_gpt.layernorm_mlp.layer_norm_weight.clone())
            torch_gpt.ln_mlp.ln.bias = Parameter(te_gpt.layernorm_mlp.layer_norm_bias.clone())
            torch_gpt.ln_mlp.fc1.weight = Parameter(te_gpt.layernorm_mlp.fc1_weight.clone())
            torch_gpt.ln_mlp.fc1.bias = Parameter(te_gpt.layernorm_mlp.fc1_bias.clone())
            torch_gpt.ln_mlp.fc2.weight = Parameter(te_gpt.layernorm_mlp.fc2_weight.clone())
            torch_gpt.ln_mlp.fc2.bias = Parameter(te_gpt.layernorm_mlp.fc2_bias.clone())
    
        te_outputs = _test_e2e_gpt_accuracy(te_gpt, bs, dtype, config)
>       torch_outputs = _test_e2e_gpt_accuracy(torch_gpt, bs, dtype, config)

tests/pytorch/test_numerics.py:987: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/test_numerics.py:916: in _test_e2e_gpt_accuracy
    out = block(inp_hidden_states, attention_mask=inp_attn_mask)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:551: in forward
    b = self.causal_attn(a, attention_mask)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:426: in forward
    output = self.mhsa(x, x, x, attn_mask=attention_mask, need_weights=False)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1488: in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = tensor([[[-1.6719,  0.5508, -0.5508,  ..., -1.4062,  2.7188, -0.1992],
         [-0.2754,  1.3516,  0.1406,  ..., -1.5...,  ...,  0.1973,  0.7422, -0.3613]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<NativeLayerNormBackward0>)
key = tensor([[[-1.6719,  0.5508, -0.5508,  ..., -1.4062,  2.7188, -0.1992],
         [-0.2754,  1.3516,  0.1406,  ..., -1.5...,  ...,  0.1973,  0.7422, -0.3613]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<NativeLayerNormBackward0>)
value = tensor([[[-1.6719,  0.5508, -0.5508,  ..., -1.4062,  2.7188, -0.1992],
         [-0.2754,  1.3516,  0.1406,  ..., -1.5...,  ...,  0.1973,  0.7422, -0.3613]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<NativeLayerNormBackward0>)
embed_dim_to_check = 128, num_heads = 8
in_proj_weight = Parameter containing:
tensor([[ 1.4587e-02,  3.7598e-02, -7.9346e-03,  ..., -6.2256e-03,
          7.5340e-05,  1.5335....., -3.3936e-02,
         -1.2451e-02,  3.1281e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
in_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
bias_k = None, bias_v = None, add_zero_attn = False, dropout_p = 0.0
out_proj_weight = Parameter containing:
tensor([[-0.0037,  0.0415, -0.0242,  ...,  0.0052, -0.0132, -0.0171],
        [ 0.0056,  0.0354,... -0.0457,  0.0081,  ..., -0.0051, -0.0339,  0.0095]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
out_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ... 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
training = False, key_padding_mask = None, need_weights = False
attn_mask = tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],
          [0., 0., -inf,  ..., -inf, -inf, -inf],
          [0., 0...0., 0.,  ..., 0., 0., -inf],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
use_separate_proj_weight = False, q_proj_weight = None, k_proj_weight = None
v_proj_weight = None, static_k = None, static_v = None
average_attn_weights = True, is_causal = False

    def multi_head_attention_forward(
        query: Tensor,
        key: Tensor,
        value: Tensor,
        embed_dim_to_check: int,
        num_heads: int,
        in_proj_weight: Optional[Tensor],
        in_proj_bias: Optional[Tensor],
        bias_k: Optional[Tensor],
        bias_v: Optional[Tensor],
        add_zero_attn: bool,
        dropout_p: float,
        out_proj_weight: Tensor,
        out_proj_bias: Optional[Tensor],
        training: bool = True,
        key_padding_mask: Optional[Tensor] = None,
        need_weights: bool = True,
        attn_mask: Optional[Tensor] = None,
        use_separate_proj_weight: bool = False,
        q_proj_weight: Optional[Tensor] = None,
        k_proj_weight: Optional[Tensor] = None,
        v_proj_weight: Optional[Tensor] = None,
        static_k: Optional[Tensor] = None,
        static_v: Optional[Tensor] = None,
        average_attn_weights: bool = True,
        is_causal: bool = False,
    ) -> tuple[Tensor, Optional[Tensor]]:
        r"""Forward method for MultiHeadAttention.
    
        See :class:`torch.nn.MultiheadAttention` for details.
    
        Args:
            query, key, value: map a query and a set of key-value pairs to an output.
                See "Attention Is All You Need" for more details.
            embed_dim_to_check: total dimension of the model.
            num_heads: parallel attention heads.
            in_proj_weight, in_proj_bias: input projection weight and bias.
            bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
            add_zero_attn: add a new batch of zeros to the key and
                           value sequences at dim=1.
            dropout_p: probability of an element to be zeroed.
            out_proj_weight, out_proj_bias: the output projection weight and bias.
            training: apply dropout if is ``True``.
            key_padding_mask: if provided, specified padding elements in the key will
                be ignored by the attention. This is an binary mask. When the value is True,
                the corresponding value on the attention layer will be filled with -inf.
            need_weights: output attn_output_weights.
                Default: `True`
                Note: `needs_weight` defaults to `True`, but should be set to `False`
                For best performance when attention weights are not needed.
                *Setting needs_weights to `True`
                leads to a significant performance degradation.*
            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
                the batches while a 3D mask allows to specify a different mask for the entries of each batch.
            is_causal: If specified, applies a causal mask as attention mask, and ignores
                attn_mask for computing scaled dot product attention.
                Default: ``False``.
                .. warning::
                    is_causal is provides a hint that the attn_mask is the
                    causal mask.Providing incorrect hints can result in
                    incorrect execution, including forward and backward
                    compatibility.
            use_separate_proj_weight: the function accept the proj. weights for query, key,
                and value in different forms. If false, in_proj_weight will be used, which is
                a combination of q_proj_weight, k_proj_weight, v_proj_weight.
            q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
            static_k, static_v: static key and value used for attention operators.
            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.
                Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect
                when ``need_weights=True.``. Default: True
    
    
        Shape:
            Inputs:
            - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
              the embedding dimension.
            - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.
              If a FloatTensor is provided, it will be directly added to the value.
              If a BoolTensor is provided, the positions with the
              value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
            - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
              3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
              S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked
              positions. If a BoolTensor is provided, positions with ``True``
              are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
              is provided, it will be added to the attention weight.
            - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
            - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
    
            Outputs:
            - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
              E is the embedding dimension.
            - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns
              attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or
              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and
              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per
              head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.
        """
        tens_ops = (
            query,
            key,
            value,
            in_proj_weight,
            in_proj_bias,
            bias_k,
            bias_v,
            out_proj_weight,
            out_proj_bias,
        )
        if has_torch_function(tens_ops):
            return handle_torch_function(
                multi_head_attention_forward,
                tens_ops,
                query,
                key,
                value,
                embed_dim_to_check,
                num_heads,
                in_proj_weight,
                in_proj_bias,
                bias_k,
                bias_v,
                add_zero_attn,
                dropout_p,
                out_proj_weight,
                out_proj_bias,
                training=training,
                key_padding_mask=key_padding_mask,
                need_weights=need_weights,
                attn_mask=attn_mask,
                is_causal=is_causal,
                use_separate_proj_weight=use_separate_proj_weight,
                q_proj_weight=q_proj_weight,
                k_proj_weight=k_proj_weight,
                v_proj_weight=v_proj_weight,
                static_k=static_k,
                static_v=static_v,
                average_attn_weights=average_attn_weights,
            )
    
        is_batched = _mha_shape_check(
            query, key, value, key_padding_mask, attn_mask, num_heads
        )
    
        # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input
        # is batched, run the computation and before returning squeeze the
        # batch dimension so that the output doesn't carry this temporary batch dimension.
        if not is_batched:
            # unsqueeze if the input is unbatched
            query = query.unsqueeze(1)
            key = key.unsqueeze(1)
            value = value.unsqueeze(1)
            if key_padding_mask is not None:
                key_padding_mask = key_padding_mask.unsqueeze(0)
    
        # set up shape vars
        tgt_len, bsz, embed_dim = query.shape
        src_len, _, _ = key.shape
    
        key_padding_mask = _canonical_mask(
            mask=key_padding_mask,
            mask_name="key_padding_mask",
            other_type=_none_or_dtype(attn_mask),
            other_name="attn_mask",
            target_type=query.dtype,
        )
    
        if is_causal and attn_mask is None:
            raise RuntimeError(
                "Need attn_mask if specifying the is_causal hint. "
                "You may use the Transformer module method "
                "`generate_square_subsequent_mask` to create this mask."
            )
    
        if is_causal and key_padding_mask is None and not need_weights:
            # when we have a kpm or need weights, we need attn_mask
            # Otherwise, we use the is_causal hint go as is_causal
            # indicator to SDPA.
            attn_mask = None
        else:
            attn_mask = _canonical_mask(
                mask=attn_mask,
                mask_name="attn_mask",
                other_type=None,
                other_name="",
                target_type=query.dtype,
                check_other=False,
            )
    
            if key_padding_mask is not None:
                # We have the attn_mask, and use that to merge kpm into it.
                # Turn off use of is_causal hint, as the merged mask is no
                # longer causal.
                is_causal = False
    
        assert embed_dim == embed_dim_to_check, (
            f"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}"
        )
        if isinstance(embed_dim, torch.Tensor):
            # embed_dim can be a tensor when JIT tracing
            head_dim = embed_dim.div(num_heads, rounding_mode="trunc")
        else:
            head_dim = embed_dim // num_heads
        assert head_dim * num_heads == embed_dim, (
            f"embed_dim {embed_dim} not divisible by num_heads {num_heads}"
        )
        if use_separate_proj_weight:
            # allow MHA to have different embedding dimensions when separate projection weights are used
            assert key.shape[:2] == value.shape[:2], (
                f"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}"
            )
        else:
            assert key.shape == value.shape, (
                f"key shape {key.shape} does not match value shape {value.shape}"
            )
    
        #
        # compute in-projection
        #
        if not use_separate_proj_weight:
            assert in_proj_weight is not None, (
                "use_separate_proj_weight is False but in_proj_weight is None"
            )
            q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
        else:
            assert q_proj_weight is not None, (
                "use_separate_proj_weight is True but q_proj_weight is None"
            )
            assert k_proj_weight is not None, (
                "use_separate_proj_weight is True but k_proj_weight is None"
            )
            assert v_proj_weight is not None, (
                "use_separate_proj_weight is True but v_proj_weight is None"
            )
            if in_proj_bias is None:
                b_q = b_k = b_v = None
            else:
                b_q, b_k, b_v = in_proj_bias.chunk(3)
            q, k, v = _in_projection(
                query,
                key,
                value,
                q_proj_weight,
                k_proj_weight,
                v_proj_weight,
                b_q,
                b_k,
                b_v,
            )
    
        # prep attention mask
    
        if attn_mask is not None:
            # ensure attn_mask's dim is 3
            if attn_mask.dim() == 2:
                correct_2d_size = (tgt_len, src_len)
                if attn_mask.shape != correct_2d_size:
                    raise RuntimeError(
                        f"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}."
                    )
                attn_mask = attn_mask.unsqueeze(0)
            elif attn_mask.dim() == 3:
                correct_3d_size = (bsz * num_heads, tgt_len, src_len)
                if attn_mask.shape != correct_3d_size:
                    raise RuntimeError(
                        f"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}."
                    )
            else:
                raise RuntimeError(
                    f"attn_mask's dimension {attn_mask.dim()} is not supported"
                )
    
        # add bias along batch dimension (currently second)
        if bias_k is not None and bias_v is not None:
            assert static_k is None, "bias cannot be added to static key."
            assert static_v is None, "bias cannot be added to static value."
            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
        else:
            assert bias_k is None
            assert bias_v is None
    
        #
        # reshape q, k, v for multihead attention and make them batch first
        #
        q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
        if static_k is None:
            k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_k.size(0) == bsz * num_heads, (
                f"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}"
            )
            assert static_k.size(2) == head_dim, (
                f"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}"
            )
            k = static_k
        if static_v is None:
            v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_v.size(0) == bsz * num_heads, (
                f"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}"
            )
            assert static_v.size(2) == head_dim, (
                f"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}"
            )
            v = static_v
    
        # add zero attention along batch dimension (now first)
        if add_zero_attn:
            zero_attn_shape = (bsz * num_heads, 1, head_dim)
            k = torch.cat(
                [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1
            )
            v = torch.cat(
                [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1
            )
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
    
        # update source sequence length after adjustments
        src_len = k.size(1)
    
        # merge key padding and attention masks
        if key_padding_mask is not None:
            if not torch.jit.is_scripting() and not torch.jit.is_tracing():
                _check_key_padding_mask(key_padding_mask, src_len, bsz)
    
            key_padding_mask = (
                key_padding_mask.view(bsz, 1, 1, src_len)
                .expand(-1, num_heads, -1, -1)
                .reshape(bsz * num_heads, 1, src_len)
            )
            if attn_mask is None:
                attn_mask = key_padding_mask
            else:
                attn_mask = attn_mask + key_padding_mask
    
        # adjust dropout probability
        if not training:
            dropout_p = 0.0
    
        #
        # (deep breath) calculate attention and out projection
        #
    
        if need_weights:
            _B, _Nt, E = q.shape
            q_scaled = q * math.sqrt(1.0 / float(E))
    
            assert not (is_causal and attn_mask is None), (
                "FIXME: is_causal not implemented for need_weights"
            )
    
            if attn_mask is not None:
                attn_output_weights = torch.baddbmm(
                    attn_mask, q_scaled, k.transpose(-2, -1)
                )
            else:
                attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
            attn_output_weights = softmax(attn_output_weights, dim=-1)
            if dropout_p > 0.0:
                attn_output_weights = dropout(attn_output_weights, p=dropout_p)
    
            attn_output = torch.bmm(attn_output_weights, v)
    
            attn_output = (
                attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
            )
            attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
            attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
    
            # optionally average attention weights over heads
            attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
            if average_attn_weights:
                attn_output_weights = attn_output_weights.mean(dim=1)
    
            if not is_batched:
                # squeeze the output if input was unbatched
                attn_output = attn_output.squeeze(1)
                attn_output_weights = attn_output_weights.squeeze(0)
            return attn_output, attn_output_weights
        else:
            # attn_mask can be either (L,S) or (N*num_heads, L, S)
            # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)
            # in order to match the input for SDPA of (N, num_heads, L, S)
            if attn_mask is not None:
                if attn_mask.size(0) == 1 and attn_mask.dim() == 3:
                    attn_mask = attn_mask.unsqueeze(0)
                else:
                    attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)
    
            q = q.view(bsz, num_heads, tgt_len, head_dim)
            k = k.view(bsz, num_heads, src_len, head_dim)
            v = v.view(bsz, num_heads, src_len, head_dim)
    
>           attn_output = scaled_dot_product_attention(
                q, k, v, attn_mask, dropout_p, is_causal
            )
E           RuntimeError: cuDNN Frontend error: CUDNN_BACKEND_TENSOR_DESCRIPTOR cudnnFinalize failedptrDesc->finalize() cudnn_status: CUDNN_STATUS_SUBLIBRARY_LOADING_FAILED

/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6487: RuntimeError
___________________ test_gpt_accuracy[False-small-1-dtype1] ____________________

dtype = torch.float16, bs = 1, model = 'small', parallel_attention_mlp = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("parallel_attention_mlp", all_boolean)
    def test_gpt_accuracy(dtype, bs, model, parallel_attention_mlp):
        config = model_configs[model]
    
        te_gpt = TransformerLayer(
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            num_attention_heads=config.num_heads,
            layernorm_epsilon=config.eps,
            attention_dropout=0.1,
            hidden_dropout=0.1,
            params_dtype=dtype,
            fuse_qkv_params=True,
            qkv_weight_interleaved=False,
            parallel_attention_mlp=parallel_attention_mlp,
            device="cuda",
        ).eval()
    
        torch_gpt = (
            TorchGPT(
                config.hidden_size,
                config.eps,
                config.num_heads,
                parallel_attention_mlp=parallel_attention_mlp,
            )
            .to(dtype=dtype)
            .cuda()
            .eval()
        )
    
        # Share params
        with torch.no_grad():
            torch_gpt.ln.weight = Parameter(
                te_gpt.self_attention.layernorm_qkv.layer_norm_weight.clone()
            )
            torch_gpt.ln.bias = Parameter(te_gpt.self_attention.layernorm_qkv.layer_norm_bias.clone())
            torch_gpt.causal_attn.mhsa.in_proj_weight = Parameter(
                te_gpt.self_attention.layernorm_qkv.weight.clone()
            )
            torch_gpt.causal_attn.mhsa.in_proj_bias = Parameter(
                te_gpt.self_attention.layernorm_qkv.bias.clone()
            )
            torch_gpt.causal_attn.mhsa.out_proj.weight = Parameter(
                te_gpt.self_attention.proj.weight.clone()
            )
            torch_gpt.causal_attn.mhsa.out_proj.bias = Parameter(
                te_gpt.self_attention.proj.bias.clone()
            )
            torch_gpt.ln_mlp.ln.weight = Parameter(te_gpt.layernorm_mlp.layer_norm_weight.clone())
            torch_gpt.ln_mlp.ln.bias = Parameter(te_gpt.layernorm_mlp.layer_norm_bias.clone())
            torch_gpt.ln_mlp.fc1.weight = Parameter(te_gpt.layernorm_mlp.fc1_weight.clone())
            torch_gpt.ln_mlp.fc1.bias = Parameter(te_gpt.layernorm_mlp.fc1_bias.clone())
            torch_gpt.ln_mlp.fc2.weight = Parameter(te_gpt.layernorm_mlp.fc2_weight.clone())
            torch_gpt.ln_mlp.fc2.bias = Parameter(te_gpt.layernorm_mlp.fc2_bias.clone())
    
        te_outputs = _test_e2e_gpt_accuracy(te_gpt, bs, dtype, config)
>       torch_outputs = _test_e2e_gpt_accuracy(torch_gpt, bs, dtype, config)

tests/pytorch/test_numerics.py:987: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/test_numerics.py:916: in _test_e2e_gpt_accuracy
    out = block(inp_hidden_states, attention_mask=inp_attn_mask)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:551: in forward
    b = self.causal_attn(a, attention_mask)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:426: in forward
    output = self.mhsa(x, x, x, attn_mask=attention_mask, need_weights=False)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1488: in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = tensor([[[-1.6729,  0.5493, -0.5479,  ..., -1.4033,  2.7266, -0.1997]],

        [[-0.2756,  1.3506,  0.1406,  ..., -1...3,  ..., -1.1631, -0.6611, -1.6035]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)
key = tensor([[[-1.6729,  0.5493, -0.5479,  ..., -1.4033,  2.7266, -0.1997]],

        [[-0.2756,  1.3506,  0.1406,  ..., -1...3,  ..., -1.1631, -0.6611, -1.6035]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)
value = tensor([[[-1.6729,  0.5493, -0.5479,  ..., -1.4033,  2.7266, -0.1997]],

        [[-0.2756,  1.3506,  0.1406,  ..., -1...3,  ..., -1.1631, -0.6611, -1.6035]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)
embed_dim_to_check = 128, num_heads = 8
in_proj_weight = Parameter containing:
tensor([[ 1.4565e-02,  3.7628e-02, -7.9575e-03,  ..., -6.2141e-03,
          7.5221e-05,  1.5306......, -3.4058e-02,
         -1.2466e-02,  3.1300e-03]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
in_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
bias_k = None, bias_v = None, add_zero_attn = False, dropout_p = 0.0
out_proj_weight = Parameter containing:
tensor([[-0.0037,  0.0416, -0.0242,  ...,  0.0051, -0.0132, -0.0170],
        [ 0.0056,  0.0353,..., -0.0457,  0.0080,  ..., -0.0051, -0.0340,  0.0095]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
out_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ..., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
training = False, key_padding_mask = None, need_weights = False
attn_mask = tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],
          [0., 0., -inf,  ..., -inf, -inf, -inf],
          [0., 0... 0., 0.,  ..., 0., 0., -inf],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
use_separate_proj_weight = False, q_proj_weight = None, k_proj_weight = None
v_proj_weight = None, static_k = None, static_v = None
average_attn_weights = True, is_causal = False

    def multi_head_attention_forward(
        query: Tensor,
        key: Tensor,
        value: Tensor,
        embed_dim_to_check: int,
        num_heads: int,
        in_proj_weight: Optional[Tensor],
        in_proj_bias: Optional[Tensor],
        bias_k: Optional[Tensor],
        bias_v: Optional[Tensor],
        add_zero_attn: bool,
        dropout_p: float,
        out_proj_weight: Tensor,
        out_proj_bias: Optional[Tensor],
        training: bool = True,
        key_padding_mask: Optional[Tensor] = None,
        need_weights: bool = True,
        attn_mask: Optional[Tensor] = None,
        use_separate_proj_weight: bool = False,
        q_proj_weight: Optional[Tensor] = None,
        k_proj_weight: Optional[Tensor] = None,
        v_proj_weight: Optional[Tensor] = None,
        static_k: Optional[Tensor] = None,
        static_v: Optional[Tensor] = None,
        average_attn_weights: bool = True,
        is_causal: bool = False,
    ) -> tuple[Tensor, Optional[Tensor]]:
        r"""Forward method for MultiHeadAttention.
    
        See :class:`torch.nn.MultiheadAttention` for details.
    
        Args:
            query, key, value: map a query and a set of key-value pairs to an output.
                See "Attention Is All You Need" for more details.
            embed_dim_to_check: total dimension of the model.
            num_heads: parallel attention heads.
            in_proj_weight, in_proj_bias: input projection weight and bias.
            bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
            add_zero_attn: add a new batch of zeros to the key and
                           value sequences at dim=1.
            dropout_p: probability of an element to be zeroed.
            out_proj_weight, out_proj_bias: the output projection weight and bias.
            training: apply dropout if is ``True``.
            key_padding_mask: if provided, specified padding elements in the key will
                be ignored by the attention. This is an binary mask. When the value is True,
                the corresponding value on the attention layer will be filled with -inf.
            need_weights: output attn_output_weights.
                Default: `True`
                Note: `needs_weight` defaults to `True`, but should be set to `False`
                For best performance when attention weights are not needed.
                *Setting needs_weights to `True`
                leads to a significant performance degradation.*
            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
                the batches while a 3D mask allows to specify a different mask for the entries of each batch.
            is_causal: If specified, applies a causal mask as attention mask, and ignores
                attn_mask for computing scaled dot product attention.
                Default: ``False``.
                .. warning::
                    is_causal is provides a hint that the attn_mask is the
                    causal mask.Providing incorrect hints can result in
                    incorrect execution, including forward and backward
                    compatibility.
            use_separate_proj_weight: the function accept the proj. weights for query, key,
                and value in different forms. If false, in_proj_weight will be used, which is
                a combination of q_proj_weight, k_proj_weight, v_proj_weight.
            q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
            static_k, static_v: static key and value used for attention operators.
            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.
                Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect
                when ``need_weights=True.``. Default: True
    
    
        Shape:
            Inputs:
            - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
              the embedding dimension.
            - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.
              If a FloatTensor is provided, it will be directly added to the value.
              If a BoolTensor is provided, the positions with the
              value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
            - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
              3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
              S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked
              positions. If a BoolTensor is provided, positions with ``True``
              are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
              is provided, it will be added to the attention weight.
            - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
            - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
    
            Outputs:
            - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
              E is the embedding dimension.
            - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns
              attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or
              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and
              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per
              head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.
        """
        tens_ops = (
            query,
            key,
            value,
            in_proj_weight,
            in_proj_bias,
            bias_k,
            bias_v,
            out_proj_weight,
            out_proj_bias,
        )
        if has_torch_function(tens_ops):
            return handle_torch_function(
                multi_head_attention_forward,
                tens_ops,
                query,
                key,
                value,
                embed_dim_to_check,
                num_heads,
                in_proj_weight,
                in_proj_bias,
                bias_k,
                bias_v,
                add_zero_attn,
                dropout_p,
                out_proj_weight,
                out_proj_bias,
                training=training,
                key_padding_mask=key_padding_mask,
                need_weights=need_weights,
                attn_mask=attn_mask,
                is_causal=is_causal,
                use_separate_proj_weight=use_separate_proj_weight,
                q_proj_weight=q_proj_weight,
                k_proj_weight=k_proj_weight,
                v_proj_weight=v_proj_weight,
                static_k=static_k,
                static_v=static_v,
                average_attn_weights=average_attn_weights,
            )
    
        is_batched = _mha_shape_check(
            query, key, value, key_padding_mask, attn_mask, num_heads
        )
    
        # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input
        # is batched, run the computation and before returning squeeze the
        # batch dimension so that the output doesn't carry this temporary batch dimension.
        if not is_batched:
            # unsqueeze if the input is unbatched
            query = query.unsqueeze(1)
            key = key.unsqueeze(1)
            value = value.unsqueeze(1)
            if key_padding_mask is not None:
                key_padding_mask = key_padding_mask.unsqueeze(0)
    
        # set up shape vars
        tgt_len, bsz, embed_dim = query.shape
        src_len, _, _ = key.shape
    
        key_padding_mask = _canonical_mask(
            mask=key_padding_mask,
            mask_name="key_padding_mask",
            other_type=_none_or_dtype(attn_mask),
            other_name="attn_mask",
            target_type=query.dtype,
        )
    
        if is_causal and attn_mask is None:
            raise RuntimeError(
                "Need attn_mask if specifying the is_causal hint. "
                "You may use the Transformer module method "
                "`generate_square_subsequent_mask` to create this mask."
            )
    
        if is_causal and key_padding_mask is None and not need_weights:
            # when we have a kpm or need weights, we need attn_mask
            # Otherwise, we use the is_causal hint go as is_causal
            # indicator to SDPA.
            attn_mask = None
        else:
            attn_mask = _canonical_mask(
                mask=attn_mask,
                mask_name="attn_mask",
                other_type=None,
                other_name="",
                target_type=query.dtype,
                check_other=False,
            )
    
            if key_padding_mask is not None:
                # We have the attn_mask, and use that to merge kpm into it.
                # Turn off use of is_causal hint, as the merged mask is no
                # longer causal.
                is_causal = False
    
        assert embed_dim == embed_dim_to_check, (
            f"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}"
        )
        if isinstance(embed_dim, torch.Tensor):
            # embed_dim can be a tensor when JIT tracing
            head_dim = embed_dim.div(num_heads, rounding_mode="trunc")
        else:
            head_dim = embed_dim // num_heads
        assert head_dim * num_heads == embed_dim, (
            f"embed_dim {embed_dim} not divisible by num_heads {num_heads}"
        )
        if use_separate_proj_weight:
            # allow MHA to have different embedding dimensions when separate projection weights are used
            assert key.shape[:2] == value.shape[:2], (
                f"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}"
            )
        else:
            assert key.shape == value.shape, (
                f"key shape {key.shape} does not match value shape {value.shape}"
            )
    
        #
        # compute in-projection
        #
        if not use_separate_proj_weight:
            assert in_proj_weight is not None, (
                "use_separate_proj_weight is False but in_proj_weight is None"
            )
            q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
        else:
            assert q_proj_weight is not None, (
                "use_separate_proj_weight is True but q_proj_weight is None"
            )
            assert k_proj_weight is not None, (
                "use_separate_proj_weight is True but k_proj_weight is None"
            )
            assert v_proj_weight is not None, (
                "use_separate_proj_weight is True but v_proj_weight is None"
            )
            if in_proj_bias is None:
                b_q = b_k = b_v = None
            else:
                b_q, b_k, b_v = in_proj_bias.chunk(3)
            q, k, v = _in_projection(
                query,
                key,
                value,
                q_proj_weight,
                k_proj_weight,
                v_proj_weight,
                b_q,
                b_k,
                b_v,
            )
    
        # prep attention mask
    
        if attn_mask is not None:
            # ensure attn_mask's dim is 3
            if attn_mask.dim() == 2:
                correct_2d_size = (tgt_len, src_len)
                if attn_mask.shape != correct_2d_size:
                    raise RuntimeError(
                        f"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}."
                    )
                attn_mask = attn_mask.unsqueeze(0)
            elif attn_mask.dim() == 3:
                correct_3d_size = (bsz * num_heads, tgt_len, src_len)
                if attn_mask.shape != correct_3d_size:
                    raise RuntimeError(
                        f"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}."
                    )
            else:
                raise RuntimeError(
                    f"attn_mask's dimension {attn_mask.dim()} is not supported"
                )
    
        # add bias along batch dimension (currently second)
        if bias_k is not None and bias_v is not None:
            assert static_k is None, "bias cannot be added to static key."
            assert static_v is None, "bias cannot be added to static value."
            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
        else:
            assert bias_k is None
            assert bias_v is None
    
        #
        # reshape q, k, v for multihead attention and make them batch first
        #
        q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
        if static_k is None:
            k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_k.size(0) == bsz * num_heads, (
                f"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}"
            )
            assert static_k.size(2) == head_dim, (
                f"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}"
            )
            k = static_k
        if static_v is None:
            v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_v.size(0) == bsz * num_heads, (
                f"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}"
            )
            assert static_v.size(2) == head_dim, (
                f"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}"
            )
            v = static_v
    
        # add zero attention along batch dimension (now first)
        if add_zero_attn:
            zero_attn_shape = (bsz * num_heads, 1, head_dim)
            k = torch.cat(
                [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1
            )
            v = torch.cat(
                [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1
            )
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
    
        # update source sequence length after adjustments
        src_len = k.size(1)
    
        # merge key padding and attention masks
        if key_padding_mask is not None:
            if not torch.jit.is_scripting() and not torch.jit.is_tracing():
                _check_key_padding_mask(key_padding_mask, src_len, bsz)
    
            key_padding_mask = (
                key_padding_mask.view(bsz, 1, 1, src_len)
                .expand(-1, num_heads, -1, -1)
                .reshape(bsz * num_heads, 1, src_len)
            )
            if attn_mask is None:
                attn_mask = key_padding_mask
            else:
                attn_mask = attn_mask + key_padding_mask
    
        # adjust dropout probability
        if not training:
            dropout_p = 0.0
    
        #
        # (deep breath) calculate attention and out projection
        #
    
        if need_weights:
            _B, _Nt, E = q.shape
            q_scaled = q * math.sqrt(1.0 / float(E))
    
            assert not (is_causal and attn_mask is None), (
                "FIXME: is_causal not implemented for need_weights"
            )
    
            if attn_mask is not None:
                attn_output_weights = torch.baddbmm(
                    attn_mask, q_scaled, k.transpose(-2, -1)
                )
            else:
                attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
            attn_output_weights = softmax(attn_output_weights, dim=-1)
            if dropout_p > 0.0:
                attn_output_weights = dropout(attn_output_weights, p=dropout_p)
    
            attn_output = torch.bmm(attn_output_weights, v)
    
            attn_output = (
                attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
            )
            attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
            attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
    
            # optionally average attention weights over heads
            attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
            if average_attn_weights:
                attn_output_weights = attn_output_weights.mean(dim=1)
    
            if not is_batched:
                # squeeze the output if input was unbatched
                attn_output = attn_output.squeeze(1)
                attn_output_weights = attn_output_weights.squeeze(0)
            return attn_output, attn_output_weights
        else:
            # attn_mask can be either (L,S) or (N*num_heads, L, S)
            # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)
            # in order to match the input for SDPA of (N, num_heads, L, S)
            if attn_mask is not None:
                if attn_mask.size(0) == 1 and attn_mask.dim() == 3:
                    attn_mask = attn_mask.unsqueeze(0)
                else:
                    attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)
    
            q = q.view(bsz, num_heads, tgt_len, head_dim)
            k = k.view(bsz, num_heads, src_len, head_dim)
            v = v.view(bsz, num_heads, src_len, head_dim)
    
>           attn_output = scaled_dot_product_attention(
                q, k, v, attn_mask, dropout_p, is_causal
            )
E           RuntimeError: cuDNN Frontend error: CUDNN_BACKEND_TENSOR_DESCRIPTOR cudnnFinalize failedptrDesc->finalize() cudnn_status: CUDNN_STATUS_SUBLIBRARY_LOADING_FAILED

/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6487: RuntimeError
___________________ test_gpt_accuracy[False-small-1-dtype2] ____________________

dtype = torch.bfloat16, bs = 1, model = 'small', parallel_attention_mlp = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("parallel_attention_mlp", all_boolean)
    def test_gpt_accuracy(dtype, bs, model, parallel_attention_mlp):
        config = model_configs[model]
    
        te_gpt = TransformerLayer(
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            num_attention_heads=config.num_heads,
            layernorm_epsilon=config.eps,
            attention_dropout=0.1,
            hidden_dropout=0.1,
            params_dtype=dtype,
            fuse_qkv_params=True,
            qkv_weight_interleaved=False,
            parallel_attention_mlp=parallel_attention_mlp,
            device="cuda",
        ).eval()
    
        torch_gpt = (
            TorchGPT(
                config.hidden_size,
                config.eps,
                config.num_heads,
                parallel_attention_mlp=parallel_attention_mlp,
            )
            .to(dtype=dtype)
            .cuda()
            .eval()
        )
    
        # Share params
        with torch.no_grad():
            torch_gpt.ln.weight = Parameter(
                te_gpt.self_attention.layernorm_qkv.layer_norm_weight.clone()
            )
            torch_gpt.ln.bias = Parameter(te_gpt.self_attention.layernorm_qkv.layer_norm_bias.clone())
            torch_gpt.causal_attn.mhsa.in_proj_weight = Parameter(
                te_gpt.self_attention.layernorm_qkv.weight.clone()
            )
            torch_gpt.causal_attn.mhsa.in_proj_bias = Parameter(
                te_gpt.self_attention.layernorm_qkv.bias.clone()
            )
            torch_gpt.causal_attn.mhsa.out_proj.weight = Parameter(
                te_gpt.self_attention.proj.weight.clone()
            )
            torch_gpt.causal_attn.mhsa.out_proj.bias = Parameter(
                te_gpt.self_attention.proj.bias.clone()
            )
            torch_gpt.ln_mlp.ln.weight = Parameter(te_gpt.layernorm_mlp.layer_norm_weight.clone())
            torch_gpt.ln_mlp.ln.bias = Parameter(te_gpt.layernorm_mlp.layer_norm_bias.clone())
            torch_gpt.ln_mlp.fc1.weight = Parameter(te_gpt.layernorm_mlp.fc1_weight.clone())
            torch_gpt.ln_mlp.fc1.bias = Parameter(te_gpt.layernorm_mlp.fc1_bias.clone())
            torch_gpt.ln_mlp.fc2.weight = Parameter(te_gpt.layernorm_mlp.fc2_weight.clone())
            torch_gpt.ln_mlp.fc2.bias = Parameter(te_gpt.layernorm_mlp.fc2_bias.clone())
    
        te_outputs = _test_e2e_gpt_accuracy(te_gpt, bs, dtype, config)
>       torch_outputs = _test_e2e_gpt_accuracy(torch_gpt, bs, dtype, config)

tests/pytorch/test_numerics.py:987: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/test_numerics.py:916: in _test_e2e_gpt_accuracy
    out = block(inp_hidden_states, attention_mask=inp_attn_mask)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:551: in forward
    b = self.causal_attn(a, attention_mask)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:426: in forward
    output = self.mhsa(x, x, x, attn_mask=attention_mask, need_weights=False)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1488: in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = tensor([[[-1.6719,  0.5508, -0.5508,  ..., -1.4062,  2.7188, -0.1992]],

        [[-0.2754,  1.3516,  0.1406,  ..., -1...,  ..., -1.1641, -0.6641, -1.6016]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<NativeLayerNormBackward0>)
key = tensor([[[-1.6719,  0.5508, -0.5508,  ..., -1.4062,  2.7188, -0.1992]],

        [[-0.2754,  1.3516,  0.1406,  ..., -1...,  ..., -1.1641, -0.6641, -1.6016]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<NativeLayerNormBackward0>)
value = tensor([[[-1.6719,  0.5508, -0.5508,  ..., -1.4062,  2.7188, -0.1992]],

        [[-0.2754,  1.3516,  0.1406,  ..., -1...,  ..., -1.1641, -0.6641, -1.6016]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<NativeLayerNormBackward0>)
embed_dim_to_check = 128, num_heads = 8
in_proj_weight = Parameter containing:
tensor([[ 1.4587e-02,  3.7598e-02, -7.9346e-03,  ..., -6.2256e-03,
          7.5340e-05,  1.5335....., -3.3936e-02,
         -1.2451e-02,  3.1281e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
in_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
bias_k = None, bias_v = None, add_zero_attn = False, dropout_p = 0.0
out_proj_weight = Parameter containing:
tensor([[-0.0037,  0.0415, -0.0242,  ...,  0.0052, -0.0132, -0.0171],
        [ 0.0056,  0.0354,... -0.0457,  0.0081,  ..., -0.0051, -0.0339,  0.0095]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
out_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ... 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
training = False, key_padding_mask = None, need_weights = False
attn_mask = tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],
          [0., 0., -inf,  ..., -inf, -inf, -inf],
          [0., 0...0., 0.,  ..., 0., 0., -inf],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
use_separate_proj_weight = False, q_proj_weight = None, k_proj_weight = None
v_proj_weight = None, static_k = None, static_v = None
average_attn_weights = True, is_causal = False

    def multi_head_attention_forward(
        query: Tensor,
        key: Tensor,
        value: Tensor,
        embed_dim_to_check: int,
        num_heads: int,
        in_proj_weight: Optional[Tensor],
        in_proj_bias: Optional[Tensor],
        bias_k: Optional[Tensor],
        bias_v: Optional[Tensor],
        add_zero_attn: bool,
        dropout_p: float,
        out_proj_weight: Tensor,
        out_proj_bias: Optional[Tensor],
        training: bool = True,
        key_padding_mask: Optional[Tensor] = None,
        need_weights: bool = True,
        attn_mask: Optional[Tensor] = None,
        use_separate_proj_weight: bool = False,
        q_proj_weight: Optional[Tensor] = None,
        k_proj_weight: Optional[Tensor] = None,
        v_proj_weight: Optional[Tensor] = None,
        static_k: Optional[Tensor] = None,
        static_v: Optional[Tensor] = None,
        average_attn_weights: bool = True,
        is_causal: bool = False,
    ) -> tuple[Tensor, Optional[Tensor]]:
        r"""Forward method for MultiHeadAttention.
    
        See :class:`torch.nn.MultiheadAttention` for details.
    
        Args:
            query, key, value: map a query and a set of key-value pairs to an output.
                See "Attention Is All You Need" for more details.
            embed_dim_to_check: total dimension of the model.
            num_heads: parallel attention heads.
            in_proj_weight, in_proj_bias: input projection weight and bias.
            bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
            add_zero_attn: add a new batch of zeros to the key and
                           value sequences at dim=1.
            dropout_p: probability of an element to be zeroed.
            out_proj_weight, out_proj_bias: the output projection weight and bias.
            training: apply dropout if is ``True``.
            key_padding_mask: if provided, specified padding elements in the key will
                be ignored by the attention. This is an binary mask. When the value is True,
                the corresponding value on the attention layer will be filled with -inf.
            need_weights: output attn_output_weights.
                Default: `True`
                Note: `needs_weight` defaults to `True`, but should be set to `False`
                For best performance when attention weights are not needed.
                *Setting needs_weights to `True`
                leads to a significant performance degradation.*
            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
                the batches while a 3D mask allows to specify a different mask for the entries of each batch.
            is_causal: If specified, applies a causal mask as attention mask, and ignores
                attn_mask for computing scaled dot product attention.
                Default: ``False``.
                .. warning::
                    is_causal is provides a hint that the attn_mask is the
                    causal mask.Providing incorrect hints can result in
                    incorrect execution, including forward and backward
                    compatibility.
            use_separate_proj_weight: the function accept the proj. weights for query, key,
                and value in different forms. If false, in_proj_weight will be used, which is
                a combination of q_proj_weight, k_proj_weight, v_proj_weight.
            q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
            static_k, static_v: static key and value used for attention operators.
            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.
                Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect
                when ``need_weights=True.``. Default: True
    
    
        Shape:
            Inputs:
            - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
              the embedding dimension.
            - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.
              If a FloatTensor is provided, it will be directly added to the value.
              If a BoolTensor is provided, the positions with the
              value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
            - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
              3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
              S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked
              positions. If a BoolTensor is provided, positions with ``True``
              are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
              is provided, it will be added to the attention weight.
            - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
            - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
    
            Outputs:
            - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
              E is the embedding dimension.
            - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns
              attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or
              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and
              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per
              head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.
        """
        tens_ops = (
            query,
            key,
            value,
            in_proj_weight,
            in_proj_bias,
            bias_k,
            bias_v,
            out_proj_weight,
            out_proj_bias,
        )
        if has_torch_function(tens_ops):
            return handle_torch_function(
                multi_head_attention_forward,
                tens_ops,
                query,
                key,
                value,
                embed_dim_to_check,
                num_heads,
                in_proj_weight,
                in_proj_bias,
                bias_k,
                bias_v,
                add_zero_attn,
                dropout_p,
                out_proj_weight,
                out_proj_bias,
                training=training,
                key_padding_mask=key_padding_mask,
                need_weights=need_weights,
                attn_mask=attn_mask,
                is_causal=is_causal,
                use_separate_proj_weight=use_separate_proj_weight,
                q_proj_weight=q_proj_weight,
                k_proj_weight=k_proj_weight,
                v_proj_weight=v_proj_weight,
                static_k=static_k,
                static_v=static_v,
                average_attn_weights=average_attn_weights,
            )
    
        is_batched = _mha_shape_check(
            query, key, value, key_padding_mask, attn_mask, num_heads
        )
    
        # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input
        # is batched, run the computation and before returning squeeze the
        # batch dimension so that the output doesn't carry this temporary batch dimension.
        if not is_batched:
            # unsqueeze if the input is unbatched
            query = query.unsqueeze(1)
            key = key.unsqueeze(1)
            value = value.unsqueeze(1)
            if key_padding_mask is not None:
                key_padding_mask = key_padding_mask.unsqueeze(0)
    
        # set up shape vars
        tgt_len, bsz, embed_dim = query.shape
        src_len, _, _ = key.shape
    
        key_padding_mask = _canonical_mask(
            mask=key_padding_mask,
            mask_name="key_padding_mask",
            other_type=_none_or_dtype(attn_mask),
            other_name="attn_mask",
            target_type=query.dtype,
        )
    
        if is_causal and attn_mask is None:
            raise RuntimeError(
                "Need attn_mask if specifying the is_causal hint. "
                "You may use the Transformer module method "
                "`generate_square_subsequent_mask` to create this mask."
            )
    
        if is_causal and key_padding_mask is None and not need_weights:
            # when we have a kpm or need weights, we need attn_mask
            # Otherwise, we use the is_causal hint go as is_causal
            # indicator to SDPA.
            attn_mask = None
        else:
            attn_mask = _canonical_mask(
                mask=attn_mask,
                mask_name="attn_mask",
                other_type=None,
                other_name="",
                target_type=query.dtype,
                check_other=False,
            )
    
            if key_padding_mask is not None:
                # We have the attn_mask, and use that to merge kpm into it.
                # Turn off use of is_causal hint, as the merged mask is no
                # longer causal.
                is_causal = False
    
        assert embed_dim == embed_dim_to_check, (
            f"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}"
        )
        if isinstance(embed_dim, torch.Tensor):
            # embed_dim can be a tensor when JIT tracing
            head_dim = embed_dim.div(num_heads, rounding_mode="trunc")
        else:
            head_dim = embed_dim // num_heads
        assert head_dim * num_heads == embed_dim, (
            f"embed_dim {embed_dim} not divisible by num_heads {num_heads}"
        )
        if use_separate_proj_weight:
            # allow MHA to have different embedding dimensions when separate projection weights are used
            assert key.shape[:2] == value.shape[:2], (
                f"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}"
            )
        else:
            assert key.shape == value.shape, (
                f"key shape {key.shape} does not match value shape {value.shape}"
            )
    
        #
        # compute in-projection
        #
        if not use_separate_proj_weight:
            assert in_proj_weight is not None, (
                "use_separate_proj_weight is False but in_proj_weight is None"
            )
            q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
        else:
            assert q_proj_weight is not None, (
                "use_separate_proj_weight is True but q_proj_weight is None"
            )
            assert k_proj_weight is not None, (
                "use_separate_proj_weight is True but k_proj_weight is None"
            )
            assert v_proj_weight is not None, (
                "use_separate_proj_weight is True but v_proj_weight is None"
            )
            if in_proj_bias is None:
                b_q = b_k = b_v = None
            else:
                b_q, b_k, b_v = in_proj_bias.chunk(3)
            q, k, v = _in_projection(
                query,
                key,
                value,
                q_proj_weight,
                k_proj_weight,
                v_proj_weight,
                b_q,
                b_k,
                b_v,
            )
    
        # prep attention mask
    
        if attn_mask is not None:
            # ensure attn_mask's dim is 3
            if attn_mask.dim() == 2:
                correct_2d_size = (tgt_len, src_len)
                if attn_mask.shape != correct_2d_size:
                    raise RuntimeError(
                        f"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}."
                    )
                attn_mask = attn_mask.unsqueeze(0)
            elif attn_mask.dim() == 3:
                correct_3d_size = (bsz * num_heads, tgt_len, src_len)
                if attn_mask.shape != correct_3d_size:
                    raise RuntimeError(
                        f"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}."
                    )
            else:
                raise RuntimeError(
                    f"attn_mask's dimension {attn_mask.dim()} is not supported"
                )
    
        # add bias along batch dimension (currently second)
        if bias_k is not None and bias_v is not None:
            assert static_k is None, "bias cannot be added to static key."
            assert static_v is None, "bias cannot be added to static value."
            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
        else:
            assert bias_k is None
            assert bias_v is None
    
        #
        # reshape q, k, v for multihead attention and make them batch first
        #
        q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
        if static_k is None:
            k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_k.size(0) == bsz * num_heads, (
                f"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}"
            )
            assert static_k.size(2) == head_dim, (
                f"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}"
            )
            k = static_k
        if static_v is None:
            v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_v.size(0) == bsz * num_heads, (
                f"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}"
            )
            assert static_v.size(2) == head_dim, (
                f"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}"
            )
            v = static_v
    
        # add zero attention along batch dimension (now first)
        if add_zero_attn:
            zero_attn_shape = (bsz * num_heads, 1, head_dim)
            k = torch.cat(
                [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1
            )
            v = torch.cat(
                [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1
            )
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
    
        # update source sequence length after adjustments
        src_len = k.size(1)
    
        # merge key padding and attention masks
        if key_padding_mask is not None:
            if not torch.jit.is_scripting() and not torch.jit.is_tracing():
                _check_key_padding_mask(key_padding_mask, src_len, bsz)
    
            key_padding_mask = (
                key_padding_mask.view(bsz, 1, 1, src_len)
                .expand(-1, num_heads, -1, -1)
                .reshape(bsz * num_heads, 1, src_len)
            )
            if attn_mask is None:
                attn_mask = key_padding_mask
            else:
                attn_mask = attn_mask + key_padding_mask
    
        # adjust dropout probability
        if not training:
            dropout_p = 0.0
    
        #
        # (deep breath) calculate attention and out projection
        #
    
        if need_weights:
            _B, _Nt, E = q.shape
            q_scaled = q * math.sqrt(1.0 / float(E))
    
            assert not (is_causal and attn_mask is None), (
                "FIXME: is_causal not implemented for need_weights"
            )
    
            if attn_mask is not None:
                attn_output_weights = torch.baddbmm(
                    attn_mask, q_scaled, k.transpose(-2, -1)
                )
            else:
                attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
            attn_output_weights = softmax(attn_output_weights, dim=-1)
            if dropout_p > 0.0:
                attn_output_weights = dropout(attn_output_weights, p=dropout_p)
    
            attn_output = torch.bmm(attn_output_weights, v)
    
            attn_output = (
                attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
            )
            attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
            attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
    
            # optionally average attention weights over heads
            attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
            if average_attn_weights:
                attn_output_weights = attn_output_weights.mean(dim=1)
    
            if not is_batched:
                # squeeze the output if input was unbatched
                attn_output = attn_output.squeeze(1)
                attn_output_weights = attn_output_weights.squeeze(0)
            return attn_output, attn_output_weights
        else:
            # attn_mask can be either (L,S) or (N*num_heads, L, S)
            # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)
            # in order to match the input for SDPA of (N, num_heads, L, S)
            if attn_mask is not None:
                if attn_mask.size(0) == 1 and attn_mask.dim() == 3:
                    attn_mask = attn_mask.unsqueeze(0)
                else:
                    attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)
    
            q = q.view(bsz, num_heads, tgt_len, head_dim)
            k = k.view(bsz, num_heads, src_len, head_dim)
            v = v.view(bsz, num_heads, src_len, head_dim)
    
>           attn_output = scaled_dot_product_attention(
                q, k, v, attn_mask, dropout_p, is_causal
            )
E           RuntimeError: cuDNN Frontend error: CUDNN_BACKEND_TENSOR_DESCRIPTOR cudnnFinalize failedptrDesc->finalize() cudnn_status: CUDNN_STATUS_SUBLIBRARY_LOADING_FAILED

/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6487: RuntimeError
___________________ test_gpt_accuracy[False-small-2-dtype1] ____________________

dtype = torch.float16, bs = 2, model = 'small', parallel_attention_mlp = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("parallel_attention_mlp", all_boolean)
    def test_gpt_accuracy(dtype, bs, model, parallel_attention_mlp):
        config = model_configs[model]
    
        te_gpt = TransformerLayer(
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            num_attention_heads=config.num_heads,
            layernorm_epsilon=config.eps,
            attention_dropout=0.1,
            hidden_dropout=0.1,
            params_dtype=dtype,
            fuse_qkv_params=True,
            qkv_weight_interleaved=False,
            parallel_attention_mlp=parallel_attention_mlp,
            device="cuda",
        ).eval()
    
        torch_gpt = (
            TorchGPT(
                config.hidden_size,
                config.eps,
                config.num_heads,
                parallel_attention_mlp=parallel_attention_mlp,
            )
            .to(dtype=dtype)
            .cuda()
            .eval()
        )
    
        # Share params
        with torch.no_grad():
            torch_gpt.ln.weight = Parameter(
                te_gpt.self_attention.layernorm_qkv.layer_norm_weight.clone()
            )
            torch_gpt.ln.bias = Parameter(te_gpt.self_attention.layernorm_qkv.layer_norm_bias.clone())
            torch_gpt.causal_attn.mhsa.in_proj_weight = Parameter(
                te_gpt.self_attention.layernorm_qkv.weight.clone()
            )
            torch_gpt.causal_attn.mhsa.in_proj_bias = Parameter(
                te_gpt.self_attention.layernorm_qkv.bias.clone()
            )
            torch_gpt.causal_attn.mhsa.out_proj.weight = Parameter(
                te_gpt.self_attention.proj.weight.clone()
            )
            torch_gpt.causal_attn.mhsa.out_proj.bias = Parameter(
                te_gpt.self_attention.proj.bias.clone()
            )
            torch_gpt.ln_mlp.ln.weight = Parameter(te_gpt.layernorm_mlp.layer_norm_weight.clone())
            torch_gpt.ln_mlp.ln.bias = Parameter(te_gpt.layernorm_mlp.layer_norm_bias.clone())
            torch_gpt.ln_mlp.fc1.weight = Parameter(te_gpt.layernorm_mlp.fc1_weight.clone())
            torch_gpt.ln_mlp.fc1.bias = Parameter(te_gpt.layernorm_mlp.fc1_bias.clone())
            torch_gpt.ln_mlp.fc2.weight = Parameter(te_gpt.layernorm_mlp.fc2_weight.clone())
            torch_gpt.ln_mlp.fc2.bias = Parameter(te_gpt.layernorm_mlp.fc2_bias.clone())
    
        te_outputs = _test_e2e_gpt_accuracy(te_gpt, bs, dtype, config)
>       torch_outputs = _test_e2e_gpt_accuracy(torch_gpt, bs, dtype, config)

tests/pytorch/test_numerics.py:987: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/test_numerics.py:916: in _test_e2e_gpt_accuracy
    out = block(inp_hidden_states, attention_mask=inp_attn_mask)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:551: in forward
    b = self.causal_attn(a, attention_mask)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:426: in forward
    output = self.mhsa(x, x, x, attn_mask=attention_mask, need_weights=False)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1488: in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = tensor([[[-1.6729,  0.5493, -0.5479,  ..., -1.4033,  2.7266, -0.1997],
         [-0.2756,  1.3506,  0.1406,  ..., -1.5...0,  ...,  0.1979,  0.7422, -0.3604]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)
key = tensor([[[-1.6729,  0.5493, -0.5479,  ..., -1.4033,  2.7266, -0.1997],
         [-0.2756,  1.3506,  0.1406,  ..., -1.5...0,  ...,  0.1979,  0.7422, -0.3604]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)
value = tensor([[[-1.6729,  0.5493, -0.5479,  ..., -1.4033,  2.7266, -0.1997],
         [-0.2756,  1.3506,  0.1406,  ..., -1.5...0,  ...,  0.1979,  0.7422, -0.3604]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)
embed_dim_to_check = 128, num_heads = 8
in_proj_weight = Parameter containing:
tensor([[ 1.4565e-02,  3.7628e-02, -7.9575e-03,  ..., -6.2141e-03,
          7.5221e-05,  1.5306......, -3.4058e-02,
         -1.2466e-02,  3.1300e-03]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
in_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
bias_k = None, bias_v = None, add_zero_attn = False, dropout_p = 0.0
out_proj_weight = Parameter containing:
tensor([[-0.0037,  0.0416, -0.0242,  ...,  0.0051, -0.0132, -0.0170],
        [ 0.0056,  0.0353,..., -0.0457,  0.0080,  ..., -0.0051, -0.0340,  0.0095]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
out_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ..., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
training = False, key_padding_mask = None, need_weights = False
attn_mask = tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],
          [0., 0., -inf,  ..., -inf, -inf, -inf],
          [0., 0... 0., 0.,  ..., 0., 0., -inf],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
use_separate_proj_weight = False, q_proj_weight = None, k_proj_weight = None
v_proj_weight = None, static_k = None, static_v = None
average_attn_weights = True, is_causal = False

    def multi_head_attention_forward(
        query: Tensor,
        key: Tensor,
        value: Tensor,
        embed_dim_to_check: int,
        num_heads: int,
        in_proj_weight: Optional[Tensor],
        in_proj_bias: Optional[Tensor],
        bias_k: Optional[Tensor],
        bias_v: Optional[Tensor],
        add_zero_attn: bool,
        dropout_p: float,
        out_proj_weight: Tensor,
        out_proj_bias: Optional[Tensor],
        training: bool = True,
        key_padding_mask: Optional[Tensor] = None,
        need_weights: bool = True,
        attn_mask: Optional[Tensor] = None,
        use_separate_proj_weight: bool = False,
        q_proj_weight: Optional[Tensor] = None,
        k_proj_weight: Optional[Tensor] = None,
        v_proj_weight: Optional[Tensor] = None,
        static_k: Optional[Tensor] = None,
        static_v: Optional[Tensor] = None,
        average_attn_weights: bool = True,
        is_causal: bool = False,
    ) -> tuple[Tensor, Optional[Tensor]]:
        r"""Forward method for MultiHeadAttention.
    
        See :class:`torch.nn.MultiheadAttention` for details.
    
        Args:
            query, key, value: map a query and a set of key-value pairs to an output.
                See "Attention Is All You Need" for more details.
            embed_dim_to_check: total dimension of the model.
            num_heads: parallel attention heads.
            in_proj_weight, in_proj_bias: input projection weight and bias.
            bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
            add_zero_attn: add a new batch of zeros to the key and
                           value sequences at dim=1.
            dropout_p: probability of an element to be zeroed.
            out_proj_weight, out_proj_bias: the output projection weight and bias.
            training: apply dropout if is ``True``.
            key_padding_mask: if provided, specified padding elements in the key will
                be ignored by the attention. This is an binary mask. When the value is True,
                the corresponding value on the attention layer will be filled with -inf.
            need_weights: output attn_output_weights.
                Default: `True`
                Note: `needs_weight` defaults to `True`, but should be set to `False`
                For best performance when attention weights are not needed.
                *Setting needs_weights to `True`
                leads to a significant performance degradation.*
            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
                the batches while a 3D mask allows to specify a different mask for the entries of each batch.
            is_causal: If specified, applies a causal mask as attention mask, and ignores
                attn_mask for computing scaled dot product attention.
                Default: ``False``.
                .. warning::
                    is_causal is provides a hint that the attn_mask is the
                    causal mask.Providing incorrect hints can result in
                    incorrect execution, including forward and backward
                    compatibility.
            use_separate_proj_weight: the function accept the proj. weights for query, key,
                and value in different forms. If false, in_proj_weight will be used, which is
                a combination of q_proj_weight, k_proj_weight, v_proj_weight.
            q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
            static_k, static_v: static key and value used for attention operators.
            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.
                Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect
                when ``need_weights=True.``. Default: True
    
    
        Shape:
            Inputs:
            - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
              the embedding dimension.
            - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.
              If a FloatTensor is provided, it will be directly added to the value.
              If a BoolTensor is provided, the positions with the
              value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
            - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
              3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
              S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked
              positions. If a BoolTensor is provided, positions with ``True``
              are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
              is provided, it will be added to the attention weight.
            - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
            - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
    
            Outputs:
            - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
              E is the embedding dimension.
            - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns
              attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or
              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and
              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per
              head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.
        """
        tens_ops = (
            query,
            key,
            value,
            in_proj_weight,
            in_proj_bias,
            bias_k,
            bias_v,
            out_proj_weight,
            out_proj_bias,
        )
        if has_torch_function(tens_ops):
            return handle_torch_function(
                multi_head_attention_forward,
                tens_ops,
                query,
                key,
                value,
                embed_dim_to_check,
                num_heads,
                in_proj_weight,
                in_proj_bias,
                bias_k,
                bias_v,
                add_zero_attn,
                dropout_p,
                out_proj_weight,
                out_proj_bias,
                training=training,
                key_padding_mask=key_padding_mask,
                need_weights=need_weights,
                attn_mask=attn_mask,
                is_causal=is_causal,
                use_separate_proj_weight=use_separate_proj_weight,
                q_proj_weight=q_proj_weight,
                k_proj_weight=k_proj_weight,
                v_proj_weight=v_proj_weight,
                static_k=static_k,
                static_v=static_v,
                average_attn_weights=average_attn_weights,
            )
    
        is_batched = _mha_shape_check(
            query, key, value, key_padding_mask, attn_mask, num_heads
        )
    
        # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input
        # is batched, run the computation and before returning squeeze the
        # batch dimension so that the output doesn't carry this temporary batch dimension.
        if not is_batched:
            # unsqueeze if the input is unbatched
            query = query.unsqueeze(1)
            key = key.unsqueeze(1)
            value = value.unsqueeze(1)
            if key_padding_mask is not None:
                key_padding_mask = key_padding_mask.unsqueeze(0)
    
        # set up shape vars
        tgt_len, bsz, embed_dim = query.shape
        src_len, _, _ = key.shape
    
        key_padding_mask = _canonical_mask(
            mask=key_padding_mask,
            mask_name="key_padding_mask",
            other_type=_none_or_dtype(attn_mask),
            other_name="attn_mask",
            target_type=query.dtype,
        )
    
        if is_causal and attn_mask is None:
            raise RuntimeError(
                "Need attn_mask if specifying the is_causal hint. "
                "You may use the Transformer module method "
                "`generate_square_subsequent_mask` to create this mask."
            )
    
        if is_causal and key_padding_mask is None and not need_weights:
            # when we have a kpm or need weights, we need attn_mask
            # Otherwise, we use the is_causal hint go as is_causal
            # indicator to SDPA.
            attn_mask = None
        else:
            attn_mask = _canonical_mask(
                mask=attn_mask,
                mask_name="attn_mask",
                other_type=None,
                other_name="",
                target_type=query.dtype,
                check_other=False,
            )
    
            if key_padding_mask is not None:
                # We have the attn_mask, and use that to merge kpm into it.
                # Turn off use of is_causal hint, as the merged mask is no
                # longer causal.
                is_causal = False
    
        assert embed_dim == embed_dim_to_check, (
            f"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}"
        )
        if isinstance(embed_dim, torch.Tensor):
            # embed_dim can be a tensor when JIT tracing
            head_dim = embed_dim.div(num_heads, rounding_mode="trunc")
        else:
            head_dim = embed_dim // num_heads
        assert head_dim * num_heads == embed_dim, (
            f"embed_dim {embed_dim} not divisible by num_heads {num_heads}"
        )
        if use_separate_proj_weight:
            # allow MHA to have different embedding dimensions when separate projection weights are used
            assert key.shape[:2] == value.shape[:2], (
                f"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}"
            )
        else:
            assert key.shape == value.shape, (
                f"key shape {key.shape} does not match value shape {value.shape}"
            )
    
        #
        # compute in-projection
        #
        if not use_separate_proj_weight:
            assert in_proj_weight is not None, (
                "use_separate_proj_weight is False but in_proj_weight is None"
            )
            q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
        else:
            assert q_proj_weight is not None, (
                "use_separate_proj_weight is True but q_proj_weight is None"
            )
            assert k_proj_weight is not None, (
                "use_separate_proj_weight is True but k_proj_weight is None"
            )
            assert v_proj_weight is not None, (
                "use_separate_proj_weight is True but v_proj_weight is None"
            )
            if in_proj_bias is None:
                b_q = b_k = b_v = None
            else:
                b_q, b_k, b_v = in_proj_bias.chunk(3)
            q, k, v = _in_projection(
                query,
                key,
                value,
                q_proj_weight,
                k_proj_weight,
                v_proj_weight,
                b_q,
                b_k,
                b_v,
            )
    
        # prep attention mask
    
        if attn_mask is not None:
            # ensure attn_mask's dim is 3
            if attn_mask.dim() == 2:
                correct_2d_size = (tgt_len, src_len)
                if attn_mask.shape != correct_2d_size:
                    raise RuntimeError(
                        f"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}."
                    )
                attn_mask = attn_mask.unsqueeze(0)
            elif attn_mask.dim() == 3:
                correct_3d_size = (bsz * num_heads, tgt_len, src_len)
                if attn_mask.shape != correct_3d_size:
                    raise RuntimeError(
                        f"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}."
                    )
            else:
                raise RuntimeError(
                    f"attn_mask's dimension {attn_mask.dim()} is not supported"
                )
    
        # add bias along batch dimension (currently second)
        if bias_k is not None and bias_v is not None:
            assert static_k is None, "bias cannot be added to static key."
            assert static_v is None, "bias cannot be added to static value."
            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
        else:
            assert bias_k is None
            assert bias_v is None
    
        #
        # reshape q, k, v for multihead attention and make them batch first
        #
        q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
        if static_k is None:
            k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_k.size(0) == bsz * num_heads, (
                f"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}"
            )
            assert static_k.size(2) == head_dim, (
                f"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}"
            )
            k = static_k
        if static_v is None:
            v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_v.size(0) == bsz * num_heads, (
                f"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}"
            )
            assert static_v.size(2) == head_dim, (
                f"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}"
            )
            v = static_v
    
        # add zero attention along batch dimension (now first)
        if add_zero_attn:
            zero_attn_shape = (bsz * num_heads, 1, head_dim)
            k = torch.cat(
                [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1
            )
            v = torch.cat(
                [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1
            )
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
    
        # update source sequence length after adjustments
        src_len = k.size(1)
    
        # merge key padding and attention masks
        if key_padding_mask is not None:
            if not torch.jit.is_scripting() and not torch.jit.is_tracing():
                _check_key_padding_mask(key_padding_mask, src_len, bsz)
    
            key_padding_mask = (
                key_padding_mask.view(bsz, 1, 1, src_len)
                .expand(-1, num_heads, -1, -1)
                .reshape(bsz * num_heads, 1, src_len)
            )
            if attn_mask is None:
                attn_mask = key_padding_mask
            else:
                attn_mask = attn_mask + key_padding_mask
    
        # adjust dropout probability
        if not training:
            dropout_p = 0.0
    
        #
        # (deep breath) calculate attention and out projection
        #
    
        if need_weights:
            _B, _Nt, E = q.shape
            q_scaled = q * math.sqrt(1.0 / float(E))
    
            assert not (is_causal and attn_mask is None), (
                "FIXME: is_causal not implemented for need_weights"
            )
    
            if attn_mask is not None:
                attn_output_weights = torch.baddbmm(
                    attn_mask, q_scaled, k.transpose(-2, -1)
                )
            else:
                attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
            attn_output_weights = softmax(attn_output_weights, dim=-1)
            if dropout_p > 0.0:
                attn_output_weights = dropout(attn_output_weights, p=dropout_p)
    
            attn_output = torch.bmm(attn_output_weights, v)
    
            attn_output = (
                attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
            )
            attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
            attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
    
            # optionally average attention weights over heads
            attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
            if average_attn_weights:
                attn_output_weights = attn_output_weights.mean(dim=1)
    
            if not is_batched:
                # squeeze the output if input was unbatched
                attn_output = attn_output.squeeze(1)
                attn_output_weights = attn_output_weights.squeeze(0)
            return attn_output, attn_output_weights
        else:
            # attn_mask can be either (L,S) or (N*num_heads, L, S)
            # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)
            # in order to match the input for SDPA of (N, num_heads, L, S)
            if attn_mask is not None:
                if attn_mask.size(0) == 1 and attn_mask.dim() == 3:
                    attn_mask = attn_mask.unsqueeze(0)
                else:
                    attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)
    
            q = q.view(bsz, num_heads, tgt_len, head_dim)
            k = k.view(bsz, num_heads, src_len, head_dim)
            v = v.view(bsz, num_heads, src_len, head_dim)
    
>           attn_output = scaled_dot_product_attention(
                q, k, v, attn_mask, dropout_p, is_causal
            )
E           RuntimeError: cuDNN Frontend error: CUDNN_BACKEND_TENSOR_DESCRIPTOR cudnnFinalize failedptrDesc->finalize() cudnn_status: CUDNN_STATUS_SUBLIBRARY_LOADING_FAILED

/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6487: RuntimeError
___________________ test_gpt_accuracy[False-small-2-dtype2] ____________________

dtype = torch.bfloat16, bs = 2, model = 'small', parallel_attention_mlp = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("parallel_attention_mlp", all_boolean)
    def test_gpt_accuracy(dtype, bs, model, parallel_attention_mlp):
        config = model_configs[model]
    
        te_gpt = TransformerLayer(
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            num_attention_heads=config.num_heads,
            layernorm_epsilon=config.eps,
            attention_dropout=0.1,
            hidden_dropout=0.1,
            params_dtype=dtype,
            fuse_qkv_params=True,
            qkv_weight_interleaved=False,
            parallel_attention_mlp=parallel_attention_mlp,
            device="cuda",
        ).eval()
    
        torch_gpt = (
            TorchGPT(
                config.hidden_size,
                config.eps,
                config.num_heads,
                parallel_attention_mlp=parallel_attention_mlp,
            )
            .to(dtype=dtype)
            .cuda()
            .eval()
        )
    
        # Share params
        with torch.no_grad():
            torch_gpt.ln.weight = Parameter(
                te_gpt.self_attention.layernorm_qkv.layer_norm_weight.clone()
            )
            torch_gpt.ln.bias = Parameter(te_gpt.self_attention.layernorm_qkv.layer_norm_bias.clone())
            torch_gpt.causal_attn.mhsa.in_proj_weight = Parameter(
                te_gpt.self_attention.layernorm_qkv.weight.clone()
            )
            torch_gpt.causal_attn.mhsa.in_proj_bias = Parameter(
                te_gpt.self_attention.layernorm_qkv.bias.clone()
            )
            torch_gpt.causal_attn.mhsa.out_proj.weight = Parameter(
                te_gpt.self_attention.proj.weight.clone()
            )
            torch_gpt.causal_attn.mhsa.out_proj.bias = Parameter(
                te_gpt.self_attention.proj.bias.clone()
            )
            torch_gpt.ln_mlp.ln.weight = Parameter(te_gpt.layernorm_mlp.layer_norm_weight.clone())
            torch_gpt.ln_mlp.ln.bias = Parameter(te_gpt.layernorm_mlp.layer_norm_bias.clone())
            torch_gpt.ln_mlp.fc1.weight = Parameter(te_gpt.layernorm_mlp.fc1_weight.clone())
            torch_gpt.ln_mlp.fc1.bias = Parameter(te_gpt.layernorm_mlp.fc1_bias.clone())
            torch_gpt.ln_mlp.fc2.weight = Parameter(te_gpt.layernorm_mlp.fc2_weight.clone())
            torch_gpt.ln_mlp.fc2.bias = Parameter(te_gpt.layernorm_mlp.fc2_bias.clone())
    
        te_outputs = _test_e2e_gpt_accuracy(te_gpt, bs, dtype, config)
>       torch_outputs = _test_e2e_gpt_accuracy(torch_gpt, bs, dtype, config)

tests/pytorch/test_numerics.py:987: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/test_numerics.py:916: in _test_e2e_gpt_accuracy
    out = block(inp_hidden_states, attention_mask=inp_attn_mask)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:551: in forward
    b = self.causal_attn(a, attention_mask)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:426: in forward
    output = self.mhsa(x, x, x, attn_mask=attention_mask, need_weights=False)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1488: in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = tensor([[[-1.6719,  0.5508, -0.5508,  ..., -1.4062,  2.7188, -0.1992],
         [-0.2754,  1.3516,  0.1406,  ..., -1.5...,  ...,  0.1973,  0.7422, -0.3613]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<NativeLayerNormBackward0>)
key = tensor([[[-1.6719,  0.5508, -0.5508,  ..., -1.4062,  2.7188, -0.1992],
         [-0.2754,  1.3516,  0.1406,  ..., -1.5...,  ...,  0.1973,  0.7422, -0.3613]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<NativeLayerNormBackward0>)
value = tensor([[[-1.6719,  0.5508, -0.5508,  ..., -1.4062,  2.7188, -0.1992],
         [-0.2754,  1.3516,  0.1406,  ..., -1.5...,  ...,  0.1973,  0.7422, -0.3613]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<NativeLayerNormBackward0>)
embed_dim_to_check = 128, num_heads = 8
in_proj_weight = Parameter containing:
tensor([[ 1.4587e-02,  3.7598e-02, -7.9346e-03,  ..., -6.2256e-03,
          7.5340e-05,  1.5335....., -3.3936e-02,
         -1.2451e-02,  3.1281e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
in_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
bias_k = None, bias_v = None, add_zero_attn = False, dropout_p = 0.0
out_proj_weight = Parameter containing:
tensor([[-0.0037,  0.0415, -0.0242,  ...,  0.0052, -0.0132, -0.0171],
        [ 0.0056,  0.0354,... -0.0457,  0.0081,  ..., -0.0051, -0.0339,  0.0095]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
out_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ... 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
training = False, key_padding_mask = None, need_weights = False
attn_mask = tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],
          [0., 0., -inf,  ..., -inf, -inf, -inf],
          [0., 0...0., 0.,  ..., 0., 0., -inf],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
use_separate_proj_weight = False, q_proj_weight = None, k_proj_weight = None
v_proj_weight = None, static_k = None, static_v = None
average_attn_weights = True, is_causal = False

    def multi_head_attention_forward(
        query: Tensor,
        key: Tensor,
        value: Tensor,
        embed_dim_to_check: int,
        num_heads: int,
        in_proj_weight: Optional[Tensor],
        in_proj_bias: Optional[Tensor],
        bias_k: Optional[Tensor],
        bias_v: Optional[Tensor],
        add_zero_attn: bool,
        dropout_p: float,
        out_proj_weight: Tensor,
        out_proj_bias: Optional[Tensor],
        training: bool = True,
        key_padding_mask: Optional[Tensor] = None,
        need_weights: bool = True,
        attn_mask: Optional[Tensor] = None,
        use_separate_proj_weight: bool = False,
        q_proj_weight: Optional[Tensor] = None,
        k_proj_weight: Optional[Tensor] = None,
        v_proj_weight: Optional[Tensor] = None,
        static_k: Optional[Tensor] = None,
        static_v: Optional[Tensor] = None,
        average_attn_weights: bool = True,
        is_causal: bool = False,
    ) -> tuple[Tensor, Optional[Tensor]]:
        r"""Forward method for MultiHeadAttention.
    
        See :class:`torch.nn.MultiheadAttention` for details.
    
        Args:
            query, key, value: map a query and a set of key-value pairs to an output.
                See "Attention Is All You Need" for more details.
            embed_dim_to_check: total dimension of the model.
            num_heads: parallel attention heads.
            in_proj_weight, in_proj_bias: input projection weight and bias.
            bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
            add_zero_attn: add a new batch of zeros to the key and
                           value sequences at dim=1.
            dropout_p: probability of an element to be zeroed.
            out_proj_weight, out_proj_bias: the output projection weight and bias.
            training: apply dropout if is ``True``.
            key_padding_mask: if provided, specified padding elements in the key will
                be ignored by the attention. This is an binary mask. When the value is True,
                the corresponding value on the attention layer will be filled with -inf.
            need_weights: output attn_output_weights.
                Default: `True`
                Note: `needs_weight` defaults to `True`, but should be set to `False`
                For best performance when attention weights are not needed.
                *Setting needs_weights to `True`
                leads to a significant performance degradation.*
            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
                the batches while a 3D mask allows to specify a different mask for the entries of each batch.
            is_causal: If specified, applies a causal mask as attention mask, and ignores
                attn_mask for computing scaled dot product attention.
                Default: ``False``.
                .. warning::
                    is_causal is provides a hint that the attn_mask is the
                    causal mask.Providing incorrect hints can result in
                    incorrect execution, including forward and backward
                    compatibility.
            use_separate_proj_weight: the function accept the proj. weights for query, key,
                and value in different forms. If false, in_proj_weight will be used, which is
                a combination of q_proj_weight, k_proj_weight, v_proj_weight.
            q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
            static_k, static_v: static key and value used for attention operators.
            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.
                Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect
                when ``need_weights=True.``. Default: True
    
    
        Shape:
            Inputs:
            - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
              the embedding dimension.
            - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.
              If a FloatTensor is provided, it will be directly added to the value.
              If a BoolTensor is provided, the positions with the
              value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
            - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
              3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
              S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked
              positions. If a BoolTensor is provided, positions with ``True``
              are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
              is provided, it will be added to the attention weight.
            - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
            - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
    
            Outputs:
            - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
              E is the embedding dimension.
            - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns
              attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or
              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and
              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per
              head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.
        """
        tens_ops = (
            query,
            key,
            value,
            in_proj_weight,
            in_proj_bias,
            bias_k,
            bias_v,
            out_proj_weight,
            out_proj_bias,
        )
        if has_torch_function(tens_ops):
            return handle_torch_function(
                multi_head_attention_forward,
                tens_ops,
                query,
                key,
                value,
                embed_dim_to_check,
                num_heads,
                in_proj_weight,
                in_proj_bias,
                bias_k,
                bias_v,
                add_zero_attn,
                dropout_p,
                out_proj_weight,
                out_proj_bias,
                training=training,
                key_padding_mask=key_padding_mask,
                need_weights=need_weights,
                attn_mask=attn_mask,
                is_causal=is_causal,
                use_separate_proj_weight=use_separate_proj_weight,
                q_proj_weight=q_proj_weight,
                k_proj_weight=k_proj_weight,
                v_proj_weight=v_proj_weight,
                static_k=static_k,
                static_v=static_v,
                average_attn_weights=average_attn_weights,
            )
    
        is_batched = _mha_shape_check(
            query, key, value, key_padding_mask, attn_mask, num_heads
        )
    
        # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input
        # is batched, run the computation and before returning squeeze the
        # batch dimension so that the output doesn't carry this temporary batch dimension.
        if not is_batched:
            # unsqueeze if the input is unbatched
            query = query.unsqueeze(1)
            key = key.unsqueeze(1)
            value = value.unsqueeze(1)
            if key_padding_mask is not None:
                key_padding_mask = key_padding_mask.unsqueeze(0)
    
        # set up shape vars
        tgt_len, bsz, embed_dim = query.shape
        src_len, _, _ = key.shape
    
        key_padding_mask = _canonical_mask(
            mask=key_padding_mask,
            mask_name="key_padding_mask",
            other_type=_none_or_dtype(attn_mask),
            other_name="attn_mask",
            target_type=query.dtype,
        )
    
        if is_causal and attn_mask is None:
            raise RuntimeError(
                "Need attn_mask if specifying the is_causal hint. "
                "You may use the Transformer module method "
                "`generate_square_subsequent_mask` to create this mask."
            )
    
        if is_causal and key_padding_mask is None and not need_weights:
            # when we have a kpm or need weights, we need attn_mask
            # Otherwise, we use the is_causal hint go as is_causal
            # indicator to SDPA.
            attn_mask = None
        else:
            attn_mask = _canonical_mask(
                mask=attn_mask,
                mask_name="attn_mask",
                other_type=None,
                other_name="",
                target_type=query.dtype,
                check_other=False,
            )
    
            if key_padding_mask is not None:
                # We have the attn_mask, and use that to merge kpm into it.
                # Turn off use of is_causal hint, as the merged mask is no
                # longer causal.
                is_causal = False
    
        assert embed_dim == embed_dim_to_check, (
            f"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}"
        )
        if isinstance(embed_dim, torch.Tensor):
            # embed_dim can be a tensor when JIT tracing
            head_dim = embed_dim.div(num_heads, rounding_mode="trunc")
        else:
            head_dim = embed_dim // num_heads
        assert head_dim * num_heads == embed_dim, (
            f"embed_dim {embed_dim} not divisible by num_heads {num_heads}"
        )
        if use_separate_proj_weight:
            # allow MHA to have different embedding dimensions when separate projection weights are used
            assert key.shape[:2] == value.shape[:2], (
                f"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}"
            )
        else:
            assert key.shape == value.shape, (
                f"key shape {key.shape} does not match value shape {value.shape}"
            )
    
        #
        # compute in-projection
        #
        if not use_separate_proj_weight:
            assert in_proj_weight is not None, (
                "use_separate_proj_weight is False but in_proj_weight is None"
            )
            q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
        else:
            assert q_proj_weight is not None, (
                "use_separate_proj_weight is True but q_proj_weight is None"
            )
            assert k_proj_weight is not None, (
                "use_separate_proj_weight is True but k_proj_weight is None"
            )
            assert v_proj_weight is not None, (
                "use_separate_proj_weight is True but v_proj_weight is None"
            )
            if in_proj_bias is None:
                b_q = b_k = b_v = None
            else:
                b_q, b_k, b_v = in_proj_bias.chunk(3)
            q, k, v = _in_projection(
                query,
                key,
                value,
                q_proj_weight,
                k_proj_weight,
                v_proj_weight,
                b_q,
                b_k,
                b_v,
            )
    
        # prep attention mask
    
        if attn_mask is not None:
            # ensure attn_mask's dim is 3
            if attn_mask.dim() == 2:
                correct_2d_size = (tgt_len, src_len)
                if attn_mask.shape != correct_2d_size:
                    raise RuntimeError(
                        f"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}."
                    )
                attn_mask = attn_mask.unsqueeze(0)
            elif attn_mask.dim() == 3:
                correct_3d_size = (bsz * num_heads, tgt_len, src_len)
                if attn_mask.shape != correct_3d_size:
                    raise RuntimeError(
                        f"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}."
                    )
            else:
                raise RuntimeError(
                    f"attn_mask's dimension {attn_mask.dim()} is not supported"
                )
    
        # add bias along batch dimension (currently second)
        if bias_k is not None and bias_v is not None:
            assert static_k is None, "bias cannot be added to static key."
            assert static_v is None, "bias cannot be added to static value."
            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
        else:
            assert bias_k is None
            assert bias_v is None
    
        #
        # reshape q, k, v for multihead attention and make them batch first
        #
        q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
        if static_k is None:
            k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_k.size(0) == bsz * num_heads, (
                f"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}"
            )
            assert static_k.size(2) == head_dim, (
                f"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}"
            )
            k = static_k
        if static_v is None:
            v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_v.size(0) == bsz * num_heads, (
                f"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}"
            )
            assert static_v.size(2) == head_dim, (
                f"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}"
            )
            v = static_v
    
        # add zero attention along batch dimension (now first)
        if add_zero_attn:
            zero_attn_shape = (bsz * num_heads, 1, head_dim)
            k = torch.cat(
                [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1
            )
            v = torch.cat(
                [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1
            )
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
    
        # update source sequence length after adjustments
        src_len = k.size(1)
    
        # merge key padding and attention masks
        if key_padding_mask is not None:
            if not torch.jit.is_scripting() and not torch.jit.is_tracing():
                _check_key_padding_mask(key_padding_mask, src_len, bsz)
    
            key_padding_mask = (
                key_padding_mask.view(bsz, 1, 1, src_len)
                .expand(-1, num_heads, -1, -1)
                .reshape(bsz * num_heads, 1, src_len)
            )
            if attn_mask is None:
                attn_mask = key_padding_mask
            else:
                attn_mask = attn_mask + key_padding_mask
    
        # adjust dropout probability
        if not training:
            dropout_p = 0.0
    
        #
        # (deep breath) calculate attention and out projection
        #
    
        if need_weights:
            _B, _Nt, E = q.shape
            q_scaled = q * math.sqrt(1.0 / float(E))
    
            assert not (is_causal and attn_mask is None), (
                "FIXME: is_causal not implemented for need_weights"
            )
    
            if attn_mask is not None:
                attn_output_weights = torch.baddbmm(
                    attn_mask, q_scaled, k.transpose(-2, -1)
                )
            else:
                attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
            attn_output_weights = softmax(attn_output_weights, dim=-1)
            if dropout_p > 0.0:
                attn_output_weights = dropout(attn_output_weights, p=dropout_p)
    
            attn_output = torch.bmm(attn_output_weights, v)
    
            attn_output = (
                attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
            )
            attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
            attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
    
            # optionally average attention weights over heads
            attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
            if average_attn_weights:
                attn_output_weights = attn_output_weights.mean(dim=1)
    
            if not is_batched:
                # squeeze the output if input was unbatched
                attn_output = attn_output.squeeze(1)
                attn_output_weights = attn_output_weights.squeeze(0)
            return attn_output, attn_output_weights
        else:
            # attn_mask can be either (L,S) or (N*num_heads, L, S)
            # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)
            # in order to match the input for SDPA of (N, num_heads, L, S)
            if attn_mask is not None:
                if attn_mask.size(0) == 1 and attn_mask.dim() == 3:
                    attn_mask = attn_mask.unsqueeze(0)
                else:
                    attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)
    
            q = q.view(bsz, num_heads, tgt_len, head_dim)
            k = k.view(bsz, num_heads, src_len, head_dim)
            v = v.view(bsz, num_heads, src_len, head_dim)
    
>           attn_output = scaled_dot_product_attention(
                q, k, v, attn_mask, dropout_p, is_causal
            )
E           RuntimeError: cuDNN Frontend error: CUDNN_BACKEND_TENSOR_DESCRIPTOR cudnnFinalize failedptrDesc->finalize() cudnn_status: CUDNN_STATUS_SUBLIBRARY_LOADING_FAILED

/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6487: RuntimeError
___________________ test_mha_accuracy[causal-small-1-dtype1] ___________________

dtype = torch.float16, bs = 1, model = 'small', mask_type = 'causal'

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("mask_type", mask_types)
    def test_mha_accuracy(dtype, bs, model, mask_type):
        config = model_configs[model]
    
        te_mha = MultiheadAttention(
            config.hidden_size,
            config.num_heads,
            fuse_qkv_params=True,
            params_dtype=dtype,
            qkv_weight_interleaved=False,
            input_layernorm=False,
            device="cuda",
        ).eval()
    
        torch_mha = (
            TorchMHA(
                config.hidden_size,
                config.num_heads,
            )
            .to(dtype=dtype)
            .cuda()
            .eval()
        )
    
        # Share params
        with torch.no_grad():
            torch_mha.mhsa.in_proj_weight = Parameter(te_mha.qkv.weight.clone())
            torch_mha.mhsa.in_proj_bias = Parameter(te_mha.qkv.bias.clone())
            torch_mha.mhsa.out_proj.weight = Parameter(te_mha.proj.weight.clone())
            torch_mha.mhsa.out_proj.bias = Parameter(te_mha.proj.bias.clone())
    
        te_outputs = _test_mha_accuracy(te_mha, bs, dtype, config, mask_type, te=True)
>       torch_outputs = _test_mha_accuracy(torch_mha, bs, dtype, config, mask_type, te=False)

tests/pytorch/test_numerics.py:1074: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/test_numerics.py:1027: in _test_mha_accuracy
    out = block(inp_hidden_states, **forward_kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:426: in forward
    output = self.mhsa(x, x, x, attn_mask=attention_mask, need_weights=False)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1488: in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = tensor([[[-1.6162,  0.5684, -0.5103,  ..., -1.3506,  2.7090, -0.1680]],

        [[-0.3225,  1.4258,  0.1251,  ..., -1...  0.7158,  0.8496,  ..., -1.2715, -0.7954, -1.6895]]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
key = tensor([[[-1.6162,  0.5684, -0.5103,  ..., -1.3506,  2.7090, -0.1680]],

        [[-0.3225,  1.4258,  0.1251,  ..., -1...  0.7158,  0.8496,  ..., -1.2715, -0.7954, -1.6895]]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
value = tensor([[[-1.6162,  0.5684, -0.5103,  ..., -1.3506,  2.7090, -0.1680]],

        [[-0.3225,  1.4258,  0.1251,  ..., -1...  0.7158,  0.8496,  ..., -1.2715, -0.7954, -1.6895]]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
embed_dim_to_check = 128, num_heads = 8
in_proj_weight = Parameter containing:
tensor([[ 1.4565e-02,  3.7628e-02, -7.9575e-03,  ..., -6.2141e-03,
          7.5221e-05,  1.5306......, -3.4058e-02,
         -1.2466e-02,  3.1300e-03]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
in_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
bias_k = None, bias_v = None, add_zero_attn = False, dropout_p = 0.0
out_proj_weight = Parameter containing:
tensor([[-0.0037,  0.0416, -0.0242,  ...,  0.0051, -0.0132, -0.0170],
        [ 0.0056,  0.0353,..., -0.0457,  0.0080,  ..., -0.0051, -0.0340,  0.0095]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
out_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ..., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
training = False, key_padding_mask = None, need_weights = False
attn_mask = tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],
          [0., 0., -inf,  ..., -inf, -inf, -inf],
          [0., 0... 0., 0.,  ..., 0., 0., -inf],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
use_separate_proj_weight = False, q_proj_weight = None, k_proj_weight = None
v_proj_weight = None, static_k = None, static_v = None
average_attn_weights = True, is_causal = False

    def multi_head_attention_forward(
        query: Tensor,
        key: Tensor,
        value: Tensor,
        embed_dim_to_check: int,
        num_heads: int,
        in_proj_weight: Optional[Tensor],
        in_proj_bias: Optional[Tensor],
        bias_k: Optional[Tensor],
        bias_v: Optional[Tensor],
        add_zero_attn: bool,
        dropout_p: float,
        out_proj_weight: Tensor,
        out_proj_bias: Optional[Tensor],
        training: bool = True,
        key_padding_mask: Optional[Tensor] = None,
        need_weights: bool = True,
        attn_mask: Optional[Tensor] = None,
        use_separate_proj_weight: bool = False,
        q_proj_weight: Optional[Tensor] = None,
        k_proj_weight: Optional[Tensor] = None,
        v_proj_weight: Optional[Tensor] = None,
        static_k: Optional[Tensor] = None,
        static_v: Optional[Tensor] = None,
        average_attn_weights: bool = True,
        is_causal: bool = False,
    ) -> tuple[Tensor, Optional[Tensor]]:
        r"""Forward method for MultiHeadAttention.
    
        See :class:`torch.nn.MultiheadAttention` for details.
    
        Args:
            query, key, value: map a query and a set of key-value pairs to an output.
                See "Attention Is All You Need" for more details.
            embed_dim_to_check: total dimension of the model.
            num_heads: parallel attention heads.
            in_proj_weight, in_proj_bias: input projection weight and bias.
            bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
            add_zero_attn: add a new batch of zeros to the key and
                           value sequences at dim=1.
            dropout_p: probability of an element to be zeroed.
            out_proj_weight, out_proj_bias: the output projection weight and bias.
            training: apply dropout if is ``True``.
            key_padding_mask: if provided, specified padding elements in the key will
                be ignored by the attention. This is an binary mask. When the value is True,
                the corresponding value on the attention layer will be filled with -inf.
            need_weights: output attn_output_weights.
                Default: `True`
                Note: `needs_weight` defaults to `True`, but should be set to `False`
                For best performance when attention weights are not needed.
                *Setting needs_weights to `True`
                leads to a significant performance degradation.*
            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
                the batches while a 3D mask allows to specify a different mask for the entries of each batch.
            is_causal: If specified, applies a causal mask as attention mask, and ignores
                attn_mask for computing scaled dot product attention.
                Default: ``False``.
                .. warning::
                    is_causal is provides a hint that the attn_mask is the
                    causal mask.Providing incorrect hints can result in
                    incorrect execution, including forward and backward
                    compatibility.
            use_separate_proj_weight: the function accept the proj. weights for query, key,
                and value in different forms. If false, in_proj_weight will be used, which is
                a combination of q_proj_weight, k_proj_weight, v_proj_weight.
            q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
            static_k, static_v: static key and value used for attention operators.
            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.
                Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect
                when ``need_weights=True.``. Default: True
    
    
        Shape:
            Inputs:
            - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
              the embedding dimension.
            - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.
              If a FloatTensor is provided, it will be directly added to the value.
              If a BoolTensor is provided, the positions with the
              value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
            - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
              3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
              S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked
              positions. If a BoolTensor is provided, positions with ``True``
              are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
              is provided, it will be added to the attention weight.
            - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
            - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
    
            Outputs:
            - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
              E is the embedding dimension.
            - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns
              attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or
              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and
              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per
              head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.
        """
        tens_ops = (
            query,
            key,
            value,
            in_proj_weight,
            in_proj_bias,
            bias_k,
            bias_v,
            out_proj_weight,
            out_proj_bias,
        )
        if has_torch_function(tens_ops):
            return handle_torch_function(
                multi_head_attention_forward,
                tens_ops,
                query,
                key,
                value,
                embed_dim_to_check,
                num_heads,
                in_proj_weight,
                in_proj_bias,
                bias_k,
                bias_v,
                add_zero_attn,
                dropout_p,
                out_proj_weight,
                out_proj_bias,
                training=training,
                key_padding_mask=key_padding_mask,
                need_weights=need_weights,
                attn_mask=attn_mask,
                is_causal=is_causal,
                use_separate_proj_weight=use_separate_proj_weight,
                q_proj_weight=q_proj_weight,
                k_proj_weight=k_proj_weight,
                v_proj_weight=v_proj_weight,
                static_k=static_k,
                static_v=static_v,
                average_attn_weights=average_attn_weights,
            )
    
        is_batched = _mha_shape_check(
            query, key, value, key_padding_mask, attn_mask, num_heads
        )
    
        # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input
        # is batched, run the computation and before returning squeeze the
        # batch dimension so that the output doesn't carry this temporary batch dimension.
        if not is_batched:
            # unsqueeze if the input is unbatched
            query = query.unsqueeze(1)
            key = key.unsqueeze(1)
            value = value.unsqueeze(1)
            if key_padding_mask is not None:
                key_padding_mask = key_padding_mask.unsqueeze(0)
    
        # set up shape vars
        tgt_len, bsz, embed_dim = query.shape
        src_len, _, _ = key.shape
    
        key_padding_mask = _canonical_mask(
            mask=key_padding_mask,
            mask_name="key_padding_mask",
            other_type=_none_or_dtype(attn_mask),
            other_name="attn_mask",
            target_type=query.dtype,
        )
    
        if is_causal and attn_mask is None:
            raise RuntimeError(
                "Need attn_mask if specifying the is_causal hint. "
                "You may use the Transformer module method "
                "`generate_square_subsequent_mask` to create this mask."
            )
    
        if is_causal and key_padding_mask is None and not need_weights:
            # when we have a kpm or need weights, we need attn_mask
            # Otherwise, we use the is_causal hint go as is_causal
            # indicator to SDPA.
            attn_mask = None
        else:
            attn_mask = _canonical_mask(
                mask=attn_mask,
                mask_name="attn_mask",
                other_type=None,
                other_name="",
                target_type=query.dtype,
                check_other=False,
            )
    
            if key_padding_mask is not None:
                # We have the attn_mask, and use that to merge kpm into it.
                # Turn off use of is_causal hint, as the merged mask is no
                # longer causal.
                is_causal = False
    
        assert embed_dim == embed_dim_to_check, (
            f"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}"
        )
        if isinstance(embed_dim, torch.Tensor):
            # embed_dim can be a tensor when JIT tracing
            head_dim = embed_dim.div(num_heads, rounding_mode="trunc")
        else:
            head_dim = embed_dim // num_heads
        assert head_dim * num_heads == embed_dim, (
            f"embed_dim {embed_dim} not divisible by num_heads {num_heads}"
        )
        if use_separate_proj_weight:
            # allow MHA to have different embedding dimensions when separate projection weights are used
            assert key.shape[:2] == value.shape[:2], (
                f"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}"
            )
        else:
            assert key.shape == value.shape, (
                f"key shape {key.shape} does not match value shape {value.shape}"
            )
    
        #
        # compute in-projection
        #
        if not use_separate_proj_weight:
            assert in_proj_weight is not None, (
                "use_separate_proj_weight is False but in_proj_weight is None"
            )
            q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
        else:
            assert q_proj_weight is not None, (
                "use_separate_proj_weight is True but q_proj_weight is None"
            )
            assert k_proj_weight is not None, (
                "use_separate_proj_weight is True but k_proj_weight is None"
            )
            assert v_proj_weight is not None, (
                "use_separate_proj_weight is True but v_proj_weight is None"
            )
            if in_proj_bias is None:
                b_q = b_k = b_v = None
            else:
                b_q, b_k, b_v = in_proj_bias.chunk(3)
            q, k, v = _in_projection(
                query,
                key,
                value,
                q_proj_weight,
                k_proj_weight,
                v_proj_weight,
                b_q,
                b_k,
                b_v,
            )
    
        # prep attention mask
    
        if attn_mask is not None:
            # ensure attn_mask's dim is 3
            if attn_mask.dim() == 2:
                correct_2d_size = (tgt_len, src_len)
                if attn_mask.shape != correct_2d_size:
                    raise RuntimeError(
                        f"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}."
                    )
                attn_mask = attn_mask.unsqueeze(0)
            elif attn_mask.dim() == 3:
                correct_3d_size = (bsz * num_heads, tgt_len, src_len)
                if attn_mask.shape != correct_3d_size:
                    raise RuntimeError(
                        f"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}."
                    )
            else:
                raise RuntimeError(
                    f"attn_mask's dimension {attn_mask.dim()} is not supported"
                )
    
        # add bias along batch dimension (currently second)
        if bias_k is not None and bias_v is not None:
            assert static_k is None, "bias cannot be added to static key."
            assert static_v is None, "bias cannot be added to static value."
            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
        else:
            assert bias_k is None
            assert bias_v is None
    
        #
        # reshape q, k, v for multihead attention and make them batch first
        #
        q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
        if static_k is None:
            k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_k.size(0) == bsz * num_heads, (
                f"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}"
            )
            assert static_k.size(2) == head_dim, (
                f"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}"
            )
            k = static_k
        if static_v is None:
            v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_v.size(0) == bsz * num_heads, (
                f"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}"
            )
            assert static_v.size(2) == head_dim, (
                f"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}"
            )
            v = static_v
    
        # add zero attention along batch dimension (now first)
        if add_zero_attn:
            zero_attn_shape = (bsz * num_heads, 1, head_dim)
            k = torch.cat(
                [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1
            )
            v = torch.cat(
                [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1
            )
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
    
        # update source sequence length after adjustments
        src_len = k.size(1)
    
        # merge key padding and attention masks
        if key_padding_mask is not None:
            if not torch.jit.is_scripting() and not torch.jit.is_tracing():
                _check_key_padding_mask(key_padding_mask, src_len, bsz)
    
            key_padding_mask = (
                key_padding_mask.view(bsz, 1, 1, src_len)
                .expand(-1, num_heads, -1, -1)
                .reshape(bsz * num_heads, 1, src_len)
            )
            if attn_mask is None:
                attn_mask = key_padding_mask
            else:
                attn_mask = attn_mask + key_padding_mask
    
        # adjust dropout probability
        if not training:
            dropout_p = 0.0
    
        #
        # (deep breath) calculate attention and out projection
        #
    
        if need_weights:
            _B, _Nt, E = q.shape
            q_scaled = q * math.sqrt(1.0 / float(E))
    
            assert not (is_causal and attn_mask is None), (
                "FIXME: is_causal not implemented for need_weights"
            )
    
            if attn_mask is not None:
                attn_output_weights = torch.baddbmm(
                    attn_mask, q_scaled, k.transpose(-2, -1)
                )
            else:
                attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
            attn_output_weights = softmax(attn_output_weights, dim=-1)
            if dropout_p > 0.0:
                attn_output_weights = dropout(attn_output_weights, p=dropout_p)
    
            attn_output = torch.bmm(attn_output_weights, v)
    
            attn_output = (
                attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
            )
            attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
            attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
    
            # optionally average attention weights over heads
            attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
            if average_attn_weights:
                attn_output_weights = attn_output_weights.mean(dim=1)
    
            if not is_batched:
                # squeeze the output if input was unbatched
                attn_output = attn_output.squeeze(1)
                attn_output_weights = attn_output_weights.squeeze(0)
            return attn_output, attn_output_weights
        else:
            # attn_mask can be either (L,S) or (N*num_heads, L, S)
            # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)
            # in order to match the input for SDPA of (N, num_heads, L, S)
            if attn_mask is not None:
                if attn_mask.size(0) == 1 and attn_mask.dim() == 3:
                    attn_mask = attn_mask.unsqueeze(0)
                else:
                    attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)
    
            q = q.view(bsz, num_heads, tgt_len, head_dim)
            k = k.view(bsz, num_heads, src_len, head_dim)
            v = v.view(bsz, num_heads, src_len, head_dim)
    
>           attn_output = scaled_dot_product_attention(
                q, k, v, attn_mask, dropout_p, is_causal
            )
E           RuntimeError: cuDNN Frontend error: CUDNN_BACKEND_TENSOR_DESCRIPTOR cudnnFinalize failedptrDesc->finalize() cudnn_status: CUDNN_STATUS_SUBLIBRARY_LOADING_FAILED

/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6487: RuntimeError
___________________ test_mha_accuracy[causal-small-1-dtype2] ___________________

dtype = torch.bfloat16, bs = 1, model = 'small', mask_type = 'causal'

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("mask_type", mask_types)
    def test_mha_accuracy(dtype, bs, model, mask_type):
        config = model_configs[model]
    
        te_mha = MultiheadAttention(
            config.hidden_size,
            config.num_heads,
            fuse_qkv_params=True,
            params_dtype=dtype,
            qkv_weight_interleaved=False,
            input_layernorm=False,
            device="cuda",
        ).eval()
    
        torch_mha = (
            TorchMHA(
                config.hidden_size,
                config.num_heads,
            )
            .to(dtype=dtype)
            .cuda()
            .eval()
        )
    
        # Share params
        with torch.no_grad():
            torch_mha.mhsa.in_proj_weight = Parameter(te_mha.qkv.weight.clone())
            torch_mha.mhsa.in_proj_bias = Parameter(te_mha.qkv.bias.clone())
            torch_mha.mhsa.out_proj.weight = Parameter(te_mha.proj.weight.clone())
            torch_mha.mhsa.out_proj.bias = Parameter(te_mha.proj.bias.clone())
    
        te_outputs = _test_mha_accuracy(te_mha, bs, dtype, config, mask_type, te=True)
>       torch_outputs = _test_mha_accuracy(torch_mha, bs, dtype, config, mask_type, te=False)

tests/pytorch/test_numerics.py:1074: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/test_numerics.py:1027: in _test_mha_accuracy
    out = block(inp_hidden_states, **forward_kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:426: in forward
    output = self.mhsa(x, x, x, attn_mask=attention_mask, need_weights=False)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1488: in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = tensor([[[-1.6172,  0.5703, -0.5117,  ..., -1.3516,  2.7031, -0.1680]],

        [[-0.3223,  1.4297,  0.1250,  ..., -1... 0.7148,  0.8516,  ..., -1.2734, -0.7969, -1.6875]]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
key = tensor([[[-1.6172,  0.5703, -0.5117,  ..., -1.3516,  2.7031, -0.1680]],

        [[-0.3223,  1.4297,  0.1250,  ..., -1... 0.7148,  0.8516,  ..., -1.2734, -0.7969, -1.6875]]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
value = tensor([[[-1.6172,  0.5703, -0.5117,  ..., -1.3516,  2.7031, -0.1680]],

        [[-0.3223,  1.4297,  0.1250,  ..., -1... 0.7148,  0.8516,  ..., -1.2734, -0.7969, -1.6875]]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
embed_dim_to_check = 128, num_heads = 8
in_proj_weight = Parameter containing:
tensor([[ 1.4587e-02,  3.7598e-02, -7.9346e-03,  ..., -6.2256e-03,
          7.5340e-05,  1.5335....., -3.3936e-02,
         -1.2451e-02,  3.1281e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
in_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
bias_k = None, bias_v = None, add_zero_attn = False, dropout_p = 0.0
out_proj_weight = Parameter containing:
tensor([[-0.0037,  0.0415, -0.0242,  ...,  0.0052, -0.0132, -0.0171],
        [ 0.0056,  0.0354,... -0.0457,  0.0081,  ..., -0.0051, -0.0339,  0.0095]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
out_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ... 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
training = False, key_padding_mask = None, need_weights = False
attn_mask = tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],
          [0., 0., -inf,  ..., -inf, -inf, -inf],
          [0., 0...0., 0.,  ..., 0., 0., -inf],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
use_separate_proj_weight = False, q_proj_weight = None, k_proj_weight = None
v_proj_weight = None, static_k = None, static_v = None
average_attn_weights = True, is_causal = False

    def multi_head_attention_forward(
        query: Tensor,
        key: Tensor,
        value: Tensor,
        embed_dim_to_check: int,
        num_heads: int,
        in_proj_weight: Optional[Tensor],
        in_proj_bias: Optional[Tensor],
        bias_k: Optional[Tensor],
        bias_v: Optional[Tensor],
        add_zero_attn: bool,
        dropout_p: float,
        out_proj_weight: Tensor,
        out_proj_bias: Optional[Tensor],
        training: bool = True,
        key_padding_mask: Optional[Tensor] = None,
        need_weights: bool = True,
        attn_mask: Optional[Tensor] = None,
        use_separate_proj_weight: bool = False,
        q_proj_weight: Optional[Tensor] = None,
        k_proj_weight: Optional[Tensor] = None,
        v_proj_weight: Optional[Tensor] = None,
        static_k: Optional[Tensor] = None,
        static_v: Optional[Tensor] = None,
        average_attn_weights: bool = True,
        is_causal: bool = False,
    ) -> tuple[Tensor, Optional[Tensor]]:
        r"""Forward method for MultiHeadAttention.
    
        See :class:`torch.nn.MultiheadAttention` for details.
    
        Args:
            query, key, value: map a query and a set of key-value pairs to an output.
                See "Attention Is All You Need" for more details.
            embed_dim_to_check: total dimension of the model.
            num_heads: parallel attention heads.
            in_proj_weight, in_proj_bias: input projection weight and bias.
            bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
            add_zero_attn: add a new batch of zeros to the key and
                           value sequences at dim=1.
            dropout_p: probability of an element to be zeroed.
            out_proj_weight, out_proj_bias: the output projection weight and bias.
            training: apply dropout if is ``True``.
            key_padding_mask: if provided, specified padding elements in the key will
                be ignored by the attention. This is an binary mask. When the value is True,
                the corresponding value on the attention layer will be filled with -inf.
            need_weights: output attn_output_weights.
                Default: `True`
                Note: `needs_weight` defaults to `True`, but should be set to `False`
                For best performance when attention weights are not needed.
                *Setting needs_weights to `True`
                leads to a significant performance degradation.*
            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
                the batches while a 3D mask allows to specify a different mask for the entries of each batch.
            is_causal: If specified, applies a causal mask as attention mask, and ignores
                attn_mask for computing scaled dot product attention.
                Default: ``False``.
                .. warning::
                    is_causal is provides a hint that the attn_mask is the
                    causal mask.Providing incorrect hints can result in
                    incorrect execution, including forward and backward
                    compatibility.
            use_separate_proj_weight: the function accept the proj. weights for query, key,
                and value in different forms. If false, in_proj_weight will be used, which is
                a combination of q_proj_weight, k_proj_weight, v_proj_weight.
            q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
            static_k, static_v: static key and value used for attention operators.
            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.
                Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect
                when ``need_weights=True.``. Default: True
    
    
        Shape:
            Inputs:
            - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
              the embedding dimension.
            - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.
              If a FloatTensor is provided, it will be directly added to the value.
              If a BoolTensor is provided, the positions with the
              value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
            - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
              3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
              S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked
              positions. If a BoolTensor is provided, positions with ``True``
              are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
              is provided, it will be added to the attention weight.
            - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
            - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
    
            Outputs:
            - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
              E is the embedding dimension.
            - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns
              attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or
              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and
              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per
              head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.
        """
        tens_ops = (
            query,
            key,
            value,
            in_proj_weight,
            in_proj_bias,
            bias_k,
            bias_v,
            out_proj_weight,
            out_proj_bias,
        )
        if has_torch_function(tens_ops):
            return handle_torch_function(
                multi_head_attention_forward,
                tens_ops,
                query,
                key,
                value,
                embed_dim_to_check,
                num_heads,
                in_proj_weight,
                in_proj_bias,
                bias_k,
                bias_v,
                add_zero_attn,
                dropout_p,
                out_proj_weight,
                out_proj_bias,
                training=training,
                key_padding_mask=key_padding_mask,
                need_weights=need_weights,
                attn_mask=attn_mask,
                is_causal=is_causal,
                use_separate_proj_weight=use_separate_proj_weight,
                q_proj_weight=q_proj_weight,
                k_proj_weight=k_proj_weight,
                v_proj_weight=v_proj_weight,
                static_k=static_k,
                static_v=static_v,
                average_attn_weights=average_attn_weights,
            )
    
        is_batched = _mha_shape_check(
            query, key, value, key_padding_mask, attn_mask, num_heads
        )
    
        # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input
        # is batched, run the computation and before returning squeeze the
        # batch dimension so that the output doesn't carry this temporary batch dimension.
        if not is_batched:
            # unsqueeze if the input is unbatched
            query = query.unsqueeze(1)
            key = key.unsqueeze(1)
            value = value.unsqueeze(1)
            if key_padding_mask is not None:
                key_padding_mask = key_padding_mask.unsqueeze(0)
    
        # set up shape vars
        tgt_len, bsz, embed_dim = query.shape
        src_len, _, _ = key.shape
    
        key_padding_mask = _canonical_mask(
            mask=key_padding_mask,
            mask_name="key_padding_mask",
            other_type=_none_or_dtype(attn_mask),
            other_name="attn_mask",
            target_type=query.dtype,
        )
    
        if is_causal and attn_mask is None:
            raise RuntimeError(
                "Need attn_mask if specifying the is_causal hint. "
                "You may use the Transformer module method "
                "`generate_square_subsequent_mask` to create this mask."
            )
    
        if is_causal and key_padding_mask is None and not need_weights:
            # when we have a kpm or need weights, we need attn_mask
            # Otherwise, we use the is_causal hint go as is_causal
            # indicator to SDPA.
            attn_mask = None
        else:
            attn_mask = _canonical_mask(
                mask=attn_mask,
                mask_name="attn_mask",
                other_type=None,
                other_name="",
                target_type=query.dtype,
                check_other=False,
            )
    
            if key_padding_mask is not None:
                # We have the attn_mask, and use that to merge kpm into it.
                # Turn off use of is_causal hint, as the merged mask is no
                # longer causal.
                is_causal = False
    
        assert embed_dim == embed_dim_to_check, (
            f"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}"
        )
        if isinstance(embed_dim, torch.Tensor):
            # embed_dim can be a tensor when JIT tracing
            head_dim = embed_dim.div(num_heads, rounding_mode="trunc")
        else:
            head_dim = embed_dim // num_heads
        assert head_dim * num_heads == embed_dim, (
            f"embed_dim {embed_dim} not divisible by num_heads {num_heads}"
        )
        if use_separate_proj_weight:
            # allow MHA to have different embedding dimensions when separate projection weights are used
            assert key.shape[:2] == value.shape[:2], (
                f"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}"
            )
        else:
            assert key.shape == value.shape, (
                f"key shape {key.shape} does not match value shape {value.shape}"
            )
    
        #
        # compute in-projection
        #
        if not use_separate_proj_weight:
            assert in_proj_weight is not None, (
                "use_separate_proj_weight is False but in_proj_weight is None"
            )
            q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
        else:
            assert q_proj_weight is not None, (
                "use_separate_proj_weight is True but q_proj_weight is None"
            )
            assert k_proj_weight is not None, (
                "use_separate_proj_weight is True but k_proj_weight is None"
            )
            assert v_proj_weight is not None, (
                "use_separate_proj_weight is True but v_proj_weight is None"
            )
            if in_proj_bias is None:
                b_q = b_k = b_v = None
            else:
                b_q, b_k, b_v = in_proj_bias.chunk(3)
            q, k, v = _in_projection(
                query,
                key,
                value,
                q_proj_weight,
                k_proj_weight,
                v_proj_weight,
                b_q,
                b_k,
                b_v,
            )
    
        # prep attention mask
    
        if attn_mask is not None:
            # ensure attn_mask's dim is 3
            if attn_mask.dim() == 2:
                correct_2d_size = (tgt_len, src_len)
                if attn_mask.shape != correct_2d_size:
                    raise RuntimeError(
                        f"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}."
                    )
                attn_mask = attn_mask.unsqueeze(0)
            elif attn_mask.dim() == 3:
                correct_3d_size = (bsz * num_heads, tgt_len, src_len)
                if attn_mask.shape != correct_3d_size:
                    raise RuntimeError(
                        f"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}."
                    )
            else:
                raise RuntimeError(
                    f"attn_mask's dimension {attn_mask.dim()} is not supported"
                )
    
        # add bias along batch dimension (currently second)
        if bias_k is not None and bias_v is not None:
            assert static_k is None, "bias cannot be added to static key."
            assert static_v is None, "bias cannot be added to static value."
            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
        else:
            assert bias_k is None
            assert bias_v is None
    
        #
        # reshape q, k, v for multihead attention and make them batch first
        #
        q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
        if static_k is None:
            k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_k.size(0) == bsz * num_heads, (
                f"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}"
            )
            assert static_k.size(2) == head_dim, (
                f"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}"
            )
            k = static_k
        if static_v is None:
            v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_v.size(0) == bsz * num_heads, (
                f"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}"
            )
            assert static_v.size(2) == head_dim, (
                f"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}"
            )
            v = static_v
    
        # add zero attention along batch dimension (now first)
        if add_zero_attn:
            zero_attn_shape = (bsz * num_heads, 1, head_dim)
            k = torch.cat(
                [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1
            )
            v = torch.cat(
                [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1
            )
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
    
        # update source sequence length after adjustments
        src_len = k.size(1)
    
        # merge key padding and attention masks
        if key_padding_mask is not None:
            if not torch.jit.is_scripting() and not torch.jit.is_tracing():
                _check_key_padding_mask(key_padding_mask, src_len, bsz)
    
            key_padding_mask = (
                key_padding_mask.view(bsz, 1, 1, src_len)
                .expand(-1, num_heads, -1, -1)
                .reshape(bsz * num_heads, 1, src_len)
            )
            if attn_mask is None:
                attn_mask = key_padding_mask
            else:
                attn_mask = attn_mask + key_padding_mask
    
        # adjust dropout probability
        if not training:
            dropout_p = 0.0
    
        #
        # (deep breath) calculate attention and out projection
        #
    
        if need_weights:
            _B, _Nt, E = q.shape
            q_scaled = q * math.sqrt(1.0 / float(E))
    
            assert not (is_causal and attn_mask is None), (
                "FIXME: is_causal not implemented for need_weights"
            )
    
            if attn_mask is not None:
                attn_output_weights = torch.baddbmm(
                    attn_mask, q_scaled, k.transpose(-2, -1)
                )
            else:
                attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
            attn_output_weights = softmax(attn_output_weights, dim=-1)
            if dropout_p > 0.0:
                attn_output_weights = dropout(attn_output_weights, p=dropout_p)
    
            attn_output = torch.bmm(attn_output_weights, v)
    
            attn_output = (
                attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
            )
            attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
            attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
    
            # optionally average attention weights over heads
            attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
            if average_attn_weights:
                attn_output_weights = attn_output_weights.mean(dim=1)
    
            if not is_batched:
                # squeeze the output if input was unbatched
                attn_output = attn_output.squeeze(1)
                attn_output_weights = attn_output_weights.squeeze(0)
            return attn_output, attn_output_weights
        else:
            # attn_mask can be either (L,S) or (N*num_heads, L, S)
            # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)
            # in order to match the input for SDPA of (N, num_heads, L, S)
            if attn_mask is not None:
                if attn_mask.size(0) == 1 and attn_mask.dim() == 3:
                    attn_mask = attn_mask.unsqueeze(0)
                else:
                    attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)
    
            q = q.view(bsz, num_heads, tgt_len, head_dim)
            k = k.view(bsz, num_heads, src_len, head_dim)
            v = v.view(bsz, num_heads, src_len, head_dim)
    
>           attn_output = scaled_dot_product_attention(
                q, k, v, attn_mask, dropout_p, is_causal
            )
E           RuntimeError: cuDNN Frontend error: CUDNN_BACKEND_TENSOR_DESCRIPTOR cudnnFinalize failedptrDesc->finalize() cudnn_status: CUDNN_STATUS_SUBLIBRARY_LOADING_FAILED

/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6487: RuntimeError
___________________ test_mha_accuracy[causal-small-2-dtype1] ___________________

dtype = torch.float16, bs = 2, model = 'small', mask_type = 'causal'

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("mask_type", mask_types)
    def test_mha_accuracy(dtype, bs, model, mask_type):
        config = model_configs[model]
    
        te_mha = MultiheadAttention(
            config.hidden_size,
            config.num_heads,
            fuse_qkv_params=True,
            params_dtype=dtype,
            qkv_weight_interleaved=False,
            input_layernorm=False,
            device="cuda",
        ).eval()
    
        torch_mha = (
            TorchMHA(
                config.hidden_size,
                config.num_heads,
            )
            .to(dtype=dtype)
            .cuda()
            .eval()
        )
    
        # Share params
        with torch.no_grad():
            torch_mha.mhsa.in_proj_weight = Parameter(te_mha.qkv.weight.clone())
            torch_mha.mhsa.in_proj_bias = Parameter(te_mha.qkv.bias.clone())
            torch_mha.mhsa.out_proj.weight = Parameter(te_mha.proj.weight.clone())
            torch_mha.mhsa.out_proj.bias = Parameter(te_mha.proj.bias.clone())
    
        te_outputs = _test_mha_accuracy(te_mha, bs, dtype, config, mask_type, te=True)
>       torch_outputs = _test_mha_accuracy(torch_mha, bs, dtype, config, mask_type, te=False)

tests/pytorch/test_numerics.py:1074: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/test_numerics.py:1027: in _test_mha_accuracy
    out = block(inp_hidden_states, **forward_kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:426: in forward
    output = self.mhsa(x, x, x, attn_mask=attention_mask, need_weights=False)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1488: in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = tensor([[[-1.6162,  0.5684, -0.5103,  ..., -1.3506,  2.7090, -0.1680],
         [-0.3225,  1.4258,  0.1251,  ..., -1.7... -0.4214, -0.0599,  ...,  0.2466,  0.7554, -0.2751]]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
key = tensor([[[-1.6162,  0.5684, -0.5103,  ..., -1.3506,  2.7090, -0.1680],
         [-0.3225,  1.4258,  0.1251,  ..., -1.7... -0.4214, -0.0599,  ...,  0.2466,  0.7554, -0.2751]]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
value = tensor([[[-1.6162,  0.5684, -0.5103,  ..., -1.3506,  2.7090, -0.1680],
         [-0.3225,  1.4258,  0.1251,  ..., -1.7... -0.4214, -0.0599,  ...,  0.2466,  0.7554, -0.2751]]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
embed_dim_to_check = 128, num_heads = 8
in_proj_weight = Parameter containing:
tensor([[ 1.4565e-02,  3.7628e-02, -7.9575e-03,  ..., -6.2141e-03,
          7.5221e-05,  1.5306......, -3.4058e-02,
         -1.2466e-02,  3.1300e-03]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
in_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
bias_k = None, bias_v = None, add_zero_attn = False, dropout_p = 0.0
out_proj_weight = Parameter containing:
tensor([[-0.0037,  0.0416, -0.0242,  ...,  0.0051, -0.0132, -0.0170],
        [ 0.0056,  0.0353,..., -0.0457,  0.0080,  ..., -0.0051, -0.0340,  0.0095]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
out_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ..., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
training = False, key_padding_mask = None, need_weights = False
attn_mask = tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],
          [0., 0., -inf,  ..., -inf, -inf, -inf],
          [0., 0... 0., 0.,  ..., 0., 0., -inf],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
use_separate_proj_weight = False, q_proj_weight = None, k_proj_weight = None
v_proj_weight = None, static_k = None, static_v = None
average_attn_weights = True, is_causal = False

    def multi_head_attention_forward(
        query: Tensor,
        key: Tensor,
        value: Tensor,
        embed_dim_to_check: int,
        num_heads: int,
        in_proj_weight: Optional[Tensor],
        in_proj_bias: Optional[Tensor],
        bias_k: Optional[Tensor],
        bias_v: Optional[Tensor],
        add_zero_attn: bool,
        dropout_p: float,
        out_proj_weight: Tensor,
        out_proj_bias: Optional[Tensor],
        training: bool = True,
        key_padding_mask: Optional[Tensor] = None,
        need_weights: bool = True,
        attn_mask: Optional[Tensor] = None,
        use_separate_proj_weight: bool = False,
        q_proj_weight: Optional[Tensor] = None,
        k_proj_weight: Optional[Tensor] = None,
        v_proj_weight: Optional[Tensor] = None,
        static_k: Optional[Tensor] = None,
        static_v: Optional[Tensor] = None,
        average_attn_weights: bool = True,
        is_causal: bool = False,
    ) -> tuple[Tensor, Optional[Tensor]]:
        r"""Forward method for MultiHeadAttention.
    
        See :class:`torch.nn.MultiheadAttention` for details.
    
        Args:
            query, key, value: map a query and a set of key-value pairs to an output.
                See "Attention Is All You Need" for more details.
            embed_dim_to_check: total dimension of the model.
            num_heads: parallel attention heads.
            in_proj_weight, in_proj_bias: input projection weight and bias.
            bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
            add_zero_attn: add a new batch of zeros to the key and
                           value sequences at dim=1.
            dropout_p: probability of an element to be zeroed.
            out_proj_weight, out_proj_bias: the output projection weight and bias.
            training: apply dropout if is ``True``.
            key_padding_mask: if provided, specified padding elements in the key will
                be ignored by the attention. This is an binary mask. When the value is True,
                the corresponding value on the attention layer will be filled with -inf.
            need_weights: output attn_output_weights.
                Default: `True`
                Note: `needs_weight` defaults to `True`, but should be set to `False`
                For best performance when attention weights are not needed.
                *Setting needs_weights to `True`
                leads to a significant performance degradation.*
            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
                the batches while a 3D mask allows to specify a different mask for the entries of each batch.
            is_causal: If specified, applies a causal mask as attention mask, and ignores
                attn_mask for computing scaled dot product attention.
                Default: ``False``.
                .. warning::
                    is_causal is provides a hint that the attn_mask is the
                    causal mask.Providing incorrect hints can result in
                    incorrect execution, including forward and backward
                    compatibility.
            use_separate_proj_weight: the function accept the proj. weights for query, key,
                and value in different forms. If false, in_proj_weight will be used, which is
                a combination of q_proj_weight, k_proj_weight, v_proj_weight.
            q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
            static_k, static_v: static key and value used for attention operators.
            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.
                Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect
                when ``need_weights=True.``. Default: True
    
    
        Shape:
            Inputs:
            - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
              the embedding dimension.
            - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.
              If a FloatTensor is provided, it will be directly added to the value.
              If a BoolTensor is provided, the positions with the
              value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
            - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
              3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
              S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked
              positions. If a BoolTensor is provided, positions with ``True``
              are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
              is provided, it will be added to the attention weight.
            - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
            - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
    
            Outputs:
            - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
              E is the embedding dimension.
            - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns
              attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or
              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and
              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per
              head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.
        """
        tens_ops = (
            query,
            key,
            value,
            in_proj_weight,
            in_proj_bias,
            bias_k,
            bias_v,
            out_proj_weight,
            out_proj_bias,
        )
        if has_torch_function(tens_ops):
            return handle_torch_function(
                multi_head_attention_forward,
                tens_ops,
                query,
                key,
                value,
                embed_dim_to_check,
                num_heads,
                in_proj_weight,
                in_proj_bias,
                bias_k,
                bias_v,
                add_zero_attn,
                dropout_p,
                out_proj_weight,
                out_proj_bias,
                training=training,
                key_padding_mask=key_padding_mask,
                need_weights=need_weights,
                attn_mask=attn_mask,
                is_causal=is_causal,
                use_separate_proj_weight=use_separate_proj_weight,
                q_proj_weight=q_proj_weight,
                k_proj_weight=k_proj_weight,
                v_proj_weight=v_proj_weight,
                static_k=static_k,
                static_v=static_v,
                average_attn_weights=average_attn_weights,
            )
    
        is_batched = _mha_shape_check(
            query, key, value, key_padding_mask, attn_mask, num_heads
        )
    
        # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input
        # is batched, run the computation and before returning squeeze the
        # batch dimension so that the output doesn't carry this temporary batch dimension.
        if not is_batched:
            # unsqueeze if the input is unbatched
            query = query.unsqueeze(1)
            key = key.unsqueeze(1)
            value = value.unsqueeze(1)
            if key_padding_mask is not None:
                key_padding_mask = key_padding_mask.unsqueeze(0)
    
        # set up shape vars
        tgt_len, bsz, embed_dim = query.shape
        src_len, _, _ = key.shape
    
        key_padding_mask = _canonical_mask(
            mask=key_padding_mask,
            mask_name="key_padding_mask",
            other_type=_none_or_dtype(attn_mask),
            other_name="attn_mask",
            target_type=query.dtype,
        )
    
        if is_causal and attn_mask is None:
            raise RuntimeError(
                "Need attn_mask if specifying the is_causal hint. "
                "You may use the Transformer module method "
                "`generate_square_subsequent_mask` to create this mask."
            )
    
        if is_causal and key_padding_mask is None and not need_weights:
            # when we have a kpm or need weights, we need attn_mask
            # Otherwise, we use the is_causal hint go as is_causal
            # indicator to SDPA.
            attn_mask = None
        else:
            attn_mask = _canonical_mask(
                mask=attn_mask,
                mask_name="attn_mask",
                other_type=None,
                other_name="",
                target_type=query.dtype,
                check_other=False,
            )
    
            if key_padding_mask is not None:
                # We have the attn_mask, and use that to merge kpm into it.
                # Turn off use of is_causal hint, as the merged mask is no
                # longer causal.
                is_causal = False
    
        assert embed_dim == embed_dim_to_check, (
            f"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}"
        )
        if isinstance(embed_dim, torch.Tensor):
            # embed_dim can be a tensor when JIT tracing
            head_dim = embed_dim.div(num_heads, rounding_mode="trunc")
        else:
            head_dim = embed_dim // num_heads
        assert head_dim * num_heads == embed_dim, (
            f"embed_dim {embed_dim} not divisible by num_heads {num_heads}"
        )
        if use_separate_proj_weight:
            # allow MHA to have different embedding dimensions when separate projection weights are used
            assert key.shape[:2] == value.shape[:2], (
                f"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}"
            )
        else:
            assert key.shape == value.shape, (
                f"key shape {key.shape} does not match value shape {value.shape}"
            )
    
        #
        # compute in-projection
        #
        if not use_separate_proj_weight:
            assert in_proj_weight is not None, (
                "use_separate_proj_weight is False but in_proj_weight is None"
            )
            q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
        else:
            assert q_proj_weight is not None, (
                "use_separate_proj_weight is True but q_proj_weight is None"
            )
            assert k_proj_weight is not None, (
                "use_separate_proj_weight is True but k_proj_weight is None"
            )
            assert v_proj_weight is not None, (
                "use_separate_proj_weight is True but v_proj_weight is None"
            )
            if in_proj_bias is None:
                b_q = b_k = b_v = None
            else:
                b_q, b_k, b_v = in_proj_bias.chunk(3)
            q, k, v = _in_projection(
                query,
                key,
                value,
                q_proj_weight,
                k_proj_weight,
                v_proj_weight,
                b_q,
                b_k,
                b_v,
            )
    
        # prep attention mask
    
        if attn_mask is not None:
            # ensure attn_mask's dim is 3
            if attn_mask.dim() == 2:
                correct_2d_size = (tgt_len, src_len)
                if attn_mask.shape != correct_2d_size:
                    raise RuntimeError(
                        f"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}."
                    )
                attn_mask = attn_mask.unsqueeze(0)
            elif attn_mask.dim() == 3:
                correct_3d_size = (bsz * num_heads, tgt_len, src_len)
                if attn_mask.shape != correct_3d_size:
                    raise RuntimeError(
                        f"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}."
                    )
            else:
                raise RuntimeError(
                    f"attn_mask's dimension {attn_mask.dim()} is not supported"
                )
    
        # add bias along batch dimension (currently second)
        if bias_k is not None and bias_v is not None:
            assert static_k is None, "bias cannot be added to static key."
            assert static_v is None, "bias cannot be added to static value."
            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
        else:
            assert bias_k is None
            assert bias_v is None
    
        #
        # reshape q, k, v for multihead attention and make them batch first
        #
        q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
        if static_k is None:
            k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_k.size(0) == bsz * num_heads, (
                f"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}"
            )
            assert static_k.size(2) == head_dim, (
                f"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}"
            )
            k = static_k
        if static_v is None:
            v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_v.size(0) == bsz * num_heads, (
                f"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}"
            )
            assert static_v.size(2) == head_dim, (
                f"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}"
            )
            v = static_v
    
        # add zero attention along batch dimension (now first)
        if add_zero_attn:
            zero_attn_shape = (bsz * num_heads, 1, head_dim)
            k = torch.cat(
                [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1
            )
            v = torch.cat(
                [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1
            )
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
    
        # update source sequence length after adjustments
        src_len = k.size(1)
    
        # merge key padding and attention masks
        if key_padding_mask is not None:
            if not torch.jit.is_scripting() and not torch.jit.is_tracing():
                _check_key_padding_mask(key_padding_mask, src_len, bsz)
    
            key_padding_mask = (
                key_padding_mask.view(bsz, 1, 1, src_len)
                .expand(-1, num_heads, -1, -1)
                .reshape(bsz * num_heads, 1, src_len)
            )
            if attn_mask is None:
                attn_mask = key_padding_mask
            else:
                attn_mask = attn_mask + key_padding_mask
    
        # adjust dropout probability
        if not training:
            dropout_p = 0.0
    
        #
        # (deep breath) calculate attention and out projection
        #
    
        if need_weights:
            _B, _Nt, E = q.shape
            q_scaled = q * math.sqrt(1.0 / float(E))
    
            assert not (is_causal and attn_mask is None), (
                "FIXME: is_causal not implemented for need_weights"
            )
    
            if attn_mask is not None:
                attn_output_weights = torch.baddbmm(
                    attn_mask, q_scaled, k.transpose(-2, -1)
                )
            else:
                attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
            attn_output_weights = softmax(attn_output_weights, dim=-1)
            if dropout_p > 0.0:
                attn_output_weights = dropout(attn_output_weights, p=dropout_p)
    
            attn_output = torch.bmm(attn_output_weights, v)
    
            attn_output = (
                attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
            )
            attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
            attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
    
            # optionally average attention weights over heads
            attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
            if average_attn_weights:
                attn_output_weights = attn_output_weights.mean(dim=1)
    
            if not is_batched:
                # squeeze the output if input was unbatched
                attn_output = attn_output.squeeze(1)
                attn_output_weights = attn_output_weights.squeeze(0)
            return attn_output, attn_output_weights
        else:
            # attn_mask can be either (L,S) or (N*num_heads, L, S)
            # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)
            # in order to match the input for SDPA of (N, num_heads, L, S)
            if attn_mask is not None:
                if attn_mask.size(0) == 1 and attn_mask.dim() == 3:
                    attn_mask = attn_mask.unsqueeze(0)
                else:
                    attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)
    
            q = q.view(bsz, num_heads, tgt_len, head_dim)
            k = k.view(bsz, num_heads, src_len, head_dim)
            v = v.view(bsz, num_heads, src_len, head_dim)
    
>           attn_output = scaled_dot_product_attention(
                q, k, v, attn_mask, dropout_p, is_causal
            )
E           RuntimeError: cuDNN Frontend error: CUDNN_BACKEND_TENSOR_DESCRIPTOR cudnnFinalize failedptrDesc->finalize() cudnn_status: CUDNN_STATUS_SUBLIBRARY_LOADING_FAILED

/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6487: RuntimeError
___________________ test_mha_accuracy[causal-small-2-dtype2] ___________________

dtype = torch.bfloat16, bs = 2, model = 'small', mask_type = 'causal'

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("mask_type", mask_types)
    def test_mha_accuracy(dtype, bs, model, mask_type):
        config = model_configs[model]
    
        te_mha = MultiheadAttention(
            config.hidden_size,
            config.num_heads,
            fuse_qkv_params=True,
            params_dtype=dtype,
            qkv_weight_interleaved=False,
            input_layernorm=False,
            device="cuda",
        ).eval()
    
        torch_mha = (
            TorchMHA(
                config.hidden_size,
                config.num_heads,
            )
            .to(dtype=dtype)
            .cuda()
            .eval()
        )
    
        # Share params
        with torch.no_grad():
            torch_mha.mhsa.in_proj_weight = Parameter(te_mha.qkv.weight.clone())
            torch_mha.mhsa.in_proj_bias = Parameter(te_mha.qkv.bias.clone())
            torch_mha.mhsa.out_proj.weight = Parameter(te_mha.proj.weight.clone())
            torch_mha.mhsa.out_proj.bias = Parameter(te_mha.proj.bias.clone())
    
        te_outputs = _test_mha_accuracy(te_mha, bs, dtype, config, mask_type, te=True)
>       torch_outputs = _test_mha_accuracy(torch_mha, bs, dtype, config, mask_type, te=False)

tests/pytorch/test_numerics.py:1074: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/test_numerics.py:1027: in _test_mha_accuracy
    out = block(inp_hidden_states, **forward_kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:426: in forward
    output = self.mhsa(x, x, x, attn_mask=attention_mask, need_weights=False)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1488: in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = tensor([[[-1.6172,  0.5703, -0.5117,  ..., -1.3516,  2.7031, -0.1680],
         [-0.3223,  1.4297,  0.1250,  ..., -1.7...-0.4219, -0.0598,  ...,  0.2461,  0.7539, -0.2754]]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
key = tensor([[[-1.6172,  0.5703, -0.5117,  ..., -1.3516,  2.7031, -0.1680],
         [-0.3223,  1.4297,  0.1250,  ..., -1.7...-0.4219, -0.0598,  ...,  0.2461,  0.7539, -0.2754]]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
value = tensor([[[-1.6172,  0.5703, -0.5117,  ..., -1.3516,  2.7031, -0.1680],
         [-0.3223,  1.4297,  0.1250,  ..., -1.7...-0.4219, -0.0598,  ...,  0.2461,  0.7539, -0.2754]]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
embed_dim_to_check = 128, num_heads = 8
in_proj_weight = Parameter containing:
tensor([[ 1.4587e-02,  3.7598e-02, -7.9346e-03,  ..., -6.2256e-03,
          7.5340e-05,  1.5335....., -3.3936e-02,
         -1.2451e-02,  3.1281e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
in_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
bias_k = None, bias_v = None, add_zero_attn = False, dropout_p = 0.0
out_proj_weight = Parameter containing:
tensor([[-0.0037,  0.0415, -0.0242,  ...,  0.0052, -0.0132, -0.0171],
        [ 0.0056,  0.0354,... -0.0457,  0.0081,  ..., -0.0051, -0.0339,  0.0095]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
out_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ... 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
training = False, key_padding_mask = None, need_weights = False
attn_mask = tensor([[[[0., -inf, -inf,  ..., -inf, -inf, -inf],
          [0., 0., -inf,  ..., -inf, -inf, -inf],
          [0., 0...0., 0.,  ..., 0., 0., -inf],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
use_separate_proj_weight = False, q_proj_weight = None, k_proj_weight = None
v_proj_weight = None, static_k = None, static_v = None
average_attn_weights = True, is_causal = False

    def multi_head_attention_forward(
        query: Tensor,
        key: Tensor,
        value: Tensor,
        embed_dim_to_check: int,
        num_heads: int,
        in_proj_weight: Optional[Tensor],
        in_proj_bias: Optional[Tensor],
        bias_k: Optional[Tensor],
        bias_v: Optional[Tensor],
        add_zero_attn: bool,
        dropout_p: float,
        out_proj_weight: Tensor,
        out_proj_bias: Optional[Tensor],
        training: bool = True,
        key_padding_mask: Optional[Tensor] = None,
        need_weights: bool = True,
        attn_mask: Optional[Tensor] = None,
        use_separate_proj_weight: bool = False,
        q_proj_weight: Optional[Tensor] = None,
        k_proj_weight: Optional[Tensor] = None,
        v_proj_weight: Optional[Tensor] = None,
        static_k: Optional[Tensor] = None,
        static_v: Optional[Tensor] = None,
        average_attn_weights: bool = True,
        is_causal: bool = False,
    ) -> tuple[Tensor, Optional[Tensor]]:
        r"""Forward method for MultiHeadAttention.
    
        See :class:`torch.nn.MultiheadAttention` for details.
    
        Args:
            query, key, value: map a query and a set of key-value pairs to an output.
                See "Attention Is All You Need" for more details.
            embed_dim_to_check: total dimension of the model.
            num_heads: parallel attention heads.
            in_proj_weight, in_proj_bias: input projection weight and bias.
            bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
            add_zero_attn: add a new batch of zeros to the key and
                           value sequences at dim=1.
            dropout_p: probability of an element to be zeroed.
            out_proj_weight, out_proj_bias: the output projection weight and bias.
            training: apply dropout if is ``True``.
            key_padding_mask: if provided, specified padding elements in the key will
                be ignored by the attention. This is an binary mask. When the value is True,
                the corresponding value on the attention layer will be filled with -inf.
            need_weights: output attn_output_weights.
                Default: `True`
                Note: `needs_weight` defaults to `True`, but should be set to `False`
                For best performance when attention weights are not needed.
                *Setting needs_weights to `True`
                leads to a significant performance degradation.*
            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
                the batches while a 3D mask allows to specify a different mask for the entries of each batch.
            is_causal: If specified, applies a causal mask as attention mask, and ignores
                attn_mask for computing scaled dot product attention.
                Default: ``False``.
                .. warning::
                    is_causal is provides a hint that the attn_mask is the
                    causal mask.Providing incorrect hints can result in
                    incorrect execution, including forward and backward
                    compatibility.
            use_separate_proj_weight: the function accept the proj. weights for query, key,
                and value in different forms. If false, in_proj_weight will be used, which is
                a combination of q_proj_weight, k_proj_weight, v_proj_weight.
            q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
            static_k, static_v: static key and value used for attention operators.
            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.
                Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect
                when ``need_weights=True.``. Default: True
    
    
        Shape:
            Inputs:
            - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
              the embedding dimension.
            - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.
              If a FloatTensor is provided, it will be directly added to the value.
              If a BoolTensor is provided, the positions with the
              value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
            - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
              3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
              S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked
              positions. If a BoolTensor is provided, positions with ``True``
              are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
              is provided, it will be added to the attention weight.
            - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
            - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
    
            Outputs:
            - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
              E is the embedding dimension.
            - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns
              attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or
              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and
              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per
              head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.
        """
        tens_ops = (
            query,
            key,
            value,
            in_proj_weight,
            in_proj_bias,
            bias_k,
            bias_v,
            out_proj_weight,
            out_proj_bias,
        )
        if has_torch_function(tens_ops):
            return handle_torch_function(
                multi_head_attention_forward,
                tens_ops,
                query,
                key,
                value,
                embed_dim_to_check,
                num_heads,
                in_proj_weight,
                in_proj_bias,
                bias_k,
                bias_v,
                add_zero_attn,
                dropout_p,
                out_proj_weight,
                out_proj_bias,
                training=training,
                key_padding_mask=key_padding_mask,
                need_weights=need_weights,
                attn_mask=attn_mask,
                is_causal=is_causal,
                use_separate_proj_weight=use_separate_proj_weight,
                q_proj_weight=q_proj_weight,
                k_proj_weight=k_proj_weight,
                v_proj_weight=v_proj_weight,
                static_k=static_k,
                static_v=static_v,
                average_attn_weights=average_attn_weights,
            )
    
        is_batched = _mha_shape_check(
            query, key, value, key_padding_mask, attn_mask, num_heads
        )
    
        # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input
        # is batched, run the computation and before returning squeeze the
        # batch dimension so that the output doesn't carry this temporary batch dimension.
        if not is_batched:
            # unsqueeze if the input is unbatched
            query = query.unsqueeze(1)
            key = key.unsqueeze(1)
            value = value.unsqueeze(1)
            if key_padding_mask is not None:
                key_padding_mask = key_padding_mask.unsqueeze(0)
    
        # set up shape vars
        tgt_len, bsz, embed_dim = query.shape
        src_len, _, _ = key.shape
    
        key_padding_mask = _canonical_mask(
            mask=key_padding_mask,
            mask_name="key_padding_mask",
            other_type=_none_or_dtype(attn_mask),
            other_name="attn_mask",
            target_type=query.dtype,
        )
    
        if is_causal and attn_mask is None:
            raise RuntimeError(
                "Need attn_mask if specifying the is_causal hint. "
                "You may use the Transformer module method "
                "`generate_square_subsequent_mask` to create this mask."
            )
    
        if is_causal and key_padding_mask is None and not need_weights:
            # when we have a kpm or need weights, we need attn_mask
            # Otherwise, we use the is_causal hint go as is_causal
            # indicator to SDPA.
            attn_mask = None
        else:
            attn_mask = _canonical_mask(
                mask=attn_mask,
                mask_name="attn_mask",
                other_type=None,
                other_name="",
                target_type=query.dtype,
                check_other=False,
            )
    
            if key_padding_mask is not None:
                # We have the attn_mask, and use that to merge kpm into it.
                # Turn off use of is_causal hint, as the merged mask is no
                # longer causal.
                is_causal = False
    
        assert embed_dim == embed_dim_to_check, (
            f"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}"
        )
        if isinstance(embed_dim, torch.Tensor):
            # embed_dim can be a tensor when JIT tracing
            head_dim = embed_dim.div(num_heads, rounding_mode="trunc")
        else:
            head_dim = embed_dim // num_heads
        assert head_dim * num_heads == embed_dim, (
            f"embed_dim {embed_dim} not divisible by num_heads {num_heads}"
        )
        if use_separate_proj_weight:
            # allow MHA to have different embedding dimensions when separate projection weights are used
            assert key.shape[:2] == value.shape[:2], (
                f"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}"
            )
        else:
            assert key.shape == value.shape, (
                f"key shape {key.shape} does not match value shape {value.shape}"
            )
    
        #
        # compute in-projection
        #
        if not use_separate_proj_weight:
            assert in_proj_weight is not None, (
                "use_separate_proj_weight is False but in_proj_weight is None"
            )
            q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
        else:
            assert q_proj_weight is not None, (
                "use_separate_proj_weight is True but q_proj_weight is None"
            )
            assert k_proj_weight is not None, (
                "use_separate_proj_weight is True but k_proj_weight is None"
            )
            assert v_proj_weight is not None, (
                "use_separate_proj_weight is True but v_proj_weight is None"
            )
            if in_proj_bias is None:
                b_q = b_k = b_v = None
            else:
                b_q, b_k, b_v = in_proj_bias.chunk(3)
            q, k, v = _in_projection(
                query,
                key,
                value,
                q_proj_weight,
                k_proj_weight,
                v_proj_weight,
                b_q,
                b_k,
                b_v,
            )
    
        # prep attention mask
    
        if attn_mask is not None:
            # ensure attn_mask's dim is 3
            if attn_mask.dim() == 2:
                correct_2d_size = (tgt_len, src_len)
                if attn_mask.shape != correct_2d_size:
                    raise RuntimeError(
                        f"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}."
                    )
                attn_mask = attn_mask.unsqueeze(0)
            elif attn_mask.dim() == 3:
                correct_3d_size = (bsz * num_heads, tgt_len, src_len)
                if attn_mask.shape != correct_3d_size:
                    raise RuntimeError(
                        f"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}."
                    )
            else:
                raise RuntimeError(
                    f"attn_mask's dimension {attn_mask.dim()} is not supported"
                )
    
        # add bias along batch dimension (currently second)
        if bias_k is not None and bias_v is not None:
            assert static_k is None, "bias cannot be added to static key."
            assert static_v is None, "bias cannot be added to static value."
            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
        else:
            assert bias_k is None
            assert bias_v is None
    
        #
        # reshape q, k, v for multihead attention and make them batch first
        #
        q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
        if static_k is None:
            k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_k.size(0) == bsz * num_heads, (
                f"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}"
            )
            assert static_k.size(2) == head_dim, (
                f"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}"
            )
            k = static_k
        if static_v is None:
            v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_v.size(0) == bsz * num_heads, (
                f"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}"
            )
            assert static_v.size(2) == head_dim, (
                f"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}"
            )
            v = static_v
    
        # add zero attention along batch dimension (now first)
        if add_zero_attn:
            zero_attn_shape = (bsz * num_heads, 1, head_dim)
            k = torch.cat(
                [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1
            )
            v = torch.cat(
                [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1
            )
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
    
        # update source sequence length after adjustments
        src_len = k.size(1)
    
        # merge key padding and attention masks
        if key_padding_mask is not None:
            if not torch.jit.is_scripting() and not torch.jit.is_tracing():
                _check_key_padding_mask(key_padding_mask, src_len, bsz)
    
            key_padding_mask = (
                key_padding_mask.view(bsz, 1, 1, src_len)
                .expand(-1, num_heads, -1, -1)
                .reshape(bsz * num_heads, 1, src_len)
            )
            if attn_mask is None:
                attn_mask = key_padding_mask
            else:
                attn_mask = attn_mask + key_padding_mask
    
        # adjust dropout probability
        if not training:
            dropout_p = 0.0
    
        #
        # (deep breath) calculate attention and out projection
        #
    
        if need_weights:
            _B, _Nt, E = q.shape
            q_scaled = q * math.sqrt(1.0 / float(E))
    
            assert not (is_causal and attn_mask is None), (
                "FIXME: is_causal not implemented for need_weights"
            )
    
            if attn_mask is not None:
                attn_output_weights = torch.baddbmm(
                    attn_mask, q_scaled, k.transpose(-2, -1)
                )
            else:
                attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
            attn_output_weights = softmax(attn_output_weights, dim=-1)
            if dropout_p > 0.0:
                attn_output_weights = dropout(attn_output_weights, p=dropout_p)
    
            attn_output = torch.bmm(attn_output_weights, v)
    
            attn_output = (
                attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
            )
            attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
            attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
    
            # optionally average attention weights over heads
            attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
            if average_attn_weights:
                attn_output_weights = attn_output_weights.mean(dim=1)
    
            if not is_batched:
                # squeeze the output if input was unbatched
                attn_output = attn_output.squeeze(1)
                attn_output_weights = attn_output_weights.squeeze(0)
            return attn_output, attn_output_weights
        else:
            # attn_mask can be either (L,S) or (N*num_heads, L, S)
            # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)
            # in order to match the input for SDPA of (N, num_heads, L, S)
            if attn_mask is not None:
                if attn_mask.size(0) == 1 and attn_mask.dim() == 3:
                    attn_mask = attn_mask.unsqueeze(0)
                else:
                    attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)
    
            q = q.view(bsz, num_heads, tgt_len, head_dim)
            k = k.view(bsz, num_heads, src_len, head_dim)
            v = v.view(bsz, num_heads, src_len, head_dim)
    
>           attn_output = scaled_dot_product_attention(
                q, k, v, attn_mask, dropout_p, is_causal
            )
E           RuntimeError: cuDNN Frontend error: CUDNN_BACKEND_TENSOR_DESCRIPTOR cudnnFinalize failedptrDesc->finalize() cudnn_status: CUDNN_STATUS_SUBLIBRARY_LOADING_FAILED

/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6487: RuntimeError
__________________ test_mha_accuracy[no_mask-small-1-dtype1] ___________________

dtype = torch.float16, bs = 1, model = 'small', mask_type = 'no_mask'

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("mask_type", mask_types)
    def test_mha_accuracy(dtype, bs, model, mask_type):
        config = model_configs[model]
    
        te_mha = MultiheadAttention(
            config.hidden_size,
            config.num_heads,
            fuse_qkv_params=True,
            params_dtype=dtype,
            qkv_weight_interleaved=False,
            input_layernorm=False,
            device="cuda",
        ).eval()
    
        torch_mha = (
            TorchMHA(
                config.hidden_size,
                config.num_heads,
            )
            .to(dtype=dtype)
            .cuda()
            .eval()
        )
    
        # Share params
        with torch.no_grad():
            torch_mha.mhsa.in_proj_weight = Parameter(te_mha.qkv.weight.clone())
            torch_mha.mhsa.in_proj_bias = Parameter(te_mha.qkv.bias.clone())
            torch_mha.mhsa.out_proj.weight = Parameter(te_mha.proj.weight.clone())
            torch_mha.mhsa.out_proj.bias = Parameter(te_mha.proj.bias.clone())
    
        te_outputs = _test_mha_accuracy(te_mha, bs, dtype, config, mask_type, te=True)
>       torch_outputs = _test_mha_accuracy(torch_mha, bs, dtype, config, mask_type, te=False)

tests/pytorch/test_numerics.py:1074: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/test_numerics.py:1027: in _test_mha_accuracy
    out = block(inp_hidden_states, **forward_kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:426: in forward
    output = self.mhsa(x, x, x, attn_mask=attention_mask, need_weights=False)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1488: in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = tensor([[[-1.6162,  0.5684, -0.5103,  ..., -1.3506,  2.7090, -0.1680]],

        [[-0.3225,  1.4258,  0.1251,  ..., -1...  0.7158,  0.8496,  ..., -1.2715, -0.7954, -1.6895]]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
key = tensor([[[-1.6162,  0.5684, -0.5103,  ..., -1.3506,  2.7090, -0.1680]],

        [[-0.3225,  1.4258,  0.1251,  ..., -1...  0.7158,  0.8496,  ..., -1.2715, -0.7954, -1.6895]]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
value = tensor([[[-1.6162,  0.5684, -0.5103,  ..., -1.3506,  2.7090, -0.1680]],

        [[-0.3225,  1.4258,  0.1251,  ..., -1...  0.7158,  0.8496,  ..., -1.2715, -0.7954, -1.6895]]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
embed_dim_to_check = 128, num_heads = 8
in_proj_weight = Parameter containing:
tensor([[ 1.4565e-02,  3.7628e-02, -7.9575e-03,  ..., -6.2141e-03,
          7.5221e-05,  1.5306......, -3.4058e-02,
         -1.2466e-02,  3.1300e-03]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
in_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
bias_k = None, bias_v = None, add_zero_attn = False, dropout_p = 0.0
out_proj_weight = Parameter containing:
tensor([[-0.0037,  0.0416, -0.0242,  ...,  0.0051, -0.0132, -0.0170],
        [ 0.0056,  0.0353,..., -0.0457,  0.0080,  ..., -0.0051, -0.0340,  0.0095]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
out_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ..., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
training = False, key_padding_mask = None, need_weights = False
attn_mask = None, use_separate_proj_weight = False, q_proj_weight = None
k_proj_weight = None, v_proj_weight = None, static_k = None, static_v = None
average_attn_weights = True, is_causal = False

    def multi_head_attention_forward(
        query: Tensor,
        key: Tensor,
        value: Tensor,
        embed_dim_to_check: int,
        num_heads: int,
        in_proj_weight: Optional[Tensor],
        in_proj_bias: Optional[Tensor],
        bias_k: Optional[Tensor],
        bias_v: Optional[Tensor],
        add_zero_attn: bool,
        dropout_p: float,
        out_proj_weight: Tensor,
        out_proj_bias: Optional[Tensor],
        training: bool = True,
        key_padding_mask: Optional[Tensor] = None,
        need_weights: bool = True,
        attn_mask: Optional[Tensor] = None,
        use_separate_proj_weight: bool = False,
        q_proj_weight: Optional[Tensor] = None,
        k_proj_weight: Optional[Tensor] = None,
        v_proj_weight: Optional[Tensor] = None,
        static_k: Optional[Tensor] = None,
        static_v: Optional[Tensor] = None,
        average_attn_weights: bool = True,
        is_causal: bool = False,
    ) -> tuple[Tensor, Optional[Tensor]]:
        r"""Forward method for MultiHeadAttention.
    
        See :class:`torch.nn.MultiheadAttention` for details.
    
        Args:
            query, key, value: map a query and a set of key-value pairs to an output.
                See "Attention Is All You Need" for more details.
            embed_dim_to_check: total dimension of the model.
            num_heads: parallel attention heads.
            in_proj_weight, in_proj_bias: input projection weight and bias.
            bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
            add_zero_attn: add a new batch of zeros to the key and
                           value sequences at dim=1.
            dropout_p: probability of an element to be zeroed.
            out_proj_weight, out_proj_bias: the output projection weight and bias.
            training: apply dropout if is ``True``.
            key_padding_mask: if provided, specified padding elements in the key will
                be ignored by the attention. This is an binary mask. When the value is True,
                the corresponding value on the attention layer will be filled with -inf.
            need_weights: output attn_output_weights.
                Default: `True`
                Note: `needs_weight` defaults to `True`, but should be set to `False`
                For best performance when attention weights are not needed.
                *Setting needs_weights to `True`
                leads to a significant performance degradation.*
            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
                the batches while a 3D mask allows to specify a different mask for the entries of each batch.
            is_causal: If specified, applies a causal mask as attention mask, and ignores
                attn_mask for computing scaled dot product attention.
                Default: ``False``.
                .. warning::
                    is_causal is provides a hint that the attn_mask is the
                    causal mask.Providing incorrect hints can result in
                    incorrect execution, including forward and backward
                    compatibility.
            use_separate_proj_weight: the function accept the proj. weights for query, key,
                and value in different forms. If false, in_proj_weight will be used, which is
                a combination of q_proj_weight, k_proj_weight, v_proj_weight.
            q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
            static_k, static_v: static key and value used for attention operators.
            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.
                Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect
                when ``need_weights=True.``. Default: True
    
    
        Shape:
            Inputs:
            - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
              the embedding dimension.
            - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.
              If a FloatTensor is provided, it will be directly added to the value.
              If a BoolTensor is provided, the positions with the
              value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
            - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
              3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
              S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked
              positions. If a BoolTensor is provided, positions with ``True``
              are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
              is provided, it will be added to the attention weight.
            - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
            - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
    
            Outputs:
            - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
              E is the embedding dimension.
            - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns
              attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or
              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and
              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per
              head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.
        """
        tens_ops = (
            query,
            key,
            value,
            in_proj_weight,
            in_proj_bias,
            bias_k,
            bias_v,
            out_proj_weight,
            out_proj_bias,
        )
        if has_torch_function(tens_ops):
            return handle_torch_function(
                multi_head_attention_forward,
                tens_ops,
                query,
                key,
                value,
                embed_dim_to_check,
                num_heads,
                in_proj_weight,
                in_proj_bias,
                bias_k,
                bias_v,
                add_zero_attn,
                dropout_p,
                out_proj_weight,
                out_proj_bias,
                training=training,
                key_padding_mask=key_padding_mask,
                need_weights=need_weights,
                attn_mask=attn_mask,
                is_causal=is_causal,
                use_separate_proj_weight=use_separate_proj_weight,
                q_proj_weight=q_proj_weight,
                k_proj_weight=k_proj_weight,
                v_proj_weight=v_proj_weight,
                static_k=static_k,
                static_v=static_v,
                average_attn_weights=average_attn_weights,
            )
    
        is_batched = _mha_shape_check(
            query, key, value, key_padding_mask, attn_mask, num_heads
        )
    
        # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input
        # is batched, run the computation and before returning squeeze the
        # batch dimension so that the output doesn't carry this temporary batch dimension.
        if not is_batched:
            # unsqueeze if the input is unbatched
            query = query.unsqueeze(1)
            key = key.unsqueeze(1)
            value = value.unsqueeze(1)
            if key_padding_mask is not None:
                key_padding_mask = key_padding_mask.unsqueeze(0)
    
        # set up shape vars
        tgt_len, bsz, embed_dim = query.shape
        src_len, _, _ = key.shape
    
        key_padding_mask = _canonical_mask(
            mask=key_padding_mask,
            mask_name="key_padding_mask",
            other_type=_none_or_dtype(attn_mask),
            other_name="attn_mask",
            target_type=query.dtype,
        )
    
        if is_causal and attn_mask is None:
            raise RuntimeError(
                "Need attn_mask if specifying the is_causal hint. "
                "You may use the Transformer module method "
                "`generate_square_subsequent_mask` to create this mask."
            )
    
        if is_causal and key_padding_mask is None and not need_weights:
            # when we have a kpm or need weights, we need attn_mask
            # Otherwise, we use the is_causal hint go as is_causal
            # indicator to SDPA.
            attn_mask = None
        else:
            attn_mask = _canonical_mask(
                mask=attn_mask,
                mask_name="attn_mask",
                other_type=None,
                other_name="",
                target_type=query.dtype,
                check_other=False,
            )
    
            if key_padding_mask is not None:
                # We have the attn_mask, and use that to merge kpm into it.
                # Turn off use of is_causal hint, as the merged mask is no
                # longer causal.
                is_causal = False
    
        assert embed_dim == embed_dim_to_check, (
            f"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}"
        )
        if isinstance(embed_dim, torch.Tensor):
            # embed_dim can be a tensor when JIT tracing
            head_dim = embed_dim.div(num_heads, rounding_mode="trunc")
        else:
            head_dim = embed_dim // num_heads
        assert head_dim * num_heads == embed_dim, (
            f"embed_dim {embed_dim} not divisible by num_heads {num_heads}"
        )
        if use_separate_proj_weight:
            # allow MHA to have different embedding dimensions when separate projection weights are used
            assert key.shape[:2] == value.shape[:2], (
                f"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}"
            )
        else:
            assert key.shape == value.shape, (
                f"key shape {key.shape} does not match value shape {value.shape}"
            )
    
        #
        # compute in-projection
        #
        if not use_separate_proj_weight:
            assert in_proj_weight is not None, (
                "use_separate_proj_weight is False but in_proj_weight is None"
            )
            q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
        else:
            assert q_proj_weight is not None, (
                "use_separate_proj_weight is True but q_proj_weight is None"
            )
            assert k_proj_weight is not None, (
                "use_separate_proj_weight is True but k_proj_weight is None"
            )
            assert v_proj_weight is not None, (
                "use_separate_proj_weight is True but v_proj_weight is None"
            )
            if in_proj_bias is None:
                b_q = b_k = b_v = None
            else:
                b_q, b_k, b_v = in_proj_bias.chunk(3)
            q, k, v = _in_projection(
                query,
                key,
                value,
                q_proj_weight,
                k_proj_weight,
                v_proj_weight,
                b_q,
                b_k,
                b_v,
            )
    
        # prep attention mask
    
        if attn_mask is not None:
            # ensure attn_mask's dim is 3
            if attn_mask.dim() == 2:
                correct_2d_size = (tgt_len, src_len)
                if attn_mask.shape != correct_2d_size:
                    raise RuntimeError(
                        f"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}."
                    )
                attn_mask = attn_mask.unsqueeze(0)
            elif attn_mask.dim() == 3:
                correct_3d_size = (bsz * num_heads, tgt_len, src_len)
                if attn_mask.shape != correct_3d_size:
                    raise RuntimeError(
                        f"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}."
                    )
            else:
                raise RuntimeError(
                    f"attn_mask's dimension {attn_mask.dim()} is not supported"
                )
    
        # add bias along batch dimension (currently second)
        if bias_k is not None and bias_v is not None:
            assert static_k is None, "bias cannot be added to static key."
            assert static_v is None, "bias cannot be added to static value."
            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
        else:
            assert bias_k is None
            assert bias_v is None
    
        #
        # reshape q, k, v for multihead attention and make them batch first
        #
        q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
        if static_k is None:
            k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_k.size(0) == bsz * num_heads, (
                f"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}"
            )
            assert static_k.size(2) == head_dim, (
                f"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}"
            )
            k = static_k
        if static_v is None:
            v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_v.size(0) == bsz * num_heads, (
                f"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}"
            )
            assert static_v.size(2) == head_dim, (
                f"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}"
            )
            v = static_v
    
        # add zero attention along batch dimension (now first)
        if add_zero_attn:
            zero_attn_shape = (bsz * num_heads, 1, head_dim)
            k = torch.cat(
                [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1
            )
            v = torch.cat(
                [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1
            )
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
    
        # update source sequence length after adjustments
        src_len = k.size(1)
    
        # merge key padding and attention masks
        if key_padding_mask is not None:
            if not torch.jit.is_scripting() and not torch.jit.is_tracing():
                _check_key_padding_mask(key_padding_mask, src_len, bsz)
    
            key_padding_mask = (
                key_padding_mask.view(bsz, 1, 1, src_len)
                .expand(-1, num_heads, -1, -1)
                .reshape(bsz * num_heads, 1, src_len)
            )
            if attn_mask is None:
                attn_mask = key_padding_mask
            else:
                attn_mask = attn_mask + key_padding_mask
    
        # adjust dropout probability
        if not training:
            dropout_p = 0.0
    
        #
        # (deep breath) calculate attention and out projection
        #
    
        if need_weights:
            _B, _Nt, E = q.shape
            q_scaled = q * math.sqrt(1.0 / float(E))
    
            assert not (is_causal and attn_mask is None), (
                "FIXME: is_causal not implemented for need_weights"
            )
    
            if attn_mask is not None:
                attn_output_weights = torch.baddbmm(
                    attn_mask, q_scaled, k.transpose(-2, -1)
                )
            else:
                attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
            attn_output_weights = softmax(attn_output_weights, dim=-1)
            if dropout_p > 0.0:
                attn_output_weights = dropout(attn_output_weights, p=dropout_p)
    
            attn_output = torch.bmm(attn_output_weights, v)
    
            attn_output = (
                attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
            )
            attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
            attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
    
            # optionally average attention weights over heads
            attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
            if average_attn_weights:
                attn_output_weights = attn_output_weights.mean(dim=1)
    
            if not is_batched:
                # squeeze the output if input was unbatched
                attn_output = attn_output.squeeze(1)
                attn_output_weights = attn_output_weights.squeeze(0)
            return attn_output, attn_output_weights
        else:
            # attn_mask can be either (L,S) or (N*num_heads, L, S)
            # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)
            # in order to match the input for SDPA of (N, num_heads, L, S)
            if attn_mask is not None:
                if attn_mask.size(0) == 1 and attn_mask.dim() == 3:
                    attn_mask = attn_mask.unsqueeze(0)
                else:
                    attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)
    
            q = q.view(bsz, num_heads, tgt_len, head_dim)
            k = k.view(bsz, num_heads, src_len, head_dim)
            v = v.view(bsz, num_heads, src_len, head_dim)
    
>           attn_output = scaled_dot_product_attention(
                q, k, v, attn_mask, dropout_p, is_causal
            )
E           RuntimeError: cuDNN Frontend error: CUDNN_BACKEND_TENSOR_DESCRIPTOR cudnnFinalize failedptrDesc->finalize() cudnn_status: CUDNN_STATUS_SUBLIBRARY_LOADING_FAILED

/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6487: RuntimeError
__________________ test_mha_accuracy[no_mask-small-1-dtype2] ___________________

dtype = torch.bfloat16, bs = 1, model = 'small', mask_type = 'no_mask'

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("mask_type", mask_types)
    def test_mha_accuracy(dtype, bs, model, mask_type):
        config = model_configs[model]
    
        te_mha = MultiheadAttention(
            config.hidden_size,
            config.num_heads,
            fuse_qkv_params=True,
            params_dtype=dtype,
            qkv_weight_interleaved=False,
            input_layernorm=False,
            device="cuda",
        ).eval()
    
        torch_mha = (
            TorchMHA(
                config.hidden_size,
                config.num_heads,
            )
            .to(dtype=dtype)
            .cuda()
            .eval()
        )
    
        # Share params
        with torch.no_grad():
            torch_mha.mhsa.in_proj_weight = Parameter(te_mha.qkv.weight.clone())
            torch_mha.mhsa.in_proj_bias = Parameter(te_mha.qkv.bias.clone())
            torch_mha.mhsa.out_proj.weight = Parameter(te_mha.proj.weight.clone())
            torch_mha.mhsa.out_proj.bias = Parameter(te_mha.proj.bias.clone())
    
        te_outputs = _test_mha_accuracy(te_mha, bs, dtype, config, mask_type, te=True)
>       torch_outputs = _test_mha_accuracy(torch_mha, bs, dtype, config, mask_type, te=False)

tests/pytorch/test_numerics.py:1074: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/test_numerics.py:1027: in _test_mha_accuracy
    out = block(inp_hidden_states, **forward_kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:426: in forward
    output = self.mhsa(x, x, x, attn_mask=attention_mask, need_weights=False)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1488: in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = tensor([[[-1.6172,  0.5703, -0.5117,  ..., -1.3516,  2.7031, -0.1680]],

        [[-0.3223,  1.4297,  0.1250,  ..., -1... 0.7148,  0.8516,  ..., -1.2734, -0.7969, -1.6875]]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
key = tensor([[[-1.6172,  0.5703, -0.5117,  ..., -1.3516,  2.7031, -0.1680]],

        [[-0.3223,  1.4297,  0.1250,  ..., -1... 0.7148,  0.8516,  ..., -1.2734, -0.7969, -1.6875]]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
value = tensor([[[-1.6172,  0.5703, -0.5117,  ..., -1.3516,  2.7031, -0.1680]],

        [[-0.3223,  1.4297,  0.1250,  ..., -1... 0.7148,  0.8516,  ..., -1.2734, -0.7969, -1.6875]]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
embed_dim_to_check = 128, num_heads = 8
in_proj_weight = Parameter containing:
tensor([[ 1.4587e-02,  3.7598e-02, -7.9346e-03,  ..., -6.2256e-03,
          7.5340e-05,  1.5335....., -3.3936e-02,
         -1.2451e-02,  3.1281e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
in_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
bias_k = None, bias_v = None, add_zero_attn = False, dropout_p = 0.0
out_proj_weight = Parameter containing:
tensor([[-0.0037,  0.0415, -0.0242,  ...,  0.0052, -0.0132, -0.0171],
        [ 0.0056,  0.0354,... -0.0457,  0.0081,  ..., -0.0051, -0.0339,  0.0095]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
out_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ... 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
training = False, key_padding_mask = None, need_weights = False
attn_mask = None, use_separate_proj_weight = False, q_proj_weight = None
k_proj_weight = None, v_proj_weight = None, static_k = None, static_v = None
average_attn_weights = True, is_causal = False

    def multi_head_attention_forward(
        query: Tensor,
        key: Tensor,
        value: Tensor,
        embed_dim_to_check: int,
        num_heads: int,
        in_proj_weight: Optional[Tensor],
        in_proj_bias: Optional[Tensor],
        bias_k: Optional[Tensor],
        bias_v: Optional[Tensor],
        add_zero_attn: bool,
        dropout_p: float,
        out_proj_weight: Tensor,
        out_proj_bias: Optional[Tensor],
        training: bool = True,
        key_padding_mask: Optional[Tensor] = None,
        need_weights: bool = True,
        attn_mask: Optional[Tensor] = None,
        use_separate_proj_weight: bool = False,
        q_proj_weight: Optional[Tensor] = None,
        k_proj_weight: Optional[Tensor] = None,
        v_proj_weight: Optional[Tensor] = None,
        static_k: Optional[Tensor] = None,
        static_v: Optional[Tensor] = None,
        average_attn_weights: bool = True,
        is_causal: bool = False,
    ) -> tuple[Tensor, Optional[Tensor]]:
        r"""Forward method for MultiHeadAttention.
    
        See :class:`torch.nn.MultiheadAttention` for details.
    
        Args:
            query, key, value: map a query and a set of key-value pairs to an output.
                See "Attention Is All You Need" for more details.
            embed_dim_to_check: total dimension of the model.
            num_heads: parallel attention heads.
            in_proj_weight, in_proj_bias: input projection weight and bias.
            bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
            add_zero_attn: add a new batch of zeros to the key and
                           value sequences at dim=1.
            dropout_p: probability of an element to be zeroed.
            out_proj_weight, out_proj_bias: the output projection weight and bias.
            training: apply dropout if is ``True``.
            key_padding_mask: if provided, specified padding elements in the key will
                be ignored by the attention. This is an binary mask. When the value is True,
                the corresponding value on the attention layer will be filled with -inf.
            need_weights: output attn_output_weights.
                Default: `True`
                Note: `needs_weight` defaults to `True`, but should be set to `False`
                For best performance when attention weights are not needed.
                *Setting needs_weights to `True`
                leads to a significant performance degradation.*
            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
                the batches while a 3D mask allows to specify a different mask for the entries of each batch.
            is_causal: If specified, applies a causal mask as attention mask, and ignores
                attn_mask for computing scaled dot product attention.
                Default: ``False``.
                .. warning::
                    is_causal is provides a hint that the attn_mask is the
                    causal mask.Providing incorrect hints can result in
                    incorrect execution, including forward and backward
                    compatibility.
            use_separate_proj_weight: the function accept the proj. weights for query, key,
                and value in different forms. If false, in_proj_weight will be used, which is
                a combination of q_proj_weight, k_proj_weight, v_proj_weight.
            q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
            static_k, static_v: static key and value used for attention operators.
            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.
                Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect
                when ``need_weights=True.``. Default: True
    
    
        Shape:
            Inputs:
            - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
              the embedding dimension.
            - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.
              If a FloatTensor is provided, it will be directly added to the value.
              If a BoolTensor is provided, the positions with the
              value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
            - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
              3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
              S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked
              positions. If a BoolTensor is provided, positions with ``True``
              are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
              is provided, it will be added to the attention weight.
            - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
            - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
    
            Outputs:
            - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
              E is the embedding dimension.
            - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns
              attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or
              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and
              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per
              head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.
        """
        tens_ops = (
            query,
            key,
            value,
            in_proj_weight,
            in_proj_bias,
            bias_k,
            bias_v,
            out_proj_weight,
            out_proj_bias,
        )
        if has_torch_function(tens_ops):
            return handle_torch_function(
                multi_head_attention_forward,
                tens_ops,
                query,
                key,
                value,
                embed_dim_to_check,
                num_heads,
                in_proj_weight,
                in_proj_bias,
                bias_k,
                bias_v,
                add_zero_attn,
                dropout_p,
                out_proj_weight,
                out_proj_bias,
                training=training,
                key_padding_mask=key_padding_mask,
                need_weights=need_weights,
                attn_mask=attn_mask,
                is_causal=is_causal,
                use_separate_proj_weight=use_separate_proj_weight,
                q_proj_weight=q_proj_weight,
                k_proj_weight=k_proj_weight,
                v_proj_weight=v_proj_weight,
                static_k=static_k,
                static_v=static_v,
                average_attn_weights=average_attn_weights,
            )
    
        is_batched = _mha_shape_check(
            query, key, value, key_padding_mask, attn_mask, num_heads
        )
    
        # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input
        # is batched, run the computation and before returning squeeze the
        # batch dimension so that the output doesn't carry this temporary batch dimension.
        if not is_batched:
            # unsqueeze if the input is unbatched
            query = query.unsqueeze(1)
            key = key.unsqueeze(1)
            value = value.unsqueeze(1)
            if key_padding_mask is not None:
                key_padding_mask = key_padding_mask.unsqueeze(0)
    
        # set up shape vars
        tgt_len, bsz, embed_dim = query.shape
        src_len, _, _ = key.shape
    
        key_padding_mask = _canonical_mask(
            mask=key_padding_mask,
            mask_name="key_padding_mask",
            other_type=_none_or_dtype(attn_mask),
            other_name="attn_mask",
            target_type=query.dtype,
        )
    
        if is_causal and attn_mask is None:
            raise RuntimeError(
                "Need attn_mask if specifying the is_causal hint. "
                "You may use the Transformer module method "
                "`generate_square_subsequent_mask` to create this mask."
            )
    
        if is_causal and key_padding_mask is None and not need_weights:
            # when we have a kpm or need weights, we need attn_mask
            # Otherwise, we use the is_causal hint go as is_causal
            # indicator to SDPA.
            attn_mask = None
        else:
            attn_mask = _canonical_mask(
                mask=attn_mask,
                mask_name="attn_mask",
                other_type=None,
                other_name="",
                target_type=query.dtype,
                check_other=False,
            )
    
            if key_padding_mask is not None:
                # We have the attn_mask, and use that to merge kpm into it.
                # Turn off use of is_causal hint, as the merged mask is no
                # longer causal.
                is_causal = False
    
        assert embed_dim == embed_dim_to_check, (
            f"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}"
        )
        if isinstance(embed_dim, torch.Tensor):
            # embed_dim can be a tensor when JIT tracing
            head_dim = embed_dim.div(num_heads, rounding_mode="trunc")
        else:
            head_dim = embed_dim // num_heads
        assert head_dim * num_heads == embed_dim, (
            f"embed_dim {embed_dim} not divisible by num_heads {num_heads}"
        )
        if use_separate_proj_weight:
            # allow MHA to have different embedding dimensions when separate projection weights are used
            assert key.shape[:2] == value.shape[:2], (
                f"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}"
            )
        else:
            assert key.shape == value.shape, (
                f"key shape {key.shape} does not match value shape {value.shape}"
            )
    
        #
        # compute in-projection
        #
        if not use_separate_proj_weight:
            assert in_proj_weight is not None, (
                "use_separate_proj_weight is False but in_proj_weight is None"
            )
            q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
        else:
            assert q_proj_weight is not None, (
                "use_separate_proj_weight is True but q_proj_weight is None"
            )
            assert k_proj_weight is not None, (
                "use_separate_proj_weight is True but k_proj_weight is None"
            )
            assert v_proj_weight is not None, (
                "use_separate_proj_weight is True but v_proj_weight is None"
            )
            if in_proj_bias is None:
                b_q = b_k = b_v = None
            else:
                b_q, b_k, b_v = in_proj_bias.chunk(3)
            q, k, v = _in_projection(
                query,
                key,
                value,
                q_proj_weight,
                k_proj_weight,
                v_proj_weight,
                b_q,
                b_k,
                b_v,
            )
    
        # prep attention mask
    
        if attn_mask is not None:
            # ensure attn_mask's dim is 3
            if attn_mask.dim() == 2:
                correct_2d_size = (tgt_len, src_len)
                if attn_mask.shape != correct_2d_size:
                    raise RuntimeError(
                        f"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}."
                    )
                attn_mask = attn_mask.unsqueeze(0)
            elif attn_mask.dim() == 3:
                correct_3d_size = (bsz * num_heads, tgt_len, src_len)
                if attn_mask.shape != correct_3d_size:
                    raise RuntimeError(
                        f"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}."
                    )
            else:
                raise RuntimeError(
                    f"attn_mask's dimension {attn_mask.dim()} is not supported"
                )
    
        # add bias along batch dimension (currently second)
        if bias_k is not None and bias_v is not None:
            assert static_k is None, "bias cannot be added to static key."
            assert static_v is None, "bias cannot be added to static value."
            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
        else:
            assert bias_k is None
            assert bias_v is None
    
        #
        # reshape q, k, v for multihead attention and make them batch first
        #
        q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
        if static_k is None:
            k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_k.size(0) == bsz * num_heads, (
                f"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}"
            )
            assert static_k.size(2) == head_dim, (
                f"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}"
            )
            k = static_k
        if static_v is None:
            v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_v.size(0) == bsz * num_heads, (
                f"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}"
            )
            assert static_v.size(2) == head_dim, (
                f"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}"
            )
            v = static_v
    
        # add zero attention along batch dimension (now first)
        if add_zero_attn:
            zero_attn_shape = (bsz * num_heads, 1, head_dim)
            k = torch.cat(
                [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1
            )
            v = torch.cat(
                [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1
            )
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
    
        # update source sequence length after adjustments
        src_len = k.size(1)
    
        # merge key padding and attention masks
        if key_padding_mask is not None:
            if not torch.jit.is_scripting() and not torch.jit.is_tracing():
                _check_key_padding_mask(key_padding_mask, src_len, bsz)
    
            key_padding_mask = (
                key_padding_mask.view(bsz, 1, 1, src_len)
                .expand(-1, num_heads, -1, -1)
                .reshape(bsz * num_heads, 1, src_len)
            )
            if attn_mask is None:
                attn_mask = key_padding_mask
            else:
                attn_mask = attn_mask + key_padding_mask
    
        # adjust dropout probability
        if not training:
            dropout_p = 0.0
    
        #
        # (deep breath) calculate attention and out projection
        #
    
        if need_weights:
            _B, _Nt, E = q.shape
            q_scaled = q * math.sqrt(1.0 / float(E))
    
            assert not (is_causal and attn_mask is None), (
                "FIXME: is_causal not implemented for need_weights"
            )
    
            if attn_mask is not None:
                attn_output_weights = torch.baddbmm(
                    attn_mask, q_scaled, k.transpose(-2, -1)
                )
            else:
                attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
            attn_output_weights = softmax(attn_output_weights, dim=-1)
            if dropout_p > 0.0:
                attn_output_weights = dropout(attn_output_weights, p=dropout_p)
    
            attn_output = torch.bmm(attn_output_weights, v)
    
            attn_output = (
                attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
            )
            attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
            attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
    
            # optionally average attention weights over heads
            attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
            if average_attn_weights:
                attn_output_weights = attn_output_weights.mean(dim=1)
    
            if not is_batched:
                # squeeze the output if input was unbatched
                attn_output = attn_output.squeeze(1)
                attn_output_weights = attn_output_weights.squeeze(0)
            return attn_output, attn_output_weights
        else:
            # attn_mask can be either (L,S) or (N*num_heads, L, S)
            # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)
            # in order to match the input for SDPA of (N, num_heads, L, S)
            if attn_mask is not None:
                if attn_mask.size(0) == 1 and attn_mask.dim() == 3:
                    attn_mask = attn_mask.unsqueeze(0)
                else:
                    attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)
    
            q = q.view(bsz, num_heads, tgt_len, head_dim)
            k = k.view(bsz, num_heads, src_len, head_dim)
            v = v.view(bsz, num_heads, src_len, head_dim)
    
>           attn_output = scaled_dot_product_attention(
                q, k, v, attn_mask, dropout_p, is_causal
            )
E           RuntimeError: cuDNN Frontend error: CUDNN_BACKEND_TENSOR_DESCRIPTOR cudnnFinalize failedptrDesc->finalize() cudnn_status: CUDNN_STATUS_SUBLIBRARY_LOADING_FAILED

/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6487: RuntimeError
__________________ test_mha_accuracy[no_mask-small-2-dtype1] ___________________

dtype = torch.float16, bs = 2, model = 'small', mask_type = 'no_mask'

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("mask_type", mask_types)
    def test_mha_accuracy(dtype, bs, model, mask_type):
        config = model_configs[model]
    
        te_mha = MultiheadAttention(
            config.hidden_size,
            config.num_heads,
            fuse_qkv_params=True,
            params_dtype=dtype,
            qkv_weight_interleaved=False,
            input_layernorm=False,
            device="cuda",
        ).eval()
    
        torch_mha = (
            TorchMHA(
                config.hidden_size,
                config.num_heads,
            )
            .to(dtype=dtype)
            .cuda()
            .eval()
        )
    
        # Share params
        with torch.no_grad():
            torch_mha.mhsa.in_proj_weight = Parameter(te_mha.qkv.weight.clone())
            torch_mha.mhsa.in_proj_bias = Parameter(te_mha.qkv.bias.clone())
            torch_mha.mhsa.out_proj.weight = Parameter(te_mha.proj.weight.clone())
            torch_mha.mhsa.out_proj.bias = Parameter(te_mha.proj.bias.clone())
    
        te_outputs = _test_mha_accuracy(te_mha, bs, dtype, config, mask_type, te=True)
>       torch_outputs = _test_mha_accuracy(torch_mha, bs, dtype, config, mask_type, te=False)

tests/pytorch/test_numerics.py:1074: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/test_numerics.py:1027: in _test_mha_accuracy
    out = block(inp_hidden_states, **forward_kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:426: in forward
    output = self.mhsa(x, x, x, attn_mask=attention_mask, need_weights=False)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1488: in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = tensor([[[-1.6162,  0.5684, -0.5103,  ..., -1.3506,  2.7090, -0.1680],
         [-0.3225,  1.4258,  0.1251,  ..., -1.7... -0.4214, -0.0599,  ...,  0.2466,  0.7554, -0.2751]]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
key = tensor([[[-1.6162,  0.5684, -0.5103,  ..., -1.3506,  2.7090, -0.1680],
         [-0.3225,  1.4258,  0.1251,  ..., -1.7... -0.4214, -0.0599,  ...,  0.2466,  0.7554, -0.2751]]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
value = tensor([[[-1.6162,  0.5684, -0.5103,  ..., -1.3506,  2.7090, -0.1680],
         [-0.3225,  1.4258,  0.1251,  ..., -1.7... -0.4214, -0.0599,  ...,  0.2466,  0.7554, -0.2751]]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
embed_dim_to_check = 128, num_heads = 8
in_proj_weight = Parameter containing:
tensor([[ 1.4565e-02,  3.7628e-02, -7.9575e-03,  ..., -6.2141e-03,
          7.5221e-05,  1.5306......, -3.4058e-02,
         -1.2466e-02,  3.1300e-03]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
in_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
bias_k = None, bias_v = None, add_zero_attn = False, dropout_p = 0.0
out_proj_weight = Parameter containing:
tensor([[-0.0037,  0.0416, -0.0242,  ...,  0.0051, -0.0132, -0.0170],
        [ 0.0056,  0.0353,..., -0.0457,  0.0080,  ..., -0.0051, -0.0340,  0.0095]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
out_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ..., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
training = False, key_padding_mask = None, need_weights = False
attn_mask = None, use_separate_proj_weight = False, q_proj_weight = None
k_proj_weight = None, v_proj_weight = None, static_k = None, static_v = None
average_attn_weights = True, is_causal = False

    def multi_head_attention_forward(
        query: Tensor,
        key: Tensor,
        value: Tensor,
        embed_dim_to_check: int,
        num_heads: int,
        in_proj_weight: Optional[Tensor],
        in_proj_bias: Optional[Tensor],
        bias_k: Optional[Tensor],
        bias_v: Optional[Tensor],
        add_zero_attn: bool,
        dropout_p: float,
        out_proj_weight: Tensor,
        out_proj_bias: Optional[Tensor],
        training: bool = True,
        key_padding_mask: Optional[Tensor] = None,
        need_weights: bool = True,
        attn_mask: Optional[Tensor] = None,
        use_separate_proj_weight: bool = False,
        q_proj_weight: Optional[Tensor] = None,
        k_proj_weight: Optional[Tensor] = None,
        v_proj_weight: Optional[Tensor] = None,
        static_k: Optional[Tensor] = None,
        static_v: Optional[Tensor] = None,
        average_attn_weights: bool = True,
        is_causal: bool = False,
    ) -> tuple[Tensor, Optional[Tensor]]:
        r"""Forward method for MultiHeadAttention.
    
        See :class:`torch.nn.MultiheadAttention` for details.
    
        Args:
            query, key, value: map a query and a set of key-value pairs to an output.
                See "Attention Is All You Need" for more details.
            embed_dim_to_check: total dimension of the model.
            num_heads: parallel attention heads.
            in_proj_weight, in_proj_bias: input projection weight and bias.
            bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
            add_zero_attn: add a new batch of zeros to the key and
                           value sequences at dim=1.
            dropout_p: probability of an element to be zeroed.
            out_proj_weight, out_proj_bias: the output projection weight and bias.
            training: apply dropout if is ``True``.
            key_padding_mask: if provided, specified padding elements in the key will
                be ignored by the attention. This is an binary mask. When the value is True,
                the corresponding value on the attention layer will be filled with -inf.
            need_weights: output attn_output_weights.
                Default: `True`
                Note: `needs_weight` defaults to `True`, but should be set to `False`
                For best performance when attention weights are not needed.
                *Setting needs_weights to `True`
                leads to a significant performance degradation.*
            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
                the batches while a 3D mask allows to specify a different mask for the entries of each batch.
            is_causal: If specified, applies a causal mask as attention mask, and ignores
                attn_mask for computing scaled dot product attention.
                Default: ``False``.
                .. warning::
                    is_causal is provides a hint that the attn_mask is the
                    causal mask.Providing incorrect hints can result in
                    incorrect execution, including forward and backward
                    compatibility.
            use_separate_proj_weight: the function accept the proj. weights for query, key,
                and value in different forms. If false, in_proj_weight will be used, which is
                a combination of q_proj_weight, k_proj_weight, v_proj_weight.
            q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
            static_k, static_v: static key and value used for attention operators.
            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.
                Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect
                when ``need_weights=True.``. Default: True
    
    
        Shape:
            Inputs:
            - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
              the embedding dimension.
            - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.
              If a FloatTensor is provided, it will be directly added to the value.
              If a BoolTensor is provided, the positions with the
              value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
            - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
              3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
              S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked
              positions. If a BoolTensor is provided, positions with ``True``
              are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
              is provided, it will be added to the attention weight.
            - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
            - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
    
            Outputs:
            - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
              E is the embedding dimension.
            - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns
              attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or
              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and
              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per
              head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.
        """
        tens_ops = (
            query,
            key,
            value,
            in_proj_weight,
            in_proj_bias,
            bias_k,
            bias_v,
            out_proj_weight,
            out_proj_bias,
        )
        if has_torch_function(tens_ops):
            return handle_torch_function(
                multi_head_attention_forward,
                tens_ops,
                query,
                key,
                value,
                embed_dim_to_check,
                num_heads,
                in_proj_weight,
                in_proj_bias,
                bias_k,
                bias_v,
                add_zero_attn,
                dropout_p,
                out_proj_weight,
                out_proj_bias,
                training=training,
                key_padding_mask=key_padding_mask,
                need_weights=need_weights,
                attn_mask=attn_mask,
                is_causal=is_causal,
                use_separate_proj_weight=use_separate_proj_weight,
                q_proj_weight=q_proj_weight,
                k_proj_weight=k_proj_weight,
                v_proj_weight=v_proj_weight,
                static_k=static_k,
                static_v=static_v,
                average_attn_weights=average_attn_weights,
            )
    
        is_batched = _mha_shape_check(
            query, key, value, key_padding_mask, attn_mask, num_heads
        )
    
        # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input
        # is batched, run the computation and before returning squeeze the
        # batch dimension so that the output doesn't carry this temporary batch dimension.
        if not is_batched:
            # unsqueeze if the input is unbatched
            query = query.unsqueeze(1)
            key = key.unsqueeze(1)
            value = value.unsqueeze(1)
            if key_padding_mask is not None:
                key_padding_mask = key_padding_mask.unsqueeze(0)
    
        # set up shape vars
        tgt_len, bsz, embed_dim = query.shape
        src_len, _, _ = key.shape
    
        key_padding_mask = _canonical_mask(
            mask=key_padding_mask,
            mask_name="key_padding_mask",
            other_type=_none_or_dtype(attn_mask),
            other_name="attn_mask",
            target_type=query.dtype,
        )
    
        if is_causal and attn_mask is None:
            raise RuntimeError(
                "Need attn_mask if specifying the is_causal hint. "
                "You may use the Transformer module method "
                "`generate_square_subsequent_mask` to create this mask."
            )
    
        if is_causal and key_padding_mask is None and not need_weights:
            # when we have a kpm or need weights, we need attn_mask
            # Otherwise, we use the is_causal hint go as is_causal
            # indicator to SDPA.
            attn_mask = None
        else:
            attn_mask = _canonical_mask(
                mask=attn_mask,
                mask_name="attn_mask",
                other_type=None,
                other_name="",
                target_type=query.dtype,
                check_other=False,
            )
    
            if key_padding_mask is not None:
                # We have the attn_mask, and use that to merge kpm into it.
                # Turn off use of is_causal hint, as the merged mask is no
                # longer causal.
                is_causal = False
    
        assert embed_dim == embed_dim_to_check, (
            f"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}"
        )
        if isinstance(embed_dim, torch.Tensor):
            # embed_dim can be a tensor when JIT tracing
            head_dim = embed_dim.div(num_heads, rounding_mode="trunc")
        else:
            head_dim = embed_dim // num_heads
        assert head_dim * num_heads == embed_dim, (
            f"embed_dim {embed_dim} not divisible by num_heads {num_heads}"
        )
        if use_separate_proj_weight:
            # allow MHA to have different embedding dimensions when separate projection weights are used
            assert key.shape[:2] == value.shape[:2], (
                f"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}"
            )
        else:
            assert key.shape == value.shape, (
                f"key shape {key.shape} does not match value shape {value.shape}"
            )
    
        #
        # compute in-projection
        #
        if not use_separate_proj_weight:
            assert in_proj_weight is not None, (
                "use_separate_proj_weight is False but in_proj_weight is None"
            )
            q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
        else:
            assert q_proj_weight is not None, (
                "use_separate_proj_weight is True but q_proj_weight is None"
            )
            assert k_proj_weight is not None, (
                "use_separate_proj_weight is True but k_proj_weight is None"
            )
            assert v_proj_weight is not None, (
                "use_separate_proj_weight is True but v_proj_weight is None"
            )
            if in_proj_bias is None:
                b_q = b_k = b_v = None
            else:
                b_q, b_k, b_v = in_proj_bias.chunk(3)
            q, k, v = _in_projection(
                query,
                key,
                value,
                q_proj_weight,
                k_proj_weight,
                v_proj_weight,
                b_q,
                b_k,
                b_v,
            )
    
        # prep attention mask
    
        if attn_mask is not None:
            # ensure attn_mask's dim is 3
            if attn_mask.dim() == 2:
                correct_2d_size = (tgt_len, src_len)
                if attn_mask.shape != correct_2d_size:
                    raise RuntimeError(
                        f"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}."
                    )
                attn_mask = attn_mask.unsqueeze(0)
            elif attn_mask.dim() == 3:
                correct_3d_size = (bsz * num_heads, tgt_len, src_len)
                if attn_mask.shape != correct_3d_size:
                    raise RuntimeError(
                        f"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}."
                    )
            else:
                raise RuntimeError(
                    f"attn_mask's dimension {attn_mask.dim()} is not supported"
                )
    
        # add bias along batch dimension (currently second)
        if bias_k is not None and bias_v is not None:
            assert static_k is None, "bias cannot be added to static key."
            assert static_v is None, "bias cannot be added to static value."
            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
        else:
            assert bias_k is None
            assert bias_v is None
    
        #
        # reshape q, k, v for multihead attention and make them batch first
        #
        q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
        if static_k is None:
            k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_k.size(0) == bsz * num_heads, (
                f"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}"
            )
            assert static_k.size(2) == head_dim, (
                f"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}"
            )
            k = static_k
        if static_v is None:
            v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_v.size(0) == bsz * num_heads, (
                f"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}"
            )
            assert static_v.size(2) == head_dim, (
                f"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}"
            )
            v = static_v
    
        # add zero attention along batch dimension (now first)
        if add_zero_attn:
            zero_attn_shape = (bsz * num_heads, 1, head_dim)
            k = torch.cat(
                [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1
            )
            v = torch.cat(
                [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1
            )
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
    
        # update source sequence length after adjustments
        src_len = k.size(1)
    
        # merge key padding and attention masks
        if key_padding_mask is not None:
            if not torch.jit.is_scripting() and not torch.jit.is_tracing():
                _check_key_padding_mask(key_padding_mask, src_len, bsz)
    
            key_padding_mask = (
                key_padding_mask.view(bsz, 1, 1, src_len)
                .expand(-1, num_heads, -1, -1)
                .reshape(bsz * num_heads, 1, src_len)
            )
            if attn_mask is None:
                attn_mask = key_padding_mask
            else:
                attn_mask = attn_mask + key_padding_mask
    
        # adjust dropout probability
        if not training:
            dropout_p = 0.0
    
        #
        # (deep breath) calculate attention and out projection
        #
    
        if need_weights:
            _B, _Nt, E = q.shape
            q_scaled = q * math.sqrt(1.0 / float(E))
    
            assert not (is_causal and attn_mask is None), (
                "FIXME: is_causal not implemented for need_weights"
            )
    
            if attn_mask is not None:
                attn_output_weights = torch.baddbmm(
                    attn_mask, q_scaled, k.transpose(-2, -1)
                )
            else:
                attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
            attn_output_weights = softmax(attn_output_weights, dim=-1)
            if dropout_p > 0.0:
                attn_output_weights = dropout(attn_output_weights, p=dropout_p)
    
            attn_output = torch.bmm(attn_output_weights, v)
    
            attn_output = (
                attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
            )
            attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
            attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
    
            # optionally average attention weights over heads
            attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
            if average_attn_weights:
                attn_output_weights = attn_output_weights.mean(dim=1)
    
            if not is_batched:
                # squeeze the output if input was unbatched
                attn_output = attn_output.squeeze(1)
                attn_output_weights = attn_output_weights.squeeze(0)
            return attn_output, attn_output_weights
        else:
            # attn_mask can be either (L,S) or (N*num_heads, L, S)
            # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)
            # in order to match the input for SDPA of (N, num_heads, L, S)
            if attn_mask is not None:
                if attn_mask.size(0) == 1 and attn_mask.dim() == 3:
                    attn_mask = attn_mask.unsqueeze(0)
                else:
                    attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)
    
            q = q.view(bsz, num_heads, tgt_len, head_dim)
            k = k.view(bsz, num_heads, src_len, head_dim)
            v = v.view(bsz, num_heads, src_len, head_dim)
    
>           attn_output = scaled_dot_product_attention(
                q, k, v, attn_mask, dropout_p, is_causal
            )
E           RuntimeError: cuDNN Frontend error: CUDNN_BACKEND_TENSOR_DESCRIPTOR cudnnFinalize failedptrDesc->finalize() cudnn_status: CUDNN_STATUS_SUBLIBRARY_LOADING_FAILED

/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6487: RuntimeError
__________________ test_mha_accuracy[no_mask-small-2-dtype2] ___________________

dtype = torch.bfloat16, bs = 2, model = 'small', mask_type = 'no_mask'

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("mask_type", mask_types)
    def test_mha_accuracy(dtype, bs, model, mask_type):
        config = model_configs[model]
    
        te_mha = MultiheadAttention(
            config.hidden_size,
            config.num_heads,
            fuse_qkv_params=True,
            params_dtype=dtype,
            qkv_weight_interleaved=False,
            input_layernorm=False,
            device="cuda",
        ).eval()
    
        torch_mha = (
            TorchMHA(
                config.hidden_size,
                config.num_heads,
            )
            .to(dtype=dtype)
            .cuda()
            .eval()
        )
    
        # Share params
        with torch.no_grad():
            torch_mha.mhsa.in_proj_weight = Parameter(te_mha.qkv.weight.clone())
            torch_mha.mhsa.in_proj_bias = Parameter(te_mha.qkv.bias.clone())
            torch_mha.mhsa.out_proj.weight = Parameter(te_mha.proj.weight.clone())
            torch_mha.mhsa.out_proj.bias = Parameter(te_mha.proj.bias.clone())
    
        te_outputs = _test_mha_accuracy(te_mha, bs, dtype, config, mask_type, te=True)
>       torch_outputs = _test_mha_accuracy(torch_mha, bs, dtype, config, mask_type, te=False)

tests/pytorch/test_numerics.py:1074: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/test_numerics.py:1027: in _test_mha_accuracy
    out = block(inp_hidden_states, **forward_kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
tests/pytorch/test_numerics.py:426: in forward
    output = self.mhsa(x, x, x, attn_mask=attention_mask, need_weights=False)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1488: in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = tensor([[[-1.6172,  0.5703, -0.5117,  ..., -1.3516,  2.7031, -0.1680],
         [-0.3223,  1.4297,  0.1250,  ..., -1.7...-0.4219, -0.0598,  ...,  0.2461,  0.7539, -0.2754]]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
key = tensor([[[-1.6172,  0.5703, -0.5117,  ..., -1.3516,  2.7031, -0.1680],
         [-0.3223,  1.4297,  0.1250,  ..., -1.7...-0.4219, -0.0598,  ...,  0.2461,  0.7539, -0.2754]]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
value = tensor([[[-1.6172,  0.5703, -0.5117,  ..., -1.3516,  2.7031, -0.1680],
         [-0.3223,  1.4297,  0.1250,  ..., -1.7...-0.4219, -0.0598,  ...,  0.2461,  0.7539, -0.2754]]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
embed_dim_to_check = 128, num_heads = 8
in_proj_weight = Parameter containing:
tensor([[ 1.4587e-02,  3.7598e-02, -7.9346e-03,  ..., -6.2256e-03,
          7.5340e-05,  1.5335....., -3.3936e-02,
         -1.2451e-02,  3.1281e-03]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
in_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
bias_k = None, bias_v = None, add_zero_attn = False, dropout_p = 0.0
out_proj_weight = Parameter containing:
tensor([[-0.0037,  0.0415, -0.0242,  ...,  0.0052, -0.0132, -0.0171],
        [ 0.0056,  0.0354,... -0.0457,  0.0081,  ..., -0.0051, -0.0339,  0.0095]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
out_proj_bias = Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., ... 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
training = False, key_padding_mask = None, need_weights = False
attn_mask = None, use_separate_proj_weight = False, q_proj_weight = None
k_proj_weight = None, v_proj_weight = None, static_k = None, static_v = None
average_attn_weights = True, is_causal = False

    def multi_head_attention_forward(
        query: Tensor,
        key: Tensor,
        value: Tensor,
        embed_dim_to_check: int,
        num_heads: int,
        in_proj_weight: Optional[Tensor],
        in_proj_bias: Optional[Tensor],
        bias_k: Optional[Tensor],
        bias_v: Optional[Tensor],
        add_zero_attn: bool,
        dropout_p: float,
        out_proj_weight: Tensor,
        out_proj_bias: Optional[Tensor],
        training: bool = True,
        key_padding_mask: Optional[Tensor] = None,
        need_weights: bool = True,
        attn_mask: Optional[Tensor] = None,
        use_separate_proj_weight: bool = False,
        q_proj_weight: Optional[Tensor] = None,
        k_proj_weight: Optional[Tensor] = None,
        v_proj_weight: Optional[Tensor] = None,
        static_k: Optional[Tensor] = None,
        static_v: Optional[Tensor] = None,
        average_attn_weights: bool = True,
        is_causal: bool = False,
    ) -> tuple[Tensor, Optional[Tensor]]:
        r"""Forward method for MultiHeadAttention.
    
        See :class:`torch.nn.MultiheadAttention` for details.
    
        Args:
            query, key, value: map a query and a set of key-value pairs to an output.
                See "Attention Is All You Need" for more details.
            embed_dim_to_check: total dimension of the model.
            num_heads: parallel attention heads.
            in_proj_weight, in_proj_bias: input projection weight and bias.
            bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
            add_zero_attn: add a new batch of zeros to the key and
                           value sequences at dim=1.
            dropout_p: probability of an element to be zeroed.
            out_proj_weight, out_proj_bias: the output projection weight and bias.
            training: apply dropout if is ``True``.
            key_padding_mask: if provided, specified padding elements in the key will
                be ignored by the attention. This is an binary mask. When the value is True,
                the corresponding value on the attention layer will be filled with -inf.
            need_weights: output attn_output_weights.
                Default: `True`
                Note: `needs_weight` defaults to `True`, but should be set to `False`
                For best performance when attention weights are not needed.
                *Setting needs_weights to `True`
                leads to a significant performance degradation.*
            attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
                the batches while a 3D mask allows to specify a different mask for the entries of each batch.
            is_causal: If specified, applies a causal mask as attention mask, and ignores
                attn_mask for computing scaled dot product attention.
                Default: ``False``.
                .. warning::
                    is_causal is provides a hint that the attn_mask is the
                    causal mask.Providing incorrect hints can result in
                    incorrect execution, including forward and backward
                    compatibility.
            use_separate_proj_weight: the function accept the proj. weights for query, key,
                and value in different forms. If false, in_proj_weight will be used, which is
                a combination of q_proj_weight, k_proj_weight, v_proj_weight.
            q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
            static_k, static_v: static key and value used for attention operators.
            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across heads.
                Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an effect
                when ``need_weights=True.``. Default: True
    
    
        Shape:
            Inputs:
            - query: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
              the embedding dimension.
            - key: :math:`(S, E)` or :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - value: :math:`(S, E)` or :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
              the embedding dimension.
            - key_padding_mask: :math:`(S)` or :math:`(N, S)` where N is the batch size, S is the source sequence length.
              If a FloatTensor is provided, it will be directly added to the value.
              If a BoolTensor is provided, the positions with the
              value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
            - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
              3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
              S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked
              positions. If a BoolTensor is provided, positions with ``True``
              are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
              is provided, it will be added to the attention weight.
            - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
            - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
              N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
    
            Outputs:
            - attn_output: :math:`(L, E)` or :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
              E is the embedding dimension.
            - attn_output_weights: Only returned when ``need_weights=True``. If ``average_attn_weights=True``, returns
              attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or
              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and
              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per
              head of shape :math:`(num_heads, L, S)` when input is unbatched or :math:`(N, num_heads, L, S)`.
        """
        tens_ops = (
            query,
            key,
            value,
            in_proj_weight,
            in_proj_bias,
            bias_k,
            bias_v,
            out_proj_weight,
            out_proj_bias,
        )
        if has_torch_function(tens_ops):
            return handle_torch_function(
                multi_head_attention_forward,
                tens_ops,
                query,
                key,
                value,
                embed_dim_to_check,
                num_heads,
                in_proj_weight,
                in_proj_bias,
                bias_k,
                bias_v,
                add_zero_attn,
                dropout_p,
                out_proj_weight,
                out_proj_bias,
                training=training,
                key_padding_mask=key_padding_mask,
                need_weights=need_weights,
                attn_mask=attn_mask,
                is_causal=is_causal,
                use_separate_proj_weight=use_separate_proj_weight,
                q_proj_weight=q_proj_weight,
                k_proj_weight=k_proj_weight,
                v_proj_weight=v_proj_weight,
                static_k=static_k,
                static_v=static_v,
                average_attn_weights=average_attn_weights,
            )
    
        is_batched = _mha_shape_check(
            query, key, value, key_padding_mask, attn_mask, num_heads
        )
    
        # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input
        # is batched, run the computation and before returning squeeze the
        # batch dimension so that the output doesn't carry this temporary batch dimension.
        if not is_batched:
            # unsqueeze if the input is unbatched
            query = query.unsqueeze(1)
            key = key.unsqueeze(1)
            value = value.unsqueeze(1)
            if key_padding_mask is not None:
                key_padding_mask = key_padding_mask.unsqueeze(0)
    
        # set up shape vars
        tgt_len, bsz, embed_dim = query.shape
        src_len, _, _ = key.shape
    
        key_padding_mask = _canonical_mask(
            mask=key_padding_mask,
            mask_name="key_padding_mask",
            other_type=_none_or_dtype(attn_mask),
            other_name="attn_mask",
            target_type=query.dtype,
        )
    
        if is_causal and attn_mask is None:
            raise RuntimeError(
                "Need attn_mask if specifying the is_causal hint. "
                "You may use the Transformer module method "
                "`generate_square_subsequent_mask` to create this mask."
            )
    
        if is_causal and key_padding_mask is None and not need_weights:
            # when we have a kpm or need weights, we need attn_mask
            # Otherwise, we use the is_causal hint go as is_causal
            # indicator to SDPA.
            attn_mask = None
        else:
            attn_mask = _canonical_mask(
                mask=attn_mask,
                mask_name="attn_mask",
                other_type=None,
                other_name="",
                target_type=query.dtype,
                check_other=False,
            )
    
            if key_padding_mask is not None:
                # We have the attn_mask, and use that to merge kpm into it.
                # Turn off use of is_causal hint, as the merged mask is no
                # longer causal.
                is_causal = False
    
        assert embed_dim == embed_dim_to_check, (
            f"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}"
        )
        if isinstance(embed_dim, torch.Tensor):
            # embed_dim can be a tensor when JIT tracing
            head_dim = embed_dim.div(num_heads, rounding_mode="trunc")
        else:
            head_dim = embed_dim // num_heads
        assert head_dim * num_heads == embed_dim, (
            f"embed_dim {embed_dim} not divisible by num_heads {num_heads}"
        )
        if use_separate_proj_weight:
            # allow MHA to have different embedding dimensions when separate projection weights are used
            assert key.shape[:2] == value.shape[:2], (
                f"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}"
            )
        else:
            assert key.shape == value.shape, (
                f"key shape {key.shape} does not match value shape {value.shape}"
            )
    
        #
        # compute in-projection
        #
        if not use_separate_proj_weight:
            assert in_proj_weight is not None, (
                "use_separate_proj_weight is False but in_proj_weight is None"
            )
            q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
        else:
            assert q_proj_weight is not None, (
                "use_separate_proj_weight is True but q_proj_weight is None"
            )
            assert k_proj_weight is not None, (
                "use_separate_proj_weight is True but k_proj_weight is None"
            )
            assert v_proj_weight is not None, (
                "use_separate_proj_weight is True but v_proj_weight is None"
            )
            if in_proj_bias is None:
                b_q = b_k = b_v = None
            else:
                b_q, b_k, b_v = in_proj_bias.chunk(3)
            q, k, v = _in_projection(
                query,
                key,
                value,
                q_proj_weight,
                k_proj_weight,
                v_proj_weight,
                b_q,
                b_k,
                b_v,
            )
    
        # prep attention mask
    
        if attn_mask is not None:
            # ensure attn_mask's dim is 3
            if attn_mask.dim() == 2:
                correct_2d_size = (tgt_len, src_len)
                if attn_mask.shape != correct_2d_size:
                    raise RuntimeError(
                        f"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}."
                    )
                attn_mask = attn_mask.unsqueeze(0)
            elif attn_mask.dim() == 3:
                correct_3d_size = (bsz * num_heads, tgt_len, src_len)
                if attn_mask.shape != correct_3d_size:
                    raise RuntimeError(
                        f"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}."
                    )
            else:
                raise RuntimeError(
                    f"attn_mask's dimension {attn_mask.dim()} is not supported"
                )
    
        # add bias along batch dimension (currently second)
        if bias_k is not None and bias_v is not None:
            assert static_k is None, "bias cannot be added to static key."
            assert static_v is None, "bias cannot be added to static value."
            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
        else:
            assert bias_k is None
            assert bias_v is None
    
        #
        # reshape q, k, v for multihead attention and make them batch first
        #
        q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
        if static_k is None:
            k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_k.size(0) == bsz * num_heads, (
                f"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}"
            )
            assert static_k.size(2) == head_dim, (
                f"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}"
            )
            k = static_k
        if static_v is None:
            v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
        else:
            # TODO finish disentangling control flow so we don't do in-projections when statics are passed
            assert static_v.size(0) == bsz * num_heads, (
                f"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}"
            )
            assert static_v.size(2) == head_dim, (
                f"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}"
            )
            v = static_v
    
        # add zero attention along batch dimension (now first)
        if add_zero_attn:
            zero_attn_shape = (bsz * num_heads, 1, head_dim)
            k = torch.cat(
                [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1
            )
            v = torch.cat(
                [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1
            )
            if attn_mask is not None:
                attn_mask = pad(attn_mask, (0, 1))
            if key_padding_mask is not None:
                key_padding_mask = pad(key_padding_mask, (0, 1))
    
        # update source sequence length after adjustments
        src_len = k.size(1)
    
        # merge key padding and attention masks
        if key_padding_mask is not None:
            if not torch.jit.is_scripting() and not torch.jit.is_tracing():
                _check_key_padding_mask(key_padding_mask, src_len, bsz)
    
            key_padding_mask = (
                key_padding_mask.view(bsz, 1, 1, src_len)
                .expand(-1, num_heads, -1, -1)
                .reshape(bsz * num_heads, 1, src_len)
            )
            if attn_mask is None:
                attn_mask = key_padding_mask
            else:
                attn_mask = attn_mask + key_padding_mask
    
        # adjust dropout probability
        if not training:
            dropout_p = 0.0
    
        #
        # (deep breath) calculate attention and out projection
        #
    
        if need_weights:
            _B, _Nt, E = q.shape
            q_scaled = q * math.sqrt(1.0 / float(E))
    
            assert not (is_causal and attn_mask is None), (
                "FIXME: is_causal not implemented for need_weights"
            )
    
            if attn_mask is not None:
                attn_output_weights = torch.baddbmm(
                    attn_mask, q_scaled, k.transpose(-2, -1)
                )
            else:
                attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
            attn_output_weights = softmax(attn_output_weights, dim=-1)
            if dropout_p > 0.0:
                attn_output_weights = dropout(attn_output_weights, p=dropout_p)
    
            attn_output = torch.bmm(attn_output_weights, v)
    
            attn_output = (
                attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
            )
            attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
            attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
    
            # optionally average attention weights over heads
            attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
            if average_attn_weights:
                attn_output_weights = attn_output_weights.mean(dim=1)
    
            if not is_batched:
                # squeeze the output if input was unbatched
                attn_output = attn_output.squeeze(1)
                attn_output_weights = attn_output_weights.squeeze(0)
            return attn_output, attn_output_weights
        else:
            # attn_mask can be either (L,S) or (N*num_heads, L, S)
            # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)
            # in order to match the input for SDPA of (N, num_heads, L, S)
            if attn_mask is not None:
                if attn_mask.size(0) == 1 and attn_mask.dim() == 3:
                    attn_mask = attn_mask.unsqueeze(0)
                else:
                    attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)
    
            q = q.view(bsz, num_heads, tgt_len, head_dim)
            k = k.view(bsz, num_heads, src_len, head_dim)
            v = v.view(bsz, num_heads, src_len, head_dim)
    
>           attn_output = scaled_dot_product_attention(
                q, k, v, attn_mask, dropout_p, is_causal
            )
E           RuntimeError: cuDNN Frontend error: CUDNN_BACKEND_TENSOR_DESCRIPTOR cudnnFinalize failedptrDesc->finalize() cudnn_status: CUDNN_STATUS_SUBLIBRARY_LOADING_FAILED

/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6487: RuntimeError
________________ test_linear_accuracy[True-True-small-1-dtype0] ________________

dtype = torch.float32, bs = 1, model = 'small', return_bias = True, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_linear_accuracy(dtype, bs, model, return_bias, bias):
        config = model_configs[model]
    
        te_linear = TestReturnBiasModule(
            Linear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_linear = torch.nn.Linear(
            config.hidden_size,
            4 * config.hidden_size,
            bias=bias,
            device="cuda",
            dtype=dtype,
        )
    
        # Share params
        with torch.no_grad():
            torch_linear.weight = Parameter(te_linear.te_module.weight.clone())
            if bias:
                torch_linear.bias = Parameter(te_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_linear, bs, dtype, config)
    
        # Check output.
        if model == "small":
            tolerance = 5e-3 if dtype == torch.float32 else 5e-2
            rtol = {
                torch.float32: 1.3e-6,
                torch.half: 1e-2,
                torch.bfloat16: 2e-2,
            }
            for te_output, torch_output in zip(te_outputs, torch_outputs):
>               assert_allclose(te_output, torch_output, tolerance, rtol[dtype])

tests/pytorch/test_numerics.py:1261: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[ 17.5706,   3.3682,   8.8672,  ..., -15.9239,  -1.8259,  -1.5402],
        [ 17.5706,   3.3682,   8.8672,  .....1.8259,  -1.5402],
        [ 17.5706,   3.3682,   8.8672,  ..., -15.9239,  -1.8259,  -1.5402]],
       device='cuda:0')
l2 = tensor([[ 17.5718,   3.3661,   8.8677,  ..., -15.9255,  -1.8273,  -1.5447],
        [ 17.5718,   3.3661,   8.8677,  .....1.8273,  -1.5447],
        [ 17.5718,   3.3661,   8.8677,  ..., -15.9255,  -1.8273,  -1.5447]],
       device='cuda:0')
atol = 0.005, rtol = 1.3e-06

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=0. Maximum difference at location [50] with -3.5490684509277344 vs -3.55558705329895 (diff 0.00651860237121582).

tests/pytorch/test_numerics.py:218: AssertionError
________________ test_linear_accuracy[True-True-small-2-dtype0] ________________

dtype = torch.float32, bs = 2, model = 'small', return_bias = True, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_linear_accuracy(dtype, bs, model, return_bias, bias):
        config = model_configs[model]
    
        te_linear = TestReturnBiasModule(
            Linear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_linear = torch.nn.Linear(
            config.hidden_size,
            4 * config.hidden_size,
            bias=bias,
            device="cuda",
            dtype=dtype,
        )
    
        # Share params
        with torch.no_grad():
            torch_linear.weight = Parameter(te_linear.te_module.weight.clone())
            if bias:
                torch_linear.bias = Parameter(te_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_linear, bs, dtype, config)
    
        # Check output.
        if model == "small":
            tolerance = 5e-3 if dtype == torch.float32 else 5e-2
            rtol = {
                torch.float32: 1.3e-6,
                torch.half: 1e-2,
                torch.bfloat16: 2e-2,
            }
            for te_output, torch_output in zip(te_outputs, torch_outputs):
>               assert_allclose(te_output, torch_output, tolerance, rtol[dtype])

tests/pytorch/test_numerics.py:1261: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[ 33.6992,  10.1368,  21.0150,  ..., -18.0752,   3.0496,  -2.8523],
        [ 33.6992,  10.1368,  21.0150,  .....3.0496,  -2.8523],
        [ 33.6992,  10.1368,  21.0150,  ..., -18.0752,   3.0496,  -2.8523]],
       device='cuda:0')
l2 = tensor([[ 33.7001,  10.1309,  21.0160,  ..., -18.0761,   3.0484,  -2.8546],
        [ 33.7001,  10.1309,  21.0160,  .....3.0484,  -2.8546],
        [ 33.7001,  10.1309,  21.0160,  ..., -18.0761,   3.0484,  -2.8546]],
       device='cuda:0')
atol = 0.005, rtol = 1.3e-06

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=0. Maximum difference at location [61] with 2.6603221893310547 vs 2.6514928340911865 (diff 0.008829355239868164).

tests/pytorch/test_numerics.py:218: AssertionError
_______________ test_linear_accuracy[True-False-small-1-dtype0] ________________

dtype = torch.float32, bs = 1, model = 'small', return_bias = False, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_linear_accuracy(dtype, bs, model, return_bias, bias):
        config = model_configs[model]
    
        te_linear = TestReturnBiasModule(
            Linear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_linear = torch.nn.Linear(
            config.hidden_size,
            4 * config.hidden_size,
            bias=bias,
            device="cuda",
            dtype=dtype,
        )
    
        # Share params
        with torch.no_grad():
            torch_linear.weight = Parameter(te_linear.te_module.weight.clone())
            if bias:
                torch_linear.bias = Parameter(te_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_linear, bs, dtype, config)
    
        # Check output.
        if model == "small":
            tolerance = 5e-3 if dtype == torch.float32 else 5e-2
            rtol = {
                torch.float32: 1.3e-6,
                torch.half: 1e-2,
                torch.bfloat16: 2e-2,
            }
            for te_output, torch_output in zip(te_outputs, torch_outputs):
>               assert_allclose(te_output, torch_output, tolerance, rtol[dtype])

tests/pytorch/test_numerics.py:1261: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[ 17.5706,   3.3682,   8.8672,  ..., -15.9239,  -1.8259,  -1.5402],
        [ 17.5706,   3.3682,   8.8672,  .....1.8259,  -1.5402],
        [ 17.5706,   3.3682,   8.8672,  ..., -15.9239,  -1.8259,  -1.5402]],
       device='cuda:0')
l2 = tensor([[ 17.5718,   3.3661,   8.8677,  ..., -15.9255,  -1.8273,  -1.5447],
        [ 17.5718,   3.3661,   8.8677,  .....1.8273,  -1.5447],
        [ 17.5718,   3.3661,   8.8677,  ..., -15.9255,  -1.8273,  -1.5447]],
       device='cuda:0')
atol = 0.005, rtol = 1.3e-06

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=0. Maximum difference at location [50] with -3.5490684509277344 vs -3.55558705329895 (diff 0.00651860237121582).

tests/pytorch/test_numerics.py:218: AssertionError
_______________ test_linear_accuracy[True-False-small-2-dtype0] ________________

dtype = torch.float32, bs = 2, model = 'small', return_bias = False, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_linear_accuracy(dtype, bs, model, return_bias, bias):
        config = model_configs[model]
    
        te_linear = TestReturnBiasModule(
            Linear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_linear = torch.nn.Linear(
            config.hidden_size,
            4 * config.hidden_size,
            bias=bias,
            device="cuda",
            dtype=dtype,
        )
    
        # Share params
        with torch.no_grad():
            torch_linear.weight = Parameter(te_linear.te_module.weight.clone())
            if bias:
                torch_linear.bias = Parameter(te_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_linear, bs, dtype, config)
    
        # Check output.
        if model == "small":
            tolerance = 5e-3 if dtype == torch.float32 else 5e-2
            rtol = {
                torch.float32: 1.3e-6,
                torch.half: 1e-2,
                torch.bfloat16: 2e-2,
            }
            for te_output, torch_output in zip(te_outputs, torch_outputs):
>               assert_allclose(te_output, torch_output, tolerance, rtol[dtype])

tests/pytorch/test_numerics.py:1261: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[ 33.6992,  10.1368,  21.0150,  ..., -18.0752,   3.0496,  -2.8523],
        [ 33.6992,  10.1368,  21.0150,  .....3.0496,  -2.8523],
        [ 33.6992,  10.1368,  21.0150,  ..., -18.0752,   3.0496,  -2.8523]],
       device='cuda:0')
l2 = tensor([[ 33.7001,  10.1309,  21.0160,  ..., -18.0761,   3.0484,  -2.8546],
        [ 33.7001,  10.1309,  21.0160,  .....3.0484,  -2.8546],
        [ 33.7001,  10.1309,  21.0160,  ..., -18.0761,   3.0484,  -2.8546]],
       device='cuda:0')
atol = 0.005, rtol = 1.3e-06

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=0. Maximum difference at location [61] with 2.6603221893310547 vs 2.6514928340911865 (diff 0.008829355239868164).

tests/pytorch/test_numerics.py:218: AssertionError
_______________ test_linear_accuracy[False-True-small-1-dtype0] ________________

dtype = torch.float32, bs = 1, model = 'small', return_bias = True, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_linear_accuracy(dtype, bs, model, return_bias, bias):
        config = model_configs[model]
    
        te_linear = TestReturnBiasModule(
            Linear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_linear = torch.nn.Linear(
            config.hidden_size,
            4 * config.hidden_size,
            bias=bias,
            device="cuda",
            dtype=dtype,
        )
    
        # Share params
        with torch.no_grad():
            torch_linear.weight = Parameter(te_linear.te_module.weight.clone())
            if bias:
                torch_linear.bias = Parameter(te_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_linear, bs, dtype, config)
    
        # Check output.
        if model == "small":
            tolerance = 5e-3 if dtype == torch.float32 else 5e-2
            rtol = {
                torch.float32: 1.3e-6,
                torch.half: 1e-2,
                torch.bfloat16: 2e-2,
            }
            for te_output, torch_output in zip(te_outputs, torch_outputs):
>               assert_allclose(te_output, torch_output, tolerance, rtol[dtype])

tests/pytorch/test_numerics.py:1261: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[ 17.5706,   3.3682,   8.8672,  ..., -15.9239,  -1.8259,  -1.5402],
        [ 17.5706,   3.3682,   8.8672,  .....1.8259,  -1.5402],
        [ 17.5706,   3.3682,   8.8672,  ..., -15.9239,  -1.8259,  -1.5402]],
       device='cuda:0')
l2 = tensor([[ 17.5718,   3.3661,   8.8677,  ..., -15.9255,  -1.8273,  -1.5447],
        [ 17.5718,   3.3661,   8.8677,  .....1.8273,  -1.5447],
        [ 17.5718,   3.3661,   8.8677,  ..., -15.9255,  -1.8273,  -1.5447]],
       device='cuda:0')
atol = 0.005, rtol = 1.3e-06

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=0. Maximum difference at location [50] with -3.5490684509277344 vs -3.55558705329895 (diff 0.00651860237121582).

tests/pytorch/test_numerics.py:218: AssertionError
_______________ test_linear_accuracy[False-True-small-2-dtype0] ________________

dtype = torch.float32, bs = 2, model = 'small', return_bias = True, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_linear_accuracy(dtype, bs, model, return_bias, bias):
        config = model_configs[model]
    
        te_linear = TestReturnBiasModule(
            Linear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_linear = torch.nn.Linear(
            config.hidden_size,
            4 * config.hidden_size,
            bias=bias,
            device="cuda",
            dtype=dtype,
        )
    
        # Share params
        with torch.no_grad():
            torch_linear.weight = Parameter(te_linear.te_module.weight.clone())
            if bias:
                torch_linear.bias = Parameter(te_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_linear, bs, dtype, config)
    
        # Check output.
        if model == "small":
            tolerance = 5e-3 if dtype == torch.float32 else 5e-2
            rtol = {
                torch.float32: 1.3e-6,
                torch.half: 1e-2,
                torch.bfloat16: 2e-2,
            }
            for te_output, torch_output in zip(te_outputs, torch_outputs):
>               assert_allclose(te_output, torch_output, tolerance, rtol[dtype])

tests/pytorch/test_numerics.py:1261: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[ 33.6992,  10.1368,  21.0150,  ..., -18.0752,   3.0496,  -2.8523],
        [ 33.6992,  10.1368,  21.0150,  .....3.0496,  -2.8523],
        [ 33.6992,  10.1368,  21.0150,  ..., -18.0752,   3.0496,  -2.8523]],
       device='cuda:0')
l2 = tensor([[ 33.7001,  10.1309,  21.0160,  ..., -18.0761,   3.0484,  -2.8546],
        [ 33.7001,  10.1309,  21.0160,  .....3.0484,  -2.8546],
        [ 33.7001,  10.1309,  21.0160,  ..., -18.0761,   3.0484,  -2.8546]],
       device='cuda:0')
atol = 0.005, rtol = 1.3e-06

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=0. Maximum difference at location [61] with 2.6603221893310547 vs 2.6514928340911865 (diff 0.008829355239868164).

tests/pytorch/test_numerics.py:218: AssertionError
_______________ test_linear_accuracy[False-False-small-1-dtype0] _______________

dtype = torch.float32, bs = 1, model = 'small', return_bias = False
bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_linear_accuracy(dtype, bs, model, return_bias, bias):
        config = model_configs[model]
    
        te_linear = TestReturnBiasModule(
            Linear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_linear = torch.nn.Linear(
            config.hidden_size,
            4 * config.hidden_size,
            bias=bias,
            device="cuda",
            dtype=dtype,
        )
    
        # Share params
        with torch.no_grad():
            torch_linear.weight = Parameter(te_linear.te_module.weight.clone())
            if bias:
                torch_linear.bias = Parameter(te_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_linear, bs, dtype, config)
    
        # Check output.
        if model == "small":
            tolerance = 5e-3 if dtype == torch.float32 else 5e-2
            rtol = {
                torch.float32: 1.3e-6,
                torch.half: 1e-2,
                torch.bfloat16: 2e-2,
            }
            for te_output, torch_output in zip(te_outputs, torch_outputs):
>               assert_allclose(te_output, torch_output, tolerance, rtol[dtype])

tests/pytorch/test_numerics.py:1261: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[ 17.5706,   3.3682,   8.8672,  ..., -15.9239,  -1.8259,  -1.5402],
        [ 17.5706,   3.3682,   8.8672,  .....1.8259,  -1.5402],
        [ 17.5706,   3.3682,   8.8672,  ..., -15.9239,  -1.8259,  -1.5402]],
       device='cuda:0')
l2 = tensor([[ 17.5718,   3.3661,   8.8677,  ..., -15.9255,  -1.8273,  -1.5447],
        [ 17.5718,   3.3661,   8.8677,  .....1.8273,  -1.5447],
        [ 17.5718,   3.3661,   8.8677,  ..., -15.9255,  -1.8273,  -1.5447]],
       device='cuda:0')
atol = 0.005, rtol = 1.3e-06

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=0. Maximum difference at location [50] with -3.5490684509277344 vs -3.55558705329895 (diff 0.00651860237121582).

tests/pytorch/test_numerics.py:218: AssertionError
_______________ test_linear_accuracy[False-False-small-2-dtype0] _______________

dtype = torch.float32, bs = 2, model = 'small', return_bias = False
bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_linear_accuracy(dtype, bs, model, return_bias, bias):
        config = model_configs[model]
    
        te_linear = TestReturnBiasModule(
            Linear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_linear = torch.nn.Linear(
            config.hidden_size,
            4 * config.hidden_size,
            bias=bias,
            device="cuda",
            dtype=dtype,
        )
    
        # Share params
        with torch.no_grad():
            torch_linear.weight = Parameter(te_linear.te_module.weight.clone())
            if bias:
                torch_linear.bias = Parameter(te_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_linear, bs, dtype, config)
    
        # Check output.
        if model == "small":
            tolerance = 5e-3 if dtype == torch.float32 else 5e-2
            rtol = {
                torch.float32: 1.3e-6,
                torch.half: 1e-2,
                torch.bfloat16: 2e-2,
            }
            for te_output, torch_output in zip(te_outputs, torch_outputs):
>               assert_allclose(te_output, torch_output, tolerance, rtol[dtype])

tests/pytorch/test_numerics.py:1261: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[ 33.6992,  10.1368,  21.0150,  ..., -18.0752,   3.0496,  -2.8523],
        [ 33.6992,  10.1368,  21.0150,  .....3.0496,  -2.8523],
        [ 33.6992,  10.1368,  21.0150,  ..., -18.0752,   3.0496,  -2.8523]],
       device='cuda:0')
l2 = tensor([[ 33.7001,  10.1309,  21.0160,  ..., -18.0761,   3.0484,  -2.8546],
        [ 33.7001,  10.1309,  21.0160,  .....3.0484,  -2.8546],
        [ 33.7001,  10.1309,  21.0160,  ..., -18.0761,   3.0484,  -2.8546]],
       device='cuda:0')
atol = 0.005, rtol = 1.3e-06

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=0. Maximum difference at location [61] with 2.6603221893310547 vs 2.6514928340911865 (diff 0.008829355239868164).

tests/pytorch/test_numerics.py:218: AssertionError
___ test_layernorm_linear_accuracy[True-True-True-LayerNorm-small-1-dtype0] ____

dtype = torch.float32, bs = 1, model = 'small', normalization = 'LayerNorm'
zero_centered_gamma = True, return_bias = True, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 0.0381, -0.4768,  0.2786,  ...,  0.1066,  0.0375,  0.1076]],

        [[ 0.2742, -0.1814,  0.1419,  ..., -0...       [[ 0.1256,  0.2742,  0.1532,  ...,  0.0127,  0.3139, -0.3922]]],
       device='cuda:0', grad_fn=<AddBackward0>)
l2 = tensor([[[ 0.0381, -0.4770,  0.2785,  ...,  0.1066,  0.0376,  0.1076]],

        [[ 0.2741, -0.1814,  0.1419,  ..., -0...      [[ 0.1254,  0.2742,  0.1532,  ...,  0.0127,  0.3139, -0.3921]]],
       device='cuda:0', grad_fn=<ViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=31. Maximum difference at location [0, 203] with 0.03489203006029129 vs 0.0345475971698761 (diff 0.00034443289041519165).

tests/pytorch/test_numerics.py:218: AssertionError
___ test_layernorm_linear_accuracy[True-True-True-LayerNorm-small-2-dtype0] ____

dtype = torch.float32, bs = 2, model = 'small', normalization = 'LayerNorm'
zero_centered_gamma = True, return_bias = True, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 2.9323e-01, -2.3376e-01, -3.0747e-01,  ...,  1.7433e-01,
           5.9622e-01, -6.8145e-02],
         [ 3....26e-01, -1.4499e-01,  ..., -9.2906e-02,
           6.7892e-01, -4.8855e-02]]], device='cuda:0', grad_fn=<AddBackward0>)
l2 = tensor([[[ 2.9316e-01, -2.3391e-01, -3.0754e-01,  ...,  1.7445e-01,
           5.9627e-01, -6.8063e-02],
         [ 3....-1.4501e-01,  ..., -9.2993e-02,
           6.7890e-01, -4.8849e-02]]], device='cuda:0',
       grad_fn=<ViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=11. Maximum difference at location [0, 21] with 0.029382703825831413 vs 0.02909543365240097 (diff 0.0002872701734304428).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_linear_accuracy[True-True-True-RMSNorm-small-1-dtype0] _____

dtype = torch.float32, bs = 1, model = 'small', normalization = 'RMSNorm'
zero_centered_gamma = True, return_bias = True, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 0.2989, -0.2393, -0.3001,  ...,  0.1753,  0.5886, -0.0707]],

        [[ 0.3477,  0.2963,  0.0539,  ...,  0...       [[ 0.0587,  0.5233, -0.2191,  ...,  0.0039, -0.2120, -0.2971]]],
       device='cuda:0', grad_fn=<AddBackward0>)
l2 = tensor([[[ 0.2990, -0.2395, -0.3002,  ...,  0.1754,  0.5886, -0.0707]],

        [[ 0.3477,  0.2962,  0.0539,  ...,  0...      [[ 0.0587,  0.5233, -0.2191,  ...,  0.0038, -0.2119, -0.2972]]],
       device='cuda:0', grad_fn=<ViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=56. Maximum difference at location [0, 209] with 0.003034749999642372 vs 0.0033195200376212597 (diff 0.00028477003797888756).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_linear_accuracy[True-True-True-RMSNorm-small-2-dtype0] _____

dtype = torch.float32, bs = 2, model = 'small', normalization = 'RMSNorm'
zero_centered_gamma = True, return_bias = True, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 0.2989, -0.2393, -0.3001,  ...,  0.1753,  0.5886, -0.0707],
         [ 0.3477,  0.2963,  0.0539,  ...,  0.2...        [-0.2692, -0.1839, -0.1282,  ..., -0.0904,  0.6605, -0.0547]]],
       device='cuda:0', grad_fn=<AddBackward0>)
l2 = tensor([[[ 0.2990, -0.2395, -0.3002,  ...,  0.1754,  0.5886, -0.0707],
         [ 0.3477,  0.2962,  0.0539,  ...,  0.2...       [-0.2691, -0.1839, -0.1283,  ..., -0.0904,  0.6604, -0.0548]]],
       device='cuda:0', grad_fn=<ViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=28. Maximum difference at location [1, 421] with -0.017969021573662758 vs -0.01827884092926979 (diff 0.0003098193556070328).

tests/pytorch/test_numerics.py:218: AssertionError
___ test_layernorm_linear_accuracy[True-True-False-LayerNorm-small-1-dtype0] ___

dtype = torch.float32, bs = 1, model = 'small', normalization = 'LayerNorm'
zero_centered_gamma = False, return_bias = True, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 2.9323e-01, -2.3376e-01, -3.0747e-01,  ...,  1.7433e-01,
           5.9622e-01, -6.8145e-02]],

        [[ ...34e-01, -1.7843e-01,  ...,  1.0380e-02,
          -2.6098e-01, -3.1786e-01]]], device='cuda:0', grad_fn=<AddBackward0>)
l2 = tensor([[[ 2.9316e-01, -2.3391e-01, -3.0754e-01,  ...,  1.7445e-01,
           5.9627e-01, -6.8063e-02]],

        [[ ...-1.7838e-01,  ...,  1.0395e-02,
          -2.6084e-01, -3.1800e-01]]], device='cuda:0',
       grad_fn=<ViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=22. Maximum difference at location [0, 21] with 0.029382703825831413 vs 0.02909543365240097 (diff 0.0002872701734304428).

tests/pytorch/test_numerics.py:218: AssertionError
___ test_layernorm_linear_accuracy[True-True-False-LayerNorm-small-2-dtype0] ___

dtype = torch.float32, bs = 2, model = 'small', normalization = 'LayerNorm'
zero_centered_gamma = False, return_bias = True, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 2.9323e-01, -2.3376e-01, -3.0747e-01,  ...,  1.7433e-01,
           5.9622e-01, -6.8145e-02],
         [ 3....26e-01, -1.4499e-01,  ..., -9.2906e-02,
           6.7892e-01, -4.8855e-02]]], device='cuda:0', grad_fn=<AddBackward0>)
l2 = tensor([[[ 2.9316e-01, -2.3391e-01, -3.0754e-01,  ...,  1.7445e-01,
           5.9627e-01, -6.8063e-02],
         [ 3....-1.4501e-01,  ..., -9.2993e-02,
           6.7890e-01, -4.8849e-02]]], device='cuda:0',
       grad_fn=<ViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=11. Maximum difference at location [0, 21] with 0.029382703825831413 vs 0.02909543365240097 (diff 0.0002872701734304428).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_linear_accuracy[True-True-False-RMSNorm-small-1-dtype0] ____

dtype = torch.float32, bs = 1, model = 'small', normalization = 'RMSNorm'
zero_centered_gamma = False, return_bias = True, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 0.2989, -0.2393, -0.3001,  ...,  0.1753,  0.5886, -0.0707]],

        [[ 0.3477,  0.2963,  0.0539,  ...,  0...       [[ 0.0587,  0.5233, -0.2191,  ...,  0.0039, -0.2120, -0.2971]]],
       device='cuda:0', grad_fn=<AddBackward0>)
l2 = tensor([[[ 0.2990, -0.2395, -0.3002,  ...,  0.1754,  0.5886, -0.0707]],

        [[ 0.3477,  0.2962,  0.0539,  ...,  0...      [[ 0.0587,  0.5233, -0.2191,  ...,  0.0038, -0.2119, -0.2972]]],
       device='cuda:0', grad_fn=<ViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=56. Maximum difference at location [0, 209] with 0.003034749999642372 vs 0.0033195200376212597 (diff 0.00028477003797888756).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_linear_accuracy[True-True-False-RMSNorm-small-2-dtype0] ____

dtype = torch.float32, bs = 2, model = 'small', normalization = 'RMSNorm'
zero_centered_gamma = False, return_bias = True, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 0.2989, -0.2393, -0.3001,  ...,  0.1753,  0.5886, -0.0707],
         [ 0.3477,  0.2963,  0.0539,  ...,  0.2...        [-0.2692, -0.1839, -0.1282,  ..., -0.0904,  0.6605, -0.0547]]],
       device='cuda:0', grad_fn=<AddBackward0>)
l2 = tensor([[[ 0.2990, -0.2395, -0.3002,  ...,  0.1754,  0.5886, -0.0707],
         [ 0.3477,  0.2962,  0.0539,  ...,  0.2...       [-0.2691, -0.1839, -0.1283,  ..., -0.0904,  0.6604, -0.0548]]],
       device='cuda:0', grad_fn=<ViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=28. Maximum difference at location [1, 421] with -0.017969021573662758 vs -0.01827884092926979 (diff 0.0003098193556070328).

tests/pytorch/test_numerics.py:218: AssertionError
___ test_layernorm_linear_accuracy[True-False-True-LayerNorm-small-1-dtype0] ___

dtype = torch.float32, bs = 1, model = 'small', normalization = 'LayerNorm'
zero_centered_gamma = True, return_bias = False, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 2.9323e-01, -2.3376e-01, -3.0747e-01,  ...,  1.7433e-01,
           5.9622e-01, -6.8145e-02]],

        [[ ...,  ...,  1.0380e-02,
          -2.6098e-01, -3.1786e-01]]], device='cuda:0',
       grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 2.9316e-01, -2.3391e-01, -3.0754e-01,  ...,  1.7445e-01,
           5.9627e-01, -6.8063e-02]],

        [[ ...-1.7838e-01,  ...,  1.0395e-02,
          -2.6084e-01, -3.1800e-01]]], device='cuda:0',
       grad_fn=<ViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=22. Maximum difference at location [0, 21] with 0.029382703825831413 vs 0.02909543365240097 (diff 0.0002872701734304428).

tests/pytorch/test_numerics.py:218: AssertionError
___ test_layernorm_linear_accuracy[True-False-True-LayerNorm-small-2-dtype0] ___

dtype = torch.float32, bs = 2, model = 'small', normalization = 'LayerNorm'
zero_centered_gamma = True, return_bias = False, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 2.9323e-01, -2.3376e-01, -3.0747e-01,  ...,  1.7433e-01,
           5.9622e-01, -6.8145e-02],
         [ 3....,  ..., -9.2906e-02,
           6.7892e-01, -4.8855e-02]]], device='cuda:0',
       grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 2.9316e-01, -2.3391e-01, -3.0754e-01,  ...,  1.7445e-01,
           5.9627e-01, -6.8063e-02],
         [ 3....-1.4501e-01,  ..., -9.2993e-02,
           6.7890e-01, -4.8849e-02]]], device='cuda:0',
       grad_fn=<ViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=11. Maximum difference at location [0, 21] with 0.029382703825831413 vs 0.02909543365240097 (diff 0.0002872701734304428).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_linear_accuracy[True-False-True-RMSNorm-small-1-dtype0] ____

dtype = torch.float32, bs = 1, model = 'small', normalization = 'RMSNorm'
zero_centered_gamma = True, return_bias = False, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 0.2989, -0.2393, -0.3001,  ...,  0.1753,  0.5886, -0.0707]],

        [[ 0.3477,  0.2963,  0.0539,  ...,  0...0587,  0.5233, -0.2191,  ...,  0.0039, -0.2120, -0.2971]]],
       device='cuda:0', grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 0.2990, -0.2395, -0.3002,  ...,  0.1754,  0.5886, -0.0707]],

        [[ 0.3477,  0.2962,  0.0539,  ...,  0...      [[ 0.0587,  0.5233, -0.2191,  ...,  0.0038, -0.2119, -0.2972]]],
       device='cuda:0', grad_fn=<ViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=56. Maximum difference at location [0, 209] with 0.003034749999642372 vs 0.0033195200376212597 (diff 0.00028477003797888756).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_linear_accuracy[True-False-True-RMSNorm-small-2-dtype0] ____

dtype = torch.float32, bs = 2, model = 'small', normalization = 'RMSNorm'
zero_centered_gamma = True, return_bias = False, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 0.2989, -0.2393, -0.3001,  ...,  0.1753,  0.5886, -0.0707],
         [ 0.3477,  0.2963,  0.0539,  ...,  0.2...2692, -0.1839, -0.1282,  ..., -0.0904,  0.6605, -0.0547]]],
       device='cuda:0', grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 0.2990, -0.2395, -0.3002,  ...,  0.1754,  0.5886, -0.0707],
         [ 0.3477,  0.2962,  0.0539,  ...,  0.2...       [-0.2691, -0.1839, -0.1283,  ..., -0.0904,  0.6604, -0.0548]]],
       device='cuda:0', grad_fn=<ViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=28. Maximum difference at location [1, 421] with -0.017969021573662758 vs -0.01827884092926979 (diff 0.0003098193556070328).

tests/pytorch/test_numerics.py:218: AssertionError
__ test_layernorm_linear_accuracy[True-False-False-LayerNorm-small-1-dtype0] ___

dtype = torch.float32, bs = 1, model = 'small', normalization = 'LayerNorm'
zero_centered_gamma = False, return_bias = False, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 2.9323e-01, -2.3376e-01, -3.0747e-01,  ...,  1.7433e-01,
           5.9622e-01, -6.8145e-02]],

        [[ ...,  ...,  1.0380e-02,
          -2.6098e-01, -3.1786e-01]]], device='cuda:0',
       grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 2.9316e-01, -2.3391e-01, -3.0754e-01,  ...,  1.7445e-01,
           5.9627e-01, -6.8063e-02]],

        [[ ...-1.7838e-01,  ...,  1.0395e-02,
          -2.6084e-01, -3.1800e-01]]], device='cuda:0',
       grad_fn=<ViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=22. Maximum difference at location [0, 21] with 0.029382703825831413 vs 0.02909543365240097 (diff 0.0002872701734304428).

tests/pytorch/test_numerics.py:218: AssertionError
__ test_layernorm_linear_accuracy[True-False-False-LayerNorm-small-2-dtype0] ___

dtype = torch.float32, bs = 2, model = 'small', normalization = 'LayerNorm'
zero_centered_gamma = False, return_bias = False, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 2.9323e-01, -2.3376e-01, -3.0747e-01,  ...,  1.7433e-01,
           5.9622e-01, -6.8145e-02],
         [ 3....,  ..., -9.2906e-02,
           6.7892e-01, -4.8855e-02]]], device='cuda:0',
       grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 2.9316e-01, -2.3391e-01, -3.0754e-01,  ...,  1.7445e-01,
           5.9627e-01, -6.8063e-02],
         [ 3....-1.4501e-01,  ..., -9.2993e-02,
           6.7890e-01, -4.8849e-02]]], device='cuda:0',
       grad_fn=<ViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=11. Maximum difference at location [0, 21] with 0.029382703825831413 vs 0.02909543365240097 (diff 0.0002872701734304428).

tests/pytorch/test_numerics.py:218: AssertionError
___ test_layernorm_linear_accuracy[True-False-False-RMSNorm-small-1-dtype0] ____

dtype = torch.float32, bs = 1, model = 'small', normalization = 'RMSNorm'
zero_centered_gamma = False, return_bias = False, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 0.2989, -0.2393, -0.3001,  ...,  0.1753,  0.5886, -0.0707]],

        [[ 0.3477,  0.2963,  0.0539,  ...,  0...0587,  0.5233, -0.2191,  ...,  0.0039, -0.2120, -0.2971]]],
       device='cuda:0', grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 0.2990, -0.2395, -0.3002,  ...,  0.1754,  0.5886, -0.0707]],

        [[ 0.3477,  0.2962,  0.0539,  ...,  0...      [[ 0.0587,  0.5233, -0.2191,  ...,  0.0038, -0.2119, -0.2972]]],
       device='cuda:0', grad_fn=<ViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=56. Maximum difference at location [0, 209] with 0.003034749999642372 vs 0.0033195200376212597 (diff 0.00028477003797888756).

tests/pytorch/test_numerics.py:218: AssertionError
___ test_layernorm_linear_accuracy[True-False-False-RMSNorm-small-2-dtype0] ____

dtype = torch.float32, bs = 2, model = 'small', normalization = 'RMSNorm'
zero_centered_gamma = False, return_bias = False, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 0.2989, -0.2393, -0.3001,  ...,  0.1753,  0.5886, -0.0707],
         [ 0.3477,  0.2963,  0.0539,  ...,  0.2...2692, -0.1839, -0.1282,  ..., -0.0904,  0.6605, -0.0547]]],
       device='cuda:0', grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 0.2990, -0.2395, -0.3002,  ...,  0.1754,  0.5886, -0.0707],
         [ 0.3477,  0.2962,  0.0539,  ...,  0.2...       [-0.2691, -0.1839, -0.1283,  ..., -0.0904,  0.6604, -0.0548]]],
       device='cuda:0', grad_fn=<ViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=28. Maximum difference at location [1, 421] with -0.017969021573662758 vs -0.01827884092926979 (diff 0.0003098193556070328).

tests/pytorch/test_numerics.py:218: AssertionError
___ test_layernorm_linear_accuracy[False-True-True-LayerNorm-small-1-dtype0] ___

dtype = torch.float32, bs = 1, model = 'small', normalization = 'LayerNorm'
zero_centered_gamma = True, return_bias = True, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 2.9323e-01, -2.3376e-01, -3.0747e-01,  ...,  1.7433e-01,
           5.9622e-01, -6.8145e-02]],

        [[ ...,  ...,  1.0380e-02,
          -2.6098e-01, -3.1786e-01]]], device='cuda:0',
       grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 2.9316e-01, -2.3391e-01, -3.0754e-01,  ...,  1.7445e-01,
           5.9627e-01, -6.8063e-02]],

        [[ ...8e-01,  ...,  1.0395e-02,
          -2.6084e-01, -3.1800e-01]]], device='cuda:0',
       grad_fn=<UnsafeViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=22. Maximum difference at location [0, 21] with 0.029382703825831413 vs 0.02909543365240097 (diff 0.0002872701734304428).

tests/pytorch/test_numerics.py:218: AssertionError
___ test_layernorm_linear_accuracy[False-True-True-LayerNorm-small-2-dtype0] ___

dtype = torch.float32, bs = 2, model = 'small', normalization = 'LayerNorm'
zero_centered_gamma = True, return_bias = True, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 2.9323e-01, -2.3376e-01, -3.0747e-01,  ...,  1.7433e-01,
           5.9622e-01, -6.8145e-02],
         [ 3....,  ..., -9.2906e-02,
           6.7892e-01, -4.8855e-02]]], device='cuda:0',
       grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 2.9316e-01, -2.3391e-01, -3.0754e-01,  ...,  1.7445e-01,
           5.9627e-01, -6.8063e-02],
         [ 3....1e-01,  ..., -9.2993e-02,
           6.7890e-01, -4.8849e-02]]], device='cuda:0',
       grad_fn=<UnsafeViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=11. Maximum difference at location [0, 21] with 0.029382703825831413 vs 0.02909543365240097 (diff 0.0002872701734304428).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_linear_accuracy[False-True-True-RMSNorm-small-1-dtype0] ____

dtype = torch.float32, bs = 1, model = 'small', normalization = 'RMSNorm'
zero_centered_gamma = True, return_bias = True, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 0.2989, -0.2393, -0.3001,  ...,  0.1753,  0.5886, -0.0707]],

        [[ 0.3477,  0.2963,  0.0539,  ...,  0...0587,  0.5233, -0.2191,  ...,  0.0039, -0.2120, -0.2971]]],
       device='cuda:0', grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 0.2990, -0.2395, -0.3002,  ...,  0.1754,  0.5886, -0.0707]],

        [[ 0.3477,  0.2962,  0.0539,  ...,  0...[[ 0.0587,  0.5233, -0.2191,  ...,  0.0038, -0.2119, -0.2972]]],
       device='cuda:0', grad_fn=<UnsafeViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=56. Maximum difference at location [0, 209] with 0.003034749999642372 vs 0.0033195200376212597 (diff 0.00028477003797888756).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_linear_accuracy[False-True-True-RMSNorm-small-2-dtype0] ____

dtype = torch.float32, bs = 2, model = 'small', normalization = 'RMSNorm'
zero_centered_gamma = True, return_bias = True, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 0.2989, -0.2393, -0.3001,  ...,  0.1753,  0.5886, -0.0707],
         [ 0.3477,  0.2963,  0.0539,  ...,  0.2...2692, -0.1839, -0.1282,  ..., -0.0904,  0.6605, -0.0547]]],
       device='cuda:0', grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 0.2990, -0.2395, -0.3002,  ...,  0.1754,  0.5886, -0.0707],
         [ 0.3477,  0.2962,  0.0539,  ...,  0.2... [-0.2691, -0.1839, -0.1283,  ..., -0.0904,  0.6604, -0.0548]]],
       device='cuda:0', grad_fn=<UnsafeViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=28. Maximum difference at location [1, 421] with -0.017969021573662758 vs -0.01827884092926979 (diff 0.0003098193556070328).

tests/pytorch/test_numerics.py:218: AssertionError
__ test_layernorm_linear_accuracy[False-True-False-LayerNorm-small-1-dtype0] ___

dtype = torch.float32, bs = 1, model = 'small', normalization = 'LayerNorm'
zero_centered_gamma = False, return_bias = True, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 2.9323e-01, -2.3376e-01, -3.0747e-01,  ...,  1.7433e-01,
           5.9622e-01, -6.8145e-02]],

        [[ ...,  ...,  1.0380e-02,
          -2.6098e-01, -3.1786e-01]]], device='cuda:0',
       grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 2.9316e-01, -2.3391e-01, -3.0754e-01,  ...,  1.7445e-01,
           5.9627e-01, -6.8063e-02]],

        [[ ...8e-01,  ...,  1.0395e-02,
          -2.6084e-01, -3.1800e-01]]], device='cuda:0',
       grad_fn=<UnsafeViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=22. Maximum difference at location [0, 21] with 0.029382703825831413 vs 0.02909543365240097 (diff 0.0002872701734304428).

tests/pytorch/test_numerics.py:218: AssertionError
__ test_layernorm_linear_accuracy[False-True-False-LayerNorm-small-2-dtype0] ___

dtype = torch.float32, bs = 2, model = 'small', normalization = 'LayerNorm'
zero_centered_gamma = False, return_bias = True, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 2.9323e-01, -2.3376e-01, -3.0747e-01,  ...,  1.7433e-01,
           5.9622e-01, -6.8145e-02],
         [ 3....,  ..., -9.2906e-02,
           6.7892e-01, -4.8855e-02]]], device='cuda:0',
       grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 2.9316e-01, -2.3391e-01, -3.0754e-01,  ...,  1.7445e-01,
           5.9627e-01, -6.8063e-02],
         [ 3....1e-01,  ..., -9.2993e-02,
           6.7890e-01, -4.8849e-02]]], device='cuda:0',
       grad_fn=<UnsafeViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=11. Maximum difference at location [0, 21] with 0.029382703825831413 vs 0.02909543365240097 (diff 0.0002872701734304428).

tests/pytorch/test_numerics.py:218: AssertionError
___ test_layernorm_linear_accuracy[False-True-False-RMSNorm-small-1-dtype0] ____

dtype = torch.float32, bs = 1, model = 'small', normalization = 'RMSNorm'
zero_centered_gamma = False, return_bias = True, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 0.2989, -0.2393, -0.3001,  ...,  0.1753,  0.5886, -0.0707]],

        [[ 0.3477,  0.2963,  0.0539,  ...,  0...0587,  0.5233, -0.2191,  ...,  0.0039, -0.2120, -0.2971]]],
       device='cuda:0', grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 0.2990, -0.2395, -0.3002,  ...,  0.1754,  0.5886, -0.0707]],

        [[ 0.3477,  0.2962,  0.0539,  ...,  0...[[ 0.0587,  0.5233, -0.2191,  ...,  0.0038, -0.2119, -0.2972]]],
       device='cuda:0', grad_fn=<UnsafeViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=56. Maximum difference at location [0, 209] with 0.003034749999642372 vs 0.0033195200376212597 (diff 0.00028477003797888756).

tests/pytorch/test_numerics.py:218: AssertionError
___ test_layernorm_linear_accuracy[False-True-False-RMSNorm-small-2-dtype0] ____

dtype = torch.float32, bs = 2, model = 'small', normalization = 'RMSNorm'
zero_centered_gamma = False, return_bias = True, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 0.2989, -0.2393, -0.3001,  ...,  0.1753,  0.5886, -0.0707],
         [ 0.3477,  0.2963,  0.0539,  ...,  0.2...2692, -0.1839, -0.1282,  ..., -0.0904,  0.6605, -0.0547]]],
       device='cuda:0', grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 0.2990, -0.2395, -0.3002,  ...,  0.1754,  0.5886, -0.0707],
         [ 0.3477,  0.2962,  0.0539,  ...,  0.2... [-0.2691, -0.1839, -0.1283,  ..., -0.0904,  0.6604, -0.0548]]],
       device='cuda:0', grad_fn=<UnsafeViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=28. Maximum difference at location [1, 421] with -0.017969021573662758 vs -0.01827884092926979 (diff 0.0003098193556070328).

tests/pytorch/test_numerics.py:218: AssertionError
__ test_layernorm_linear_accuracy[False-False-True-LayerNorm-small-1-dtype0] ___

dtype = torch.float32, bs = 1, model = 'small', normalization = 'LayerNorm'
zero_centered_gamma = True, return_bias = False, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 2.9323e-01, -2.3376e-01, -3.0747e-01,  ...,  1.7433e-01,
           5.9622e-01, -6.8145e-02]],

        [[ ...,  ...,  1.0380e-02,
          -2.6098e-01, -3.1786e-01]]], device='cuda:0',
       grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 2.9316e-01, -2.3391e-01, -3.0754e-01,  ...,  1.7445e-01,
           5.9627e-01, -6.8063e-02]],

        [[ ...8e-01,  ...,  1.0395e-02,
          -2.6084e-01, -3.1800e-01]]], device='cuda:0',
       grad_fn=<UnsafeViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=22. Maximum difference at location [0, 21] with 0.029382703825831413 vs 0.02909543365240097 (diff 0.0002872701734304428).

tests/pytorch/test_numerics.py:218: AssertionError
__ test_layernorm_linear_accuracy[False-False-True-LayerNorm-small-2-dtype0] ___

dtype = torch.float32, bs = 2, model = 'small', normalization = 'LayerNorm'
zero_centered_gamma = True, return_bias = False, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 2.9323e-01, -2.3376e-01, -3.0747e-01,  ...,  1.7433e-01,
           5.9622e-01, -6.8145e-02],
         [ 3....,  ..., -9.2906e-02,
           6.7892e-01, -4.8855e-02]]], device='cuda:0',
       grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 2.9316e-01, -2.3391e-01, -3.0754e-01,  ...,  1.7445e-01,
           5.9627e-01, -6.8063e-02],
         [ 3....1e-01,  ..., -9.2993e-02,
           6.7890e-01, -4.8849e-02]]], device='cuda:0',
       grad_fn=<UnsafeViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=11. Maximum difference at location [0, 21] with 0.029382703825831413 vs 0.02909543365240097 (diff 0.0002872701734304428).

tests/pytorch/test_numerics.py:218: AssertionError
___ test_layernorm_linear_accuracy[False-False-True-RMSNorm-small-1-dtype0] ____

dtype = torch.float32, bs = 1, model = 'small', normalization = 'RMSNorm'
zero_centered_gamma = True, return_bias = False, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 0.2989, -0.2393, -0.3001,  ...,  0.1753,  0.5886, -0.0707]],

        [[ 0.3477,  0.2963,  0.0539,  ...,  0...0587,  0.5233, -0.2191,  ...,  0.0039, -0.2120, -0.2971]]],
       device='cuda:0', grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 0.2990, -0.2395, -0.3002,  ...,  0.1754,  0.5886, -0.0707]],

        [[ 0.3477,  0.2962,  0.0539,  ...,  0...[[ 0.0587,  0.5233, -0.2191,  ...,  0.0038, -0.2119, -0.2972]]],
       device='cuda:0', grad_fn=<UnsafeViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=56. Maximum difference at location [0, 209] with 0.003034749999642372 vs 0.0033195200376212597 (diff 0.00028477003797888756).

tests/pytorch/test_numerics.py:218: AssertionError
___ test_layernorm_linear_accuracy[False-False-True-RMSNorm-small-2-dtype0] ____

dtype = torch.float32, bs = 2, model = 'small', normalization = 'RMSNorm'
zero_centered_gamma = True, return_bias = False, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 0.2989, -0.2393, -0.3001,  ...,  0.1753,  0.5886, -0.0707],
         [ 0.3477,  0.2963,  0.0539,  ...,  0.2...2692, -0.1839, -0.1282,  ..., -0.0904,  0.6605, -0.0547]]],
       device='cuda:0', grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 0.2990, -0.2395, -0.3002,  ...,  0.1754,  0.5886, -0.0707],
         [ 0.3477,  0.2962,  0.0539,  ...,  0.2... [-0.2691, -0.1839, -0.1283,  ..., -0.0904,  0.6604, -0.0548]]],
       device='cuda:0', grad_fn=<UnsafeViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=28. Maximum difference at location [1, 421] with -0.017969021573662758 vs -0.01827884092926979 (diff 0.0003098193556070328).

tests/pytorch/test_numerics.py:218: AssertionError
__ test_layernorm_linear_accuracy[False-False-False-LayerNorm-small-1-dtype0] __

dtype = torch.float32, bs = 1, model = 'small', normalization = 'LayerNorm'
zero_centered_gamma = False, return_bias = False, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 2.9323e-01, -2.3376e-01, -3.0747e-01,  ...,  1.7433e-01,
           5.9622e-01, -6.8145e-02]],

        [[ ...,  ...,  1.0380e-02,
          -2.6098e-01, -3.1786e-01]]], device='cuda:0',
       grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 2.9316e-01, -2.3391e-01, -3.0754e-01,  ...,  1.7445e-01,
           5.9627e-01, -6.8063e-02]],

        [[ ...8e-01,  ...,  1.0395e-02,
          -2.6084e-01, -3.1800e-01]]], device='cuda:0',
       grad_fn=<UnsafeViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=22. Maximum difference at location [0, 21] with 0.029382703825831413 vs 0.02909543365240097 (diff 0.0002872701734304428).

tests/pytorch/test_numerics.py:218: AssertionError
__ test_layernorm_linear_accuracy[False-False-False-LayerNorm-small-2-dtype0] __

dtype = torch.float32, bs = 2, model = 'small', normalization = 'LayerNorm'
zero_centered_gamma = False, return_bias = False, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 2.9323e-01, -2.3376e-01, -3.0747e-01,  ...,  1.7433e-01,
           5.9622e-01, -6.8145e-02],
         [ 3....,  ..., -9.2906e-02,
           6.7892e-01, -4.8855e-02]]], device='cuda:0',
       grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 2.9316e-01, -2.3391e-01, -3.0754e-01,  ...,  1.7445e-01,
           5.9627e-01, -6.8063e-02],
         [ 3....1e-01,  ..., -9.2993e-02,
           6.7890e-01, -4.8849e-02]]], device='cuda:0',
       grad_fn=<UnsafeViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=11. Maximum difference at location [0, 21] with 0.029382703825831413 vs 0.02909543365240097 (diff 0.0002872701734304428).

tests/pytorch/test_numerics.py:218: AssertionError
___ test_layernorm_linear_accuracy[False-False-False-RMSNorm-small-1-dtype0] ___

dtype = torch.float32, bs = 1, model = 'small', normalization = 'RMSNorm'
zero_centered_gamma = False, return_bias = False, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 0.2989, -0.2393, -0.3001,  ...,  0.1753,  0.5886, -0.0707]],

        [[ 0.3477,  0.2963,  0.0539,  ...,  0...0587,  0.5233, -0.2191,  ...,  0.0039, -0.2120, -0.2971]]],
       device='cuda:0', grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 0.2990, -0.2395, -0.3002,  ...,  0.1754,  0.5886, -0.0707]],

        [[ 0.3477,  0.2962,  0.0539,  ...,  0...[[ 0.0587,  0.5233, -0.2191,  ...,  0.0038, -0.2119, -0.2972]]],
       device='cuda:0', grad_fn=<UnsafeViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=56. Maximum difference at location [0, 209] with 0.003034749999642372 vs 0.0033195200376212597 (diff 0.00028477003797888756).

tests/pytorch/test_numerics.py:218: AssertionError
___ test_layernorm_linear_accuracy[False-False-False-RMSNorm-small-2-dtype0] ___

dtype = torch.float32, bs = 2, model = 'small', normalization = 'RMSNorm'
zero_centered_gamma = False, return_bias = False, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("zero_centered_gamma", all_boolean)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_linear_accuracy(
        dtype, bs, model, normalization, zero_centered_gamma, return_bias, bias
    ):
        config = model_configs[model]
    
        te_ln_linear = TestReturnBiasModule(
            LayerNormLinear,
            in_features=config.hidden_size,
            out_features=4 * config.hidden_size,
            eps=config.eps,
            normalization=normalization,
            params_dtype=dtype,
            zero_centered_gamma=zero_centered_gamma,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_linear = (
            TorchLayerNormLinear(
                config.hidden_size,
                4 * config.hidden_size,
                config.eps,
                normalization=normalization,
                zero_centered_gamma=zero_centered_gamma,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_linear.layernorm.weight = Parameter(
                te_ln_linear.te_module.layer_norm_weight.clone()
            )
            if normalization != "RMSNorm":
                torch_ln_linear.layernorm.bias = Parameter(
                    te_ln_linear.te_module.layer_norm_bias.clone()
                )
            torch_ln_linear.linear.weight = Parameter(te_ln_linear.te_module.weight.clone())
            if bias:
                torch_ln_linear.linear.bias = Parameter(te_ln_linear.te_module.bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_linear, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_linear, bs, dtype, config)
    
        atol = {
            torch.float32: 2.5e-4,
            torch.half: 2e-3,
            torch.bfloat16: 2e-2,
        }
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
>       assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1538: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[ 0.2989, -0.2393, -0.3001,  ...,  0.1753,  0.5886, -0.0707],
         [ 0.3477,  0.2963,  0.0539,  ...,  0.2...2692, -0.1839, -0.1282,  ..., -0.0904,  0.6605, -0.0547]]],
       device='cuda:0', grad_fn=<_LayerNormLinearBackward>)
l2 = tensor([[[ 0.2990, -0.2395, -0.3002,  ...,  0.1754,  0.5886, -0.0707],
         [ 0.3477,  0.2962,  0.0539,  ...,  0.2... [-0.2691, -0.1839, -0.1283,  ..., -0.0904,  0.6604, -0.0548]]],
       device='cuda:0', grad_fn=<UnsafeViewBackward0>)
atol = 0.00025, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=28. Maximum difference at location [1, 421] with -0.017969021573662758 vs -0.01827884092926979 (diff 0.0003098193556070328).

tests/pytorch/test_numerics.py:218: AssertionError
_____ test_layernorm_mlp_accuracy[True-True-LayerNorm-relu-small-1-dtype0] _____

dtype = torch.float32, bs = 1, model = 'small', activation = 'relu'
normalization = 'LayerNorm', return_bias = True, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[-4.0903e-02, -2.5958e-02, -4.5321e-02,  ...,  9.8564e-02,
          -6.1800e-02,  1.4865e-01]],

        [[-...    [[-9.8368e-02, -1.2242e-01,  4.9636e-02,  ..., -2.8892e-02,
           5.7595e-02,  5.6001e-02]]], device='cuda:0')
l2 = tensor([[[-4.0842e-02, -2.5950e-02, -4.5320e-02,  ...,  9.8524e-02,
          -6.1896e-02,  1.4863e-01]],

        [[-...    [[-9.8295e-02, -1.1865e-01,  4.1502e-02,  ..., -2.8785e-02,
           5.7626e-02,  5.6190e-02]]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=14. Maximum difference at location [0, 99] with -0.07819170504808426 vs -0.10008896142244339 (diff 0.02189725637435913).

tests/pytorch/test_numerics.py:218: AssertionError
_____ test_layernorm_mlp_accuracy[True-True-LayerNorm-relu-small-2-dtype0] _____

dtype = torch.float32, bs = 2, model = 'small', activation = 'relu'
normalization = 'LayerNorm', return_bias = True, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[-4.0903e-02, -2.5958e-02, -4.5321e-02,  ...,  9.8564e-02,
          -6.1800e-02,  1.4865e-01],
         [-1....     [ 3.5071e-02, -1.3502e-01, -1.3495e-01,  ...,  7.4266e-02,
           7.5113e-02,  8.7058e-03]]], device='cuda:0')
l2 = tensor([[[-4.0842e-02, -2.5950e-02, -4.5320e-02,  ...,  9.8524e-02,
          -6.1896e-02,  1.4863e-01],
         [-1....     [ 3.5103e-02, -1.3504e-01, -1.3494e-01,  ...,  7.4227e-02,
           7.5035e-02,  8.6968e-03]]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=7. Maximum difference at location [0, 99] with -0.07819170504808426 vs -0.10008896142244339 (diff 0.02189725637435913).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_mlp_accuracy[True-True-LayerNorm-reglu-small-1-dtype0] _____

dtype = torch.float32, bs = 1, model = 'small', activation = 'reglu'
normalization = 'LayerNorm', return_bias = True, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[-4.8469e-01,  3.4499e-01, -9.7453e-04,  ..., -1.7425e-01,
          3.4103e-01, -1.4285e-01],
        [ 1.329...       [-1.7937e-02, -1.6914e-01, -1.8572e-01,  ...,  2.0881e-01,
          2.1179e-01, -4.2140e-02]], device='cuda:0')
l2 = tensor([[-4.8448e-01,  3.4507e-01, -1.0844e-03,  ..., -1.7429e-01,
          3.4099e-01, -1.4295e-01],
        [ 1.340...       [-1.7905e-02, -1.6928e-01, -1.8583e-01,  ...,  2.0898e-01,
          2.1187e-01, -4.2232e-02]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=105. Maximum difference at location [75] with -0.0472244955599308 vs -0.014243432320654392 (diff 0.032981064170598984).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_mlp_accuracy[True-True-LayerNorm-reglu-small-2-dtype0] _____

dtype = torch.float32, bs = 2, model = 'small', activation = 'reglu'
normalization = 'LayerNorm', return_bias = True, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([ 1.8349,  1.4578,  0.5213, -1.0315, -1.6942,  2.4269,  0.4855,  0.3110,
         0.1912, -0.6689, -0.5850, -1....1135,  0.1760,
        -1.1478, -0.6440, -1.3703, -0.4670,  1.5899, -0.5406,  0.6753,  0.2962],
       device='cuda:0')
l2 = tensor([ 1.8335,  1.4575,  0.5247, -1.0402, -1.6862,  2.4275,  0.4863,  0.3120,
         0.1898, -0.6858, -0.5879, -1....1102,  0.1732,
        -1.1513, -0.6487, -1.3754, -0.4696,  1.5853, -0.5460,  0.6661,  0.2978],
       device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=22. Maximum difference at location [0] with -1.3210716247558594 vs -1.299375057220459 (diff 0.02169656753540039).

tests/pytorch/test_numerics.py:218: AssertionError
______ test_layernorm_mlp_accuracy[True-True-RMSNorm-relu-small-1-dtype0] ______

dtype = torch.float32, bs = 1, model = 'small', activation = 'relu'
normalization = 'RMSNorm', return_bias = True, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[-3.2913e+00, -1.5585e+00,  7.6284e-02,  ...,  4.2568e-01,
         -1.9121e-01,  1.5443e+00],
        [-5.961...       [-2.1555e-02, -2.8658e-01, -6.1978e-01,  ...,  2.0026e-01,
          5.6838e-01, -2.2261e-01]], device='cuda:0')
l2 = tensor([[-3.2907e+00, -1.5584e+00,  7.6177e-02,  ...,  4.2493e-01,
         -1.9130e-01,  1.5448e+00],
        [-6.013...       [-2.1587e-02, -2.8688e-01, -6.2038e-01,  ...,  2.0036e-01,
          5.6897e-01, -2.2260e-01]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=50. Maximum difference at location [100] with 0.18327626585960388 vs 0.3811267912387848 (diff 0.1978505253791809).

tests/pytorch/test_numerics.py:218: AssertionError
______ test_layernorm_mlp_accuracy[True-True-RMSNorm-relu-small-2-dtype0] ______

dtype = torch.float32, bs = 2, model = 'small', activation = 'relu'
normalization = 'RMSNorm', return_bias = True, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[-4.6028e-02, -3.2271e-02, -5.2628e-02,  ...,  1.0047e-01,
          -7.6332e-02,  1.2294e-01],
         [-1....     [ 2.8777e-02, -1.6675e-01, -1.7494e-01,  ...,  7.0587e-02,
           7.1250e-02, -9.0677e-03]]], device='cuda:0')
l2 = tensor([[[-4.5971e-02, -3.2265e-02, -5.2626e-02,  ...,  1.0042e-01,
          -7.6427e-02,  1.2293e-01],
         [-1....     [ 2.8811e-02, -1.6676e-01, -1.7492e-01,  ...,  7.0552e-02,
           7.1163e-02, -9.0771e-03]]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=121. Maximum difference at location [1, 15] with -0.007176937535405159 vs 0.015477740205824375 (diff 0.02265467867255211).

tests/pytorch/test_numerics.py:218: AssertionError
_____ test_layernorm_mlp_accuracy[True-True-RMSNorm-reglu-small-1-dtype0] ______

dtype = torch.float32, bs = 1, model = 'small', activation = 'reglu'
normalization = 'RMSNorm', return_bias = True, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[-3.4965e-01,  3.6868e-01,  6.6983e-02,  ..., -1.7441e-01,
          5.1494e-01, -1.2830e-01],
        [ 1.426...       [-1.9212e-02, -1.6958e-01, -1.8650e-01,  ...,  2.0023e-01,
          2.0466e-01, -5.4184e-02]], device='cuda:0')
l2 = tensor([[-3.4950e-01,  3.6870e-01,  6.6943e-02,  ..., -1.7423e-01,
          5.1493e-01, -1.2815e-01],
        [ 1.439...       [-1.9107e-02, -1.6978e-01, -1.8666e-01,  ...,  2.0039e-01,
          2.0480e-01, -5.4196e-02]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=351. Maximum difference at location [77] with -0.04859238490462303 vs -0.07616263628005981 (diff 0.027570251375436783).

tests/pytorch/test_numerics.py:218: AssertionError
_____ test_layernorm_mlp_accuracy[True-True-RMSNorm-reglu-small-2-dtype0] ______

dtype = torch.float32, bs = 2, model = 'small', activation = 'reglu'
normalization = 'RMSNorm', return_bias = True, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[-7.1748e-01,  3.0336e-01, -1.6657e-01,  ..., -5.6715e-01,
          8.0463e-01, -5.2289e-02],
        [-5.886...       [-9.1481e-02, -2.4700e-01, -2.0221e-01,  ...,  2.6127e-01,
          5.9658e-02, -2.6389e-01]], device='cuda:0')
l2 = tensor([[-7.1713e-01,  3.0324e-01, -1.6643e-01,  ..., -5.6724e-01,
          8.0480e-01, -5.2136e-02],
        [-5.936...       [-9.1421e-02, -2.4719e-01, -2.0242e-01,  ...,  2.6152e-01,
          5.9713e-02, -2.6396e-01]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=263. Maximum difference at location [56] with 0.03622262179851532 vs 0.12370995432138443 (diff 0.08748733252286911).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_mlp_accuracy[True-False-LayerNorm-relu-small-1-dtype0] _____

dtype = torch.float32, bs = 1, model = 'small', activation = 'relu'
normalization = 'LayerNorm', return_bias = False, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[-4.0903e-02, -2.5958e-02, -4.5321e-02,  ...,  9.8564e-02,
          -6.1800e-02,  1.4865e-01]],

        [[-...    [[-9.8368e-02, -1.2242e-01,  4.9636e-02,  ..., -2.8892e-02,
           5.7595e-02,  5.6001e-02]]], device='cuda:0')
l2 = tensor([[[-4.0842e-02, -2.5950e-02, -4.5320e-02,  ...,  9.8524e-02,
          -6.1896e-02,  1.4863e-01]],

        [[-...    [[-9.8295e-02, -1.1865e-01,  4.1502e-02,  ..., -2.8785e-02,
           5.7626e-02,  5.6190e-02]]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=14. Maximum difference at location [0, 99] with -0.07819170504808426 vs -0.10008896142244339 (diff 0.02189725637435913).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_mlp_accuracy[True-False-LayerNorm-relu-small-2-dtype0] _____

dtype = torch.float32, bs = 2, model = 'small', activation = 'relu'
normalization = 'LayerNorm', return_bias = False, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[-4.0903e-02, -2.5958e-02, -4.5321e-02,  ...,  9.8564e-02,
          -6.1800e-02,  1.4865e-01],
         [-1....     [ 3.5071e-02, -1.3502e-01, -1.3495e-01,  ...,  7.4266e-02,
           7.5113e-02,  8.7058e-03]]], device='cuda:0')
l2 = tensor([[[-4.0842e-02, -2.5950e-02, -4.5320e-02,  ...,  9.8524e-02,
          -6.1896e-02,  1.4863e-01],
         [-1....     [ 3.5103e-02, -1.3504e-01, -1.3494e-01,  ...,  7.4227e-02,
           7.5035e-02,  8.6968e-03]]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=7. Maximum difference at location [0, 99] with -0.07819170504808426 vs -0.10008896142244339 (diff 0.02189725637435913).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_mlp_accuracy[True-False-LayerNorm-reglu-small-1-dtype0] ____

dtype = torch.float32, bs = 1, model = 'small', activation = 'reglu'
normalization = 'LayerNorm', return_bias = False, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[-4.8469e-01,  3.4499e-01, -9.7453e-04,  ..., -1.7425e-01,
          3.4103e-01, -1.4285e-01],
        [ 1.329...       [-1.7937e-02, -1.6914e-01, -1.8572e-01,  ...,  2.0881e-01,
          2.1179e-01, -4.2140e-02]], device='cuda:0')
l2 = tensor([[-4.8448e-01,  3.4507e-01, -1.0844e-03,  ..., -1.7429e-01,
          3.4099e-01, -1.4295e-01],
        [ 1.340...       [-1.7905e-02, -1.6928e-01, -1.8583e-01,  ...,  2.0898e-01,
          2.1187e-01, -4.2232e-02]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=105. Maximum difference at location [75] with -0.0472244955599308 vs -0.014243432320654392 (diff 0.032981064170598984).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_mlp_accuracy[True-False-LayerNorm-reglu-small-2-dtype0] ____

dtype = torch.float32, bs = 2, model = 'small', activation = 'reglu'
normalization = 'LayerNorm', return_bias = False, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([ 1.8349,  1.4578,  0.5213, -1.0315, -1.6942,  2.4269,  0.4855,  0.3110,
         0.1912, -0.6689, -0.5850, -1....1135,  0.1760,
        -1.1478, -0.6440, -1.3703, -0.4670,  1.5899, -0.5406,  0.6753,  0.2962],
       device='cuda:0')
l2 = tensor([ 1.8335,  1.4575,  0.5247, -1.0402, -1.6862,  2.4275,  0.4863,  0.3120,
         0.1898, -0.6858, -0.5879, -1....1102,  0.1732,
        -1.1513, -0.6487, -1.3754, -0.4696,  1.5853, -0.5460,  0.6661,  0.2978],
       device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=22. Maximum difference at location [0] with -1.3210716247558594 vs -1.299375057220459 (diff 0.02169656753540039).

tests/pytorch/test_numerics.py:218: AssertionError
_____ test_layernorm_mlp_accuracy[True-False-RMSNorm-relu-small-1-dtype0] ______

dtype = torch.float32, bs = 1, model = 'small', activation = 'relu'
normalization = 'RMSNorm', return_bias = False, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[-3.2913e+00, -1.5585e+00,  7.6284e-02,  ...,  4.2568e-01,
         -1.9121e-01,  1.5443e+00],
        [-5.961...       [-2.1555e-02, -2.8658e-01, -6.1978e-01,  ...,  2.0026e-01,
          5.6838e-01, -2.2261e-01]], device='cuda:0')
l2 = tensor([[-3.2907e+00, -1.5584e+00,  7.6177e-02,  ...,  4.2493e-01,
         -1.9130e-01,  1.5448e+00],
        [-6.013...       [-2.1587e-02, -2.8688e-01, -6.2038e-01,  ...,  2.0036e-01,
          5.6897e-01, -2.2260e-01]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=50. Maximum difference at location [100] with 0.18327626585960388 vs 0.3811267912387848 (diff 0.1978505253791809).

tests/pytorch/test_numerics.py:218: AssertionError
_____ test_layernorm_mlp_accuracy[True-False-RMSNorm-relu-small-2-dtype0] ______

dtype = torch.float32, bs = 2, model = 'small', activation = 'relu'
normalization = 'RMSNorm', return_bias = False, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[-4.6028e-02, -3.2271e-02, -5.2628e-02,  ...,  1.0047e-01,
          -7.6332e-02,  1.2294e-01],
         [-1....     [ 2.8777e-02, -1.6675e-01, -1.7494e-01,  ...,  7.0587e-02,
           7.1250e-02, -9.0677e-03]]], device='cuda:0')
l2 = tensor([[[-4.5971e-02, -3.2265e-02, -5.2626e-02,  ...,  1.0042e-01,
          -7.6427e-02,  1.2293e-01],
         [-1....     [ 2.8811e-02, -1.6676e-01, -1.7492e-01,  ...,  7.0552e-02,
           7.1163e-02, -9.0771e-03]]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=121. Maximum difference at location [1, 15] with -0.007176937535405159 vs 0.015477740205824375 (diff 0.02265467867255211).

tests/pytorch/test_numerics.py:218: AssertionError
_____ test_layernorm_mlp_accuracy[True-False-RMSNorm-reglu-small-1-dtype0] _____

dtype = torch.float32, bs = 1, model = 'small', activation = 'reglu'
normalization = 'RMSNorm', return_bias = False, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[-3.4965e-01,  3.6868e-01,  6.6983e-02,  ..., -1.7441e-01,
          5.1494e-01, -1.2830e-01],
        [ 1.426...       [-1.9212e-02, -1.6958e-01, -1.8650e-01,  ...,  2.0023e-01,
          2.0466e-01, -5.4184e-02]], device='cuda:0')
l2 = tensor([[-3.4950e-01,  3.6870e-01,  6.6943e-02,  ..., -1.7423e-01,
          5.1493e-01, -1.2815e-01],
        [ 1.439...       [-1.9107e-02, -1.6978e-01, -1.8666e-01,  ...,  2.0039e-01,
          2.0480e-01, -5.4196e-02]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=351. Maximum difference at location [77] with -0.04859238490462303 vs -0.07616263628005981 (diff 0.027570251375436783).

tests/pytorch/test_numerics.py:218: AssertionError
_____ test_layernorm_mlp_accuracy[True-False-RMSNorm-reglu-small-2-dtype0] _____

dtype = torch.float32, bs = 2, model = 'small', activation = 'reglu'
normalization = 'RMSNorm', return_bias = False, bias = True

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[-7.1748e-01,  3.0336e-01, -1.6657e-01,  ..., -5.6715e-01,
          8.0463e-01, -5.2289e-02],
        [-5.886...       [-9.1481e-02, -2.4700e-01, -2.0221e-01,  ...,  2.6127e-01,
          5.9658e-02, -2.6389e-01]], device='cuda:0')
l2 = tensor([[-7.1713e-01,  3.0324e-01, -1.6643e-01,  ..., -5.6724e-01,
          8.0480e-01, -5.2136e-02],
        [-5.936...       [-9.1421e-02, -2.4719e-01, -2.0242e-01,  ...,  2.6152e-01,
          5.9713e-02, -2.6396e-01]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=263. Maximum difference at location [56] with 0.03622262179851532 vs 0.12370995432138443 (diff 0.08748733252286911).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_mlp_accuracy[False-True-LayerNorm-relu-small-1-dtype0] _____

dtype = torch.float32, bs = 1, model = 'small', activation = 'relu'
normalization = 'LayerNorm', return_bias = True, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[-4.0903e-02, -2.5958e-02, -4.5321e-02,  ...,  9.8564e-02,
          -6.1800e-02,  1.4865e-01]],

        [[-...    [[-9.8368e-02, -1.2242e-01,  4.9636e-02,  ..., -2.8892e-02,
           5.7595e-02,  5.6001e-02]]], device='cuda:0')
l2 = tensor([[[-4.0842e-02, -2.5950e-02, -4.5320e-02,  ...,  9.8524e-02,
          -6.1896e-02,  1.4863e-01]],

        [[-...    [[-9.8295e-02, -1.1865e-01,  4.1502e-02,  ..., -2.8785e-02,
           5.7626e-02,  5.6190e-02]]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=14. Maximum difference at location [0, 99] with -0.07819170504808426 vs -0.10008896142244339 (diff 0.02189725637435913).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_mlp_accuracy[False-True-LayerNorm-relu-small-2-dtype0] _____

dtype = torch.float32, bs = 2, model = 'small', activation = 'relu'
normalization = 'LayerNorm', return_bias = True, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[-4.0903e-02, -2.5958e-02, -4.5321e-02,  ...,  9.8564e-02,
          -6.1800e-02,  1.4865e-01],
         [-1....     [ 3.5071e-02, -1.3502e-01, -1.3495e-01,  ...,  7.4266e-02,
           7.5113e-02,  8.7058e-03]]], device='cuda:0')
l2 = tensor([[[-4.0842e-02, -2.5950e-02, -4.5320e-02,  ...,  9.8524e-02,
          -6.1896e-02,  1.4863e-01],
         [-1....     [ 3.5103e-02, -1.3504e-01, -1.3494e-01,  ...,  7.4227e-02,
           7.5035e-02,  8.6968e-03]]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=7. Maximum difference at location [0, 99] with -0.07819170504808426 vs -0.10008896142244339 (diff 0.02189725637435913).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_mlp_accuracy[False-True-LayerNorm-reglu-small-1-dtype0] ____

dtype = torch.float32, bs = 1, model = 'small', activation = 'reglu'
normalization = 'LayerNorm', return_bias = True, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[-4.8469e-01,  3.4499e-01, -9.7453e-04,  ..., -1.7425e-01,
          3.4103e-01, -1.4285e-01],
        [ 1.329...       [-1.7937e-02, -1.6914e-01, -1.8572e-01,  ...,  2.0881e-01,
          2.1179e-01, -4.2140e-02]], device='cuda:0')
l2 = tensor([[-4.8448e-01,  3.4507e-01, -1.0844e-03,  ..., -1.7429e-01,
          3.4099e-01, -1.4295e-01],
        [ 1.340...       [-1.7905e-02, -1.6928e-01, -1.8583e-01,  ...,  2.0898e-01,
          2.1187e-01, -4.2232e-02]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=105. Maximum difference at location [75] with -0.0472244955599308 vs -0.014243432320654392 (diff 0.032981064170598984).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_mlp_accuracy[False-True-LayerNorm-reglu-small-2-dtype0] ____

dtype = torch.float32, bs = 2, model = 'small', activation = 'reglu'
normalization = 'LayerNorm', return_bias = True, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([ 1.8349,  1.4578,  0.5213, -1.0315, -1.6942,  2.4269,  0.4855,  0.3110,
         0.1912, -0.6689, -0.5850, -1....1135,  0.1760,
        -1.1478, -0.6440, -1.3703, -0.4670,  1.5899, -0.5406,  0.6753,  0.2962],
       device='cuda:0')
l2 = tensor([ 1.8335,  1.4575,  0.5247, -1.0402, -1.6862,  2.4275,  0.4863,  0.3120,
         0.1898, -0.6858, -0.5879, -1....1102,  0.1732,
        -1.1513, -0.6487, -1.3754, -0.4696,  1.5853, -0.5460,  0.6661,  0.2978],
       device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=22. Maximum difference at location [0] with -1.3210716247558594 vs -1.299375057220459 (diff 0.02169656753540039).

tests/pytorch/test_numerics.py:218: AssertionError
_____ test_layernorm_mlp_accuracy[False-True-RMSNorm-relu-small-1-dtype0] ______

dtype = torch.float32, bs = 1, model = 'small', activation = 'relu'
normalization = 'RMSNorm', return_bias = True, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[-3.2913e+00, -1.5585e+00,  7.6284e-02,  ...,  4.2568e-01,
         -1.9121e-01,  1.5443e+00],
        [-5.961...       [-2.1555e-02, -2.8658e-01, -6.1978e-01,  ...,  2.0026e-01,
          5.6838e-01, -2.2261e-01]], device='cuda:0')
l2 = tensor([[-3.2907e+00, -1.5584e+00,  7.6177e-02,  ...,  4.2493e-01,
         -1.9130e-01,  1.5448e+00],
        [-6.013...       [-2.1587e-02, -2.8688e-01, -6.2038e-01,  ...,  2.0036e-01,
          5.6897e-01, -2.2260e-01]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=50. Maximum difference at location [100] with 0.18327626585960388 vs 0.3811267912387848 (diff 0.1978505253791809).

tests/pytorch/test_numerics.py:218: AssertionError
_____ test_layernorm_mlp_accuracy[False-True-RMSNorm-relu-small-2-dtype0] ______

dtype = torch.float32, bs = 2, model = 'small', activation = 'relu'
normalization = 'RMSNorm', return_bias = True, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[-4.6028e-02, -3.2271e-02, -5.2628e-02,  ...,  1.0047e-01,
          -7.6332e-02,  1.2294e-01],
         [-1....     [ 2.8777e-02, -1.6675e-01, -1.7494e-01,  ...,  7.0587e-02,
           7.1250e-02, -9.0677e-03]]], device='cuda:0')
l2 = tensor([[[-4.5971e-02, -3.2265e-02, -5.2626e-02,  ...,  1.0042e-01,
          -7.6427e-02,  1.2293e-01],
         [-1....     [ 2.8811e-02, -1.6676e-01, -1.7492e-01,  ...,  7.0552e-02,
           7.1163e-02, -9.0771e-03]]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=121. Maximum difference at location [1, 15] with -0.007176937535405159 vs 0.015477740205824375 (diff 0.02265467867255211).

tests/pytorch/test_numerics.py:218: AssertionError
_____ test_layernorm_mlp_accuracy[False-True-RMSNorm-reglu-small-1-dtype0] _____

dtype = torch.float32, bs = 1, model = 'small', activation = 'reglu'
normalization = 'RMSNorm', return_bias = True, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[-3.4965e-01,  3.6868e-01,  6.6983e-02,  ..., -1.7441e-01,
          5.1494e-01, -1.2830e-01],
        [ 1.426...       [-1.9212e-02, -1.6958e-01, -1.8650e-01,  ...,  2.0023e-01,
          2.0466e-01, -5.4184e-02]], device='cuda:0')
l2 = tensor([[-3.4950e-01,  3.6870e-01,  6.6943e-02,  ..., -1.7423e-01,
          5.1493e-01, -1.2815e-01],
        [ 1.439...       [-1.9107e-02, -1.6978e-01, -1.8666e-01,  ...,  2.0039e-01,
          2.0480e-01, -5.4196e-02]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=351. Maximum difference at location [77] with -0.04859238490462303 vs -0.07616263628005981 (diff 0.027570251375436783).

tests/pytorch/test_numerics.py:218: AssertionError
_____ test_layernorm_mlp_accuracy[False-True-RMSNorm-reglu-small-2-dtype0] _____

dtype = torch.float32, bs = 2, model = 'small', activation = 'reglu'
normalization = 'RMSNorm', return_bias = True, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[-7.1748e-01,  3.0336e-01, -1.6657e-01,  ..., -5.6715e-01,
          8.0463e-01, -5.2289e-02],
        [-5.886...       [-9.1481e-02, -2.4700e-01, -2.0221e-01,  ...,  2.6127e-01,
          5.9658e-02, -2.6389e-01]], device='cuda:0')
l2 = tensor([[-7.1713e-01,  3.0324e-01, -1.6643e-01,  ..., -5.6724e-01,
          8.0480e-01, -5.2136e-02],
        [-5.936...       [-9.1421e-02, -2.4719e-01, -2.0242e-01,  ...,  2.6152e-01,
          5.9713e-02, -2.6396e-01]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=263. Maximum difference at location [56] with 0.03622262179851532 vs 0.12370995432138443 (diff 0.08748733252286911).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_mlp_accuracy[False-False-LayerNorm-relu-small-1-dtype0] ____

dtype = torch.float32, bs = 1, model = 'small', activation = 'relu'
normalization = 'LayerNorm', return_bias = False, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[-4.0903e-02, -2.5958e-02, -4.5321e-02,  ...,  9.8564e-02,
          -6.1800e-02,  1.4865e-01]],

        [[-...    [[-9.8368e-02, -1.2242e-01,  4.9636e-02,  ..., -2.8892e-02,
           5.7595e-02,  5.6001e-02]]], device='cuda:0')
l2 = tensor([[[-4.0842e-02, -2.5950e-02, -4.5320e-02,  ...,  9.8524e-02,
          -6.1896e-02,  1.4863e-01]],

        [[-...    [[-9.8295e-02, -1.1865e-01,  4.1502e-02,  ..., -2.8785e-02,
           5.7626e-02,  5.6190e-02]]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=14. Maximum difference at location [0, 99] with -0.07819170504808426 vs -0.10008896142244339 (diff 0.02189725637435913).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_mlp_accuracy[False-False-LayerNorm-relu-small-2-dtype0] ____

dtype = torch.float32, bs = 2, model = 'small', activation = 'relu'
normalization = 'LayerNorm', return_bias = False, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[-4.0903e-02, -2.5958e-02, -4.5321e-02,  ...,  9.8564e-02,
          -6.1800e-02,  1.4865e-01],
         [-1....     [ 3.5071e-02, -1.3502e-01, -1.3495e-01,  ...,  7.4266e-02,
           7.5113e-02,  8.7058e-03]]], device='cuda:0')
l2 = tensor([[[-4.0842e-02, -2.5950e-02, -4.5320e-02,  ...,  9.8524e-02,
          -6.1896e-02,  1.4863e-01],
         [-1....     [ 3.5103e-02, -1.3504e-01, -1.3494e-01,  ...,  7.4227e-02,
           7.5035e-02,  8.6968e-03]]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=7. Maximum difference at location [0, 99] with -0.07819170504808426 vs -0.10008896142244339 (diff 0.02189725637435913).

tests/pytorch/test_numerics.py:218: AssertionError
___ test_layernorm_mlp_accuracy[False-False-LayerNorm-reglu-small-1-dtype0] ____

dtype = torch.float32, bs = 1, model = 'small', activation = 'reglu'
normalization = 'LayerNorm', return_bias = False, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[-4.8469e-01,  3.4499e-01, -9.7453e-04,  ..., -1.7425e-01,
          3.4103e-01, -1.4285e-01],
        [ 1.329...       [-1.7937e-02, -1.6914e-01, -1.8572e-01,  ...,  2.0881e-01,
          2.1179e-01, -4.2140e-02]], device='cuda:0')
l2 = tensor([[-4.8448e-01,  3.4507e-01, -1.0844e-03,  ..., -1.7429e-01,
          3.4099e-01, -1.4295e-01],
        [ 1.340...       [-1.7905e-02, -1.6928e-01, -1.8583e-01,  ...,  2.0898e-01,
          2.1187e-01, -4.2232e-02]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=105. Maximum difference at location [75] with -0.0472244955599308 vs -0.014243432320654392 (diff 0.032981064170598984).

tests/pytorch/test_numerics.py:218: AssertionError
___ test_layernorm_mlp_accuracy[False-False-LayerNorm-reglu-small-2-dtype0] ____

dtype = torch.float32, bs = 2, model = 'small', activation = 'reglu'
normalization = 'LayerNorm', return_bias = False, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([ 1.8349,  1.4578,  0.5213, -1.0315, -1.6942,  2.4269,  0.4855,  0.3110,
         0.1912, -0.6689, -0.5850, -1....1135,  0.1760,
        -1.1478, -0.6440, -1.3703, -0.4670,  1.5899, -0.5406,  0.6753,  0.2962],
       device='cuda:0')
l2 = tensor([ 1.8335,  1.4575,  0.5247, -1.0402, -1.6862,  2.4275,  0.4863,  0.3120,
         0.1898, -0.6858, -0.5879, -1....1102,  0.1732,
        -1.1513, -0.6487, -1.3754, -0.4696,  1.5853, -0.5460,  0.6661,  0.2978],
       device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=22. Maximum difference at location [0] with -1.3210716247558594 vs -1.299375057220459 (diff 0.02169656753540039).

tests/pytorch/test_numerics.py:218: AssertionError
_____ test_layernorm_mlp_accuracy[False-False-RMSNorm-relu-small-1-dtype0] _____

dtype = torch.float32, bs = 1, model = 'small', activation = 'relu'
normalization = 'RMSNorm', return_bias = False, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[-3.2913e+00, -1.5585e+00,  7.6284e-02,  ...,  4.2568e-01,
         -1.9121e-01,  1.5443e+00],
        [-5.961...       [-2.1555e-02, -2.8658e-01, -6.1978e-01,  ...,  2.0026e-01,
          5.6838e-01, -2.2261e-01]], device='cuda:0')
l2 = tensor([[-3.2907e+00, -1.5584e+00,  7.6177e-02,  ...,  4.2493e-01,
         -1.9130e-01,  1.5448e+00],
        [-6.013...       [-2.1587e-02, -2.8688e-01, -6.2038e-01,  ...,  2.0036e-01,
          5.6897e-01, -2.2260e-01]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=50. Maximum difference at location [100] with 0.18327626585960388 vs 0.3811267912387848 (diff 0.1978505253791809).

tests/pytorch/test_numerics.py:218: AssertionError
_____ test_layernorm_mlp_accuracy[False-False-RMSNorm-relu-small-2-dtype0] _____

dtype = torch.float32, bs = 2, model = 'small', activation = 'relu'
normalization = 'RMSNorm', return_bias = False, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[[-4.6028e-02, -3.2271e-02, -5.2628e-02,  ...,  1.0047e-01,
          -7.6332e-02,  1.2294e-01],
         [-1....     [ 2.8777e-02, -1.6675e-01, -1.7494e-01,  ...,  7.0587e-02,
           7.1250e-02, -9.0677e-03]]], device='cuda:0')
l2 = tensor([[[-4.5971e-02, -3.2265e-02, -5.2626e-02,  ...,  1.0042e-01,
          -7.6427e-02,  1.2293e-01],
         [-1....     [ 2.8811e-02, -1.6676e-01, -1.7492e-01,  ...,  7.0552e-02,
           7.1163e-02, -9.0771e-03]]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=121. Maximum difference at location [1, 15] with -0.007176937535405159 vs 0.015477740205824375 (diff 0.02265467867255211).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_mlp_accuracy[False-False-RMSNorm-reglu-small-1-dtype0] _____

dtype = torch.float32, bs = 1, model = 'small', activation = 'reglu'
normalization = 'RMSNorm', return_bias = False, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[-3.4965e-01,  3.6868e-01,  6.6983e-02,  ..., -1.7441e-01,
          5.1494e-01, -1.2830e-01],
        [ 1.426...       [-1.9212e-02, -1.6958e-01, -1.8650e-01,  ...,  2.0023e-01,
          2.0466e-01, -5.4184e-02]], device='cuda:0')
l2 = tensor([[-3.4950e-01,  3.6870e-01,  6.6943e-02,  ..., -1.7423e-01,
          5.1493e-01, -1.2815e-01],
        [ 1.439...       [-1.9107e-02, -1.6978e-01, -1.8666e-01,  ...,  2.0039e-01,
          2.0480e-01, -5.4196e-02]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=351. Maximum difference at location [77] with -0.04859238490462303 vs -0.07616263628005981 (diff 0.027570251375436783).

tests/pytorch/test_numerics.py:218: AssertionError
____ test_layernorm_mlp_accuracy[False-False-RMSNorm-reglu-small-2-dtype0] _____

dtype = torch.float32, bs = 2, model = 'small', activation = 'reglu'
normalization = 'RMSNorm', return_bias = False, bias = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("bs", batch_sizes)
    @pytest.mark.parametrize("model", ["small"])
    @pytest.mark.parametrize("activation", all_activations)
    @pytest.mark.parametrize("normalization", all_normalizations)
    @pytest.mark.parametrize("return_bias", all_boolean)
    @pytest.mark.parametrize("bias", all_boolean)
    def test_layernorm_mlp_accuracy(dtype, bs, model, activation, normalization, return_bias, bias):
        config = model_configs[model]
    
        te_ln_mlp = TestReturnBiasModule(
            LayerNormMLP,
            hidden_size=config.hidden_size,
            ffn_hidden_size=4 * config.hidden_size,
            activation=activation,
            normalization=normalization,
            params_dtype=dtype,
            return_bias=return_bias,
            bias=bias,
            device="cuda",
        )
    
        torch_ln_mlp = (
            TorchLayerNormMLP(
                config.hidden_size,
                4 * config.hidden_size,
                activation=activation,
                normalization=normalization,
                bias=bias,
            )
            .to(dtype=dtype)
            .cuda()
        )
    
        # Share params
        with torch.no_grad():
            torch_ln_mlp.ln.weight = Parameter(te_ln_mlp.te_module.layer_norm_weight.clone())
            if normalization != "RMSNorm":
                torch_ln_mlp.ln.bias = Parameter(te_ln_mlp.te_module.layer_norm_bias.clone())
            torch_ln_mlp.fc1.weight = Parameter(te_ln_mlp.te_module.fc1_weight.clone())
            torch_ln_mlp.fc2.weight = Parameter(te_ln_mlp.te_module.fc2_weight.clone())
            if bias:
                torch_ln_mlp.fc1.bias = Parameter(te_ln_mlp.te_module.fc1_bias.clone())
                torch_ln_mlp.fc2.bias = Parameter(te_ln_mlp.te_module.fc2_bias.clone())
    
        te_outputs = _test_granular_accuracy(te_ln_mlp, bs, dtype, config)
        torch_outputs = _test_granular_accuracy(torch_ln_mlp, bs, dtype, config)
    
        atol = {
            torch.float32: 2e-2,
            torch.half: 5e-2,
            torch.bfloat16: 5e-2,
        }
    
        rtol = {
            torch.float32: 1e-3,
            torch.half: 4e-2,
            torch.bfloat16: 4e-2,
        }
    
        # Check output.
        assert_allclose(te_outputs[0], torch_outputs[0], atol[dtype], rtol[dtype])
    
        # Check gradients, only for small model
        rtol = {
            torch.float32: 1e-3,
            torch.half: 1e-2,
            torch.bfloat16: 4e-2,
        }
        atol[torch.half] = 2e-1
        atol[torch.bfloat16] = 2e-1
        if model == "small":
            for te_output, torch_output in zip(te_outputs[1:], torch_outputs[1:]):
>               assert_allclose(te_output, torch_output, atol[dtype], rtol[dtype])

tests/pytorch/test_numerics.py:1690: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

l1 = tensor([[-7.1748e-01,  3.0336e-01, -1.6657e-01,  ..., -5.6715e-01,
          8.0463e-01, -5.2289e-02],
        [-5.886...       [-9.1481e-02, -2.4700e-01, -2.0221e-01,  ...,  2.6127e-01,
          5.9658e-02, -2.6389e-01]], device='cuda:0')
l2 = tensor([[-7.1713e-01,  3.0324e-01, -1.6643e-01,  ..., -5.6724e-01,
          8.0480e-01, -5.2136e-02],
        [-5.936...       [-9.1421e-02, -2.4719e-01, -2.0242e-01,  ...,  2.6152e-01,
          5.9713e-02, -2.6396e-01]], device='cuda:0')
atol = 0.02, rtol = 0.001

    def assert_allclose(
        l1: List[torch.Tensor], l2: List[torch.Tensor], atol: float = None, rtol: float = None
    ) -> bool:
        """Ensures two lists are equal."""
        assert len(l1) == len(l2), "Unequal number of outputs."
        for i, (t1, t2) in enumerate(zip(l1, l2)):
            tols = dtype_tols(t2.dtype)
            if rtol is not None:
                tols["rtol"] = rtol
            if atol is not None:
                tols["atol"] = atol
            result = torch.allclose(t1, t2, **tols)
            if not result:
                diff = torch.abs(t1 - t2)
                tol = tols["atol"] + (tols["rtol"] * torch.abs(t2))
                exceed_mask = diff > tol
                if exceed_mask.any():
                    indices = torch.nonzero(exceed_mask, as_tuple=True)
                    max_diff = diff[exceed_mask].max()
                    max_idx = (diff[exceed_mask] == max_diff).nonzero(as_tuple=True)[0][0]
                    max_location = [idx[max_idx].item() for idx in indices]
                    msg = (
                        f"Outputs not close enough in tensor at idx={i}. "
                        f"Maximum difference at location {max_location} "
                        f"with {t1[exceed_mask][max_idx].item()} vs {t2[exceed_mask][max_idx].item()} "
                        f"(diff {max_diff.item()})."
                    )
>               raise AssertionError(msg)
E               AssertionError: Outputs not close enough in tensor at idx=263. Maximum difference at location [56] with 0.03622262179851532 vs 0.12370995432138443 (diff 0.08748733252286911).

tests/pytorch/test_numerics.py:218: AssertionError
=============================== warnings summary ===============================
tests/pytorch/test_numerics.py:1201
  /root/TransformerEngine/tests/pytorch/test_numerics.py:1201: PytestCollectionWarning: cannot collect test class 'TestReturnBiasModule' because it has a __init__ constructor (from: tests/pytorch/test_numerics.py)
    class TestReturnBiasModule(nn.Module):

tests/pytorch/test_numerics.py::test_mha_accuracy[no_mask-small-1-dtype0]
tests/pytorch/test_numerics.py::test_mha_accuracy[no_mask-small-1-dtype1]
tests/pytorch/test_numerics.py::test_mha_accuracy[no_mask-small-1-dtype2]
tests/pytorch/test_numerics.py::test_mha_accuracy[no_mask-small-2-dtype0]
tests/pytorch/test_numerics.py::test_mha_accuracy[no_mask-small-2-dtype1]
tests/pytorch/test_numerics.py::test_mha_accuracy[no_mask-small-2-dtype2]
  /root/TransformerEngine/transformer_engine/pytorch/attention/dot_product_attention/utils.py:2070: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask
    warnings.warn(

tests/pytorch/test_numerics.py::test_noncontiguous
  /root/TransformerEngine/tests/pytorch/test_numerics.py:2875: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)
    if inp.grad is not None:

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_numerics.xml -
=========================== short test summary info ============================
FAILED tests/pytorch/test_numerics.py::test_gpt_accuracy[True-small-1-dtype1]
FAILED tests/pytorch/test_numerics.py::test_gpt_accuracy[True-small-1-dtype2]
FAILED tests/pytorch/test_numerics.py::test_gpt_accuracy[True-small-2-dtype1]
FAILED tests/pytorch/test_numerics.py::test_gpt_accuracy[True-small-2-dtype2]
FAILED tests/pytorch/test_numerics.py::test_gpt_accuracy[False-small-1-dtype1]
FAILED tests/pytorch/test_numerics.py::test_gpt_accuracy[False-small-1-dtype2]
FAILED tests/pytorch/test_numerics.py::test_gpt_accuracy[False-small-2-dtype1]
FAILED tests/pytorch/test_numerics.py::test_gpt_accuracy[False-small-2-dtype2]
FAILED tests/pytorch/test_numerics.py::test_mha_accuracy[causal-small-1-dtype1]
FAILED tests/pytorch/test_numerics.py::test_mha_accuracy[causal-small-1-dtype2]
FAILED tests/pytorch/test_numerics.py::test_mha_accuracy[causal-small-2-dtype1]
FAILED tests/pytorch/test_numerics.py::test_mha_accuracy[causal-small-2-dtype2]
FAILED tests/pytorch/test_numerics.py::test_mha_accuracy[no_mask-small-1-dtype1]
FAILED tests/pytorch/test_numerics.py::test_mha_accuracy[no_mask-small-1-dtype2]
FAILED tests/pytorch/test_numerics.py::test_mha_accuracy[no_mask-small-2-dtype1]
FAILED tests/pytorch/test_numerics.py::test_mha_accuracy[no_mask-small-2-dtype2]
FAILED tests/pytorch/test_numerics.py::test_linear_accuracy[True-True-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_linear_accuracy[True-True-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_linear_accuracy[True-False-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_linear_accuracy[True-False-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_linear_accuracy[False-True-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_linear_accuracy[False-True-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_linear_accuracy[False-False-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_linear_accuracy[False-False-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[True-True-True-LayerNorm-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[True-True-True-LayerNorm-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[True-True-True-RMSNorm-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[True-True-True-RMSNorm-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[True-True-False-LayerNorm-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[True-True-False-LayerNorm-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[True-True-False-RMSNorm-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[True-True-False-RMSNorm-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[True-False-True-LayerNorm-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[True-False-True-LayerNorm-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[True-False-True-RMSNorm-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[True-False-True-RMSNorm-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[True-False-False-LayerNorm-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[True-False-False-LayerNorm-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[True-False-False-RMSNorm-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[True-False-False-RMSNorm-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[False-True-True-LayerNorm-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[False-True-True-LayerNorm-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[False-True-True-RMSNorm-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[False-True-True-RMSNorm-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[False-True-False-LayerNorm-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[False-True-False-LayerNorm-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[False-True-False-RMSNorm-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[False-True-False-RMSNorm-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[False-False-True-LayerNorm-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[False-False-True-LayerNorm-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[False-False-True-RMSNorm-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[False-False-True-RMSNorm-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[False-False-False-LayerNorm-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[False-False-False-LayerNorm-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[False-False-False-RMSNorm-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_linear_accuracy[False-False-False-RMSNorm-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[True-True-LayerNorm-relu-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[True-True-LayerNorm-relu-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[True-True-LayerNorm-reglu-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[True-True-LayerNorm-reglu-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[True-True-RMSNorm-relu-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[True-True-RMSNorm-relu-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[True-True-RMSNorm-reglu-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[True-True-RMSNorm-reglu-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[True-False-LayerNorm-relu-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[True-False-LayerNorm-relu-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[True-False-LayerNorm-reglu-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[True-False-LayerNorm-reglu-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[True-False-RMSNorm-relu-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[True-False-RMSNorm-relu-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[True-False-RMSNorm-reglu-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[True-False-RMSNorm-reglu-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[False-True-LayerNorm-relu-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[False-True-LayerNorm-relu-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[False-True-LayerNorm-reglu-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[False-True-LayerNorm-reglu-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[False-True-RMSNorm-relu-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[False-True-RMSNorm-relu-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[False-True-RMSNorm-reglu-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[False-True-RMSNorm-reglu-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[False-False-LayerNorm-relu-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[False-False-LayerNorm-relu-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[False-False-LayerNorm-reglu-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[False-False-LayerNorm-reglu-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[False-False-RMSNorm-relu-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[False-False-RMSNorm-relu-small-2-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[False-False-RMSNorm-reglu-small-1-dtype0]
FAILED tests/pytorch/test_numerics.py::test_layernorm_mlp_accuracy[False-False-RMSNorm-reglu-small-2-dtype0]
===== 88 failed, 2410 passed, 236 skipped, 8 warnings in 113.42s (0:01:53) =====
+ test_fail test_numerics.py
+ RET=1
+ FAILED_CASES=' test_numerics.py'
+ echo 'Error: sub-test failed: test_numerics.py'
Error: sub-test failed: test_numerics.py
+ PYTORCH_JIT=0
+ NVTE_TORCH_COMPILE=0
+ NVTE_ALLOW_NONDETERMINISTIC_ALGO=0
+ NVTE_FUSED_ATTN=0
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_cuda_graphs.xml /root/TransformerEngine/tests/pytorch/test_cuda_graphs.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 371 items

tests/pytorch/test_cuda_graphs.py ....................................ss [ 10%]
ssssssssss.....sssssssssssssssssss.....s.....s.....s.....s.....s.....s.. [ 29%]
........................................................................ [ 49%]
................ssssssssssssssssss..............................ssssssss [ 68%]
ss.....sssssssssssssss.................................................. [ 87%]
.............................................                            [100%]

=============================== warnings summary ===============================
tests/pytorch/test_cuda_graphs.py::test_make_graphed_callables[MXFP8BlockScaling-True-dtype0-linear_op]
tests/pytorch/test_cuda_graphs.py::test_make_graphed_callables[MXFP8BlockScaling-True-dtype1-linear_op]
tests/pytorch/test_cuda_graphs.py::test_make_graphed_callables[MXFP8BlockScaling-True-dtype2-linear_op]
tests/pytorch/test_cuda_graphs.py::test_make_graphed_callables[Float8CurrentScaling-True-dtype0-linear_op]
tests/pytorch/test_cuda_graphs.py::test_make_graphed_callables[Float8CurrentScaling-True-dtype1-linear_op]
tests/pytorch/test_cuda_graphs.py::test_make_graphed_callables[Float8CurrentScaling-True-dtype2-linear_op]
tests/pytorch/test_cuda_graphs.py::test_make_graphed_callables[DelayedScaling-True-dtype0-linear_op]
tests/pytorch/test_cuda_graphs.py::test_make_graphed_callables[DelayedScaling-True-dtype1-linear_op]
tests/pytorch/test_cuda_graphs.py::test_make_graphed_callables[DelayedScaling-True-dtype2-linear_op]
  /root/TransformerEngine/transformer_engine/pytorch/quantized_tensor.py:120: UserWarning: Quantizer is being updated, this may affect model behavior
    warnings.warn("Quantizer is being updated, this may affect model behavior")

tests/pytorch/test_cuda_graphs.py: 66 warnings
  /root/TransformerEngine/transformer_engine/pytorch/tensor/float8_tensor.py:785: UserWarning: A function call(aten.empty_like.default) in <class 'transformer_engine.pytorch.tensor.float8_tensor.Float8Tensor'> may not return <class 'transformer_engine.pytorch.tensor.float8_tensor.Float8Tensor'> tensor as an output. It might cause an error in torch FSDP2!
    warnings.warn(

tests/pytorch/test_cuda_graphs.py::test_make_graphed_callables_with_kwargs
  /root/TransformerEngine/transformer_engine/pytorch/attention/dot_product_attention/utils.py:2070: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=arbitrary
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_cuda_graphs.xml -
================ 291 passed, 80 skipped, 76 warnings in 34.48s =================
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_jit.xml /root/TransformerEngine/tests/pytorch/test_jit.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 8 items

tests/pytorch/test_jit.py ........                                       [100%]

-- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_jit.xml --
============================== 8 passed in 11.45s ==============================
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_fused_rope.xml /root/TransformerEngine/tests/pytorch/test_fused_rope.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 9609 items

tests/pytorch/test_fused_rope.py s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [  0%]
.s.s.s.s.................................................s.s.s.s.s.s.s.s [  1%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s........................................ [  1%]
.........s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s................ [  2%]
.................................s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [  3%]
.s.s.s.s.................................................s.s.s.s.s.s.s.s [  4%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s........................................ [  4%]
.........s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s................ [  5%]
.................................s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [  6%]
.s.s.s.s.................................................s.s.s.s.s.s.s.s [  7%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s........................................ [  7%]
.........s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s................ [  8%]
.................................s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [  9%]
.s.s.s.s.................................................s.s.s.s.s.s.s.s [ 10%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s........................................ [ 10%]
.........s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s................ [ 11%]
.................................s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 12%]
.s.s.s.s.................................................s.s.s.s.s.s.s.s [ 13%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s........................................ [ 13%]
.........s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s................ [ 14%]
.................................s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 15%]
.s.s.s.s.................................................s.s.s.s.s.s.s.s [ 16%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s........................................ [ 16%]
.........s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s................ [ 17%]
.................................s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 18%]
.s.s.s.s.................................................s.s.s.s.s.s.s.s [ 19%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s........................................ [ 19%]
.........s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s................ [ 20%]
.................................s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 21%]
.s.s.s.s.................................................s.s.s.s.s.s.s.s [ 22%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s........................................ [ 22%]
.........s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s................ [ 23%]
.................................s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 24%]
.s.s.s.s.................................................s.s.s.s.s.s.s.s [ 25%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s........................................ [ 25%]
.........s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s................ [ 26%]
.................................s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 27%]
.s.s.s.s.................................................s.s.s.s.s.s.s.s [ 28%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s........................................ [ 28%]
.........s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s................ [ 29%]
.................................s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 30%]
.s.s.s.s.................................................s.s.s.s.s.s.s.s [ 31%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s........................................ [ 31%]
.........s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s................ [ 32%]
.................................s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 33%]
.s.s.s.s.................................................s.s.s.s.s.s.s.s [ 34%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s........................................ [ 34%]
.........s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s................ [ 35%]
.................................s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 36%]
.s.s.s.s.................................................s.s.s.s.s.s.s.s [ 37%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s........................................ [ 37%]
.........s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s................ [ 38%]
.................................s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 39%]
.s.s.s.s.................................................s.s.s.s.s.s.s.s [ 40%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s........................................ [ 40%]
.........s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s................ [ 41%]
.................................s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 42%]
.s.s.s.s.................................................s.s.s.s.s.s.s.s [ 43%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s........................................ [ 43%]
.........s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s................ [ 44%]
.................................s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 45%]
.s.s.s.s.................................................s.s.s.s.s.s.s.s [ 46%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s........................................ [ 46%]
.........s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s................ [ 47%]
........................................................................ [ 48%]
........................................................................ [ 49%]
........................................................................ [ 49%]
........................................................................ [ 50%]
........................................................................ [ 51%]
.................................................................s.s.s.s [ 52%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 52%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.sssssss [ 53%]
sssss............ssssssssssss............ssssssssssss............sssssss [ 54%]
sssss............ssssssssssss............ssssssssssss............s.s.s.s [ 55%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 55%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.sssssss [ 56%]
sssss............ssssssssssss............ssssssssssss............sssssss [ 57%]
sssss............ssssssssssss............ssssssssssss............s.s.s.s [ 58%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 58%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.sssssss [ 59%]
sssss............ssssssssssss............ssssssssssss............sssssss [ 60%]
sssss............ssssssssssss............ssssssssssss............s.s.s.s [ 61%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 61%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.sssssss [ 62%]
sssss............ssssssssssss............ssssssssssss............sssssss [ 63%]
sssss............ssssssssssss............ssssssssssss............s.s.s.s [ 64%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 64%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.sssssss [ 65%]
ssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssss [ 66%]
ssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.s.s.s.s [ 67%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 67%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.sssssss [ 68%]
ssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssss [ 69%]
ssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.s.s.s.s [ 70%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 70%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.sssssss [ 71%]
ssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssss [ 72%]
ssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.s.s.s.s [ 73%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 73%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.sssssss [ 74%]
ssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssss [ 75%]
ssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.s.s.s.s [ 76%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 76%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.sssssss [ 77%]
sssss............ssssssssssss............ssssssssssss............sssssss [ 78%]
sssss............ssssssssssss............ssssssssssss............s.s.s.s [ 79%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 79%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.sssssss [ 80%]
sssss............ssssssssssss............ssssssssssss............sssssss [ 81%]
sssss............ssssssssssss............ssssssssssss............s.s.s.s [ 82%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 82%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.sssssss [ 83%]
sssss............ssssssssssss............ssssssssssss............sssssss [ 84%]
sssss............ssssssssssss............ssssssssssss............s.s.s.s [ 85%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 85%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.sssssss [ 86%]
sssss............ssssssssssss............ssssssssssss............sssssss [ 87%]
sssss............ssssssssssss............ssssssssssss............s.s.s.s [ 88%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 88%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.sssssss [ 89%]
ssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssss [ 90%]
ssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.s.s.s.s [ 91%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 91%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.sssssss [ 92%]
ssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssss [ 93%]
ssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.s.s.s.s [ 94%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 94%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.sssssss [ 95%]
ssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssss [ 96%]
ssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.s.s.s.s [ 97%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s [ 97%]
.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.s.sssssss [ 98%]
ssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssss [ 99%]
ssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s.sssssssssssss.s.s.s.s.s..       [100%]

- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_fused_rope.xml -
================ 5865 passed, 3744 skipped in 76.76s (0:01:16) =================
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_nvfp4.xml /root/TransformerEngine/tests/pytorch/nvfp4
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 720 items

tests/pytorch/nvfp4/test_nvfp4_gemm_exact.py ........................... [  3%]
........................................................................ [ 13%]
........................................................................ [ 23%]
.....                                                                    [ 24%]
tests/pytorch/nvfp4/test_nvfp4_module_exact.py sssss.....sssss.......... [ 27%]
...............sssss.....sssss.........................ss..ss..........s [ 37%]
s..ss..........                                                          [ 40%]
tests/pytorch/nvfp4/test_nvfp4_quantize_exact.py ....................... [ 43%]
........................................................................ [ 53%]
........................................................................ [ 63%]
.................................................                        [ 70%]
tests/pytorch/nvfp4/test_nvfp4_rht_quantize_exact.py ................... [ 72%]
........................................................................ [ 82%]
........................................................................ [ 92%]
.....................................                                    [ 97%]
tests/pytorch/nvfp4/test_nvfp4_sr_quantize.py ........ss..ss..           [100%]

- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_nvfp4.xml -
======================= 688 passed, 32 skipped in 17.41s =======================
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_float8tensor.xml /root/TransformerEngine/tests/pytorch/test_float8tensor.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 425 items

tests/pytorch/test_float8tensor.py ...................................ss [  8%]
ssssssssssssssssssssssssssssssssssssssssssssss.......................... [ 25%]
......................ssssssssssssssssssssssssssssssssssssssssssssssss.. [ 42%]
..............................................ssssssssssssssssssssssssss [ 59%]
ssssssssssssssssssssss................................................ss [ 76%]
ssssssssssssssssssssssssssssssssssssssssssssss.......................... [ 93%]
............................                                             [100%]

- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_float8tensor.xml -
======================= 233 passed, 192 skipped in 6.95s =======================
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_float8blockwisetensor.xml /root/TransformerEngine/tests/pytorch/test_float8blockwisetensor.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 243 items

tests/pytorch/test_float8blockwisetensor.py ...........................s [ 11%]
sssssss........ssssssss................................XXXXXXXXXXXXXXXXX [ 41%]
XXXXXXXXXXXXXXX......................................................... [ 70%]
.............ss........................................................  [100%]

- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_float8blockwisetensor.xml -
================= 193 passed, 18 skipped, 32 xpassed in 7.14s ==================
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_float8_blockwise_scaling_exact.xml /root/TransformerEngine/tests/pytorch/test_float8_blockwise_scaling_exact.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 302 items

tests/pytorch/test_float8_blockwise_scaling_exact.py ................... [  6%]
........................................................................ [ 30%]
........................................................................ [ 53%]
.........................................................sssssssssssssss [ 77%]
sssssssssssssssssssssssssssssssssssssssssssssssss....ssss....ssss..      [100%]

- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_float8_blockwise_scaling_exact.xml -
======================= 230 passed, 72 skipped in 21.67s =======================
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_float8_blockwise_gemm_exact.xml /root/TransformerEngine/tests/pytorch/test_float8_blockwise_gemm_exact.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 1579 items

tests/pytorch/test_float8_blockwise_gemm_exact.py ssssssssssssssssssssss [  1%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [  5%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 10%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 15%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 19%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 24%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 28%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 33%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 37%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 42%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 46%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 51%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 56%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 60%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 65%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 69%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 74%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 78%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 83%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 88%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 92%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 97%]
sssssssssssssssssssssssssssssssssssssssssssss                            [100%]

- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_float8_blockwise_gemm_exact.xml -
============================ 1579 skipped in 7.60s =============================
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_gqa.xml /root/TransformerEngine/tests/pytorch/test_gqa.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 20 items

tests/pytorch/test_gqa.py ....................                           [100%]

-- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_gqa.xml --
============================== 20 passed in 8.34s ==============================
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_fused_optimizer.xml /root/TransformerEngine/tests/pytorch/test_fused_optimizer.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 0 items / 1 error

==================================== ERRORS ====================================
____________ ERROR collecting tests/pytorch/test_fused_optimizer.py ____________
ImportError while importing test module '/root/TransformerEngine/tests/pytorch/test_fused_optimizer.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/usr/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
tests/pytorch/test_fused_optimizer.py:11: in <module>
    from torch.testing._internal.common_device_type import largeTensorTest
/usr/local/lib/python3.12/dist-packages/torch/testing/_internal/common_device_type.py:20: in <module>
    from torch.testing._internal.common_cuda import (
/usr/local/lib/python3.12/dist-packages/torch/testing/_internal/common_cuda.py:8: in <module>
    from torch.testing._internal.common_utils import LazyVal, TEST_NUMBA, TEST_WITH_ROCM, TEST_CUDA, IS_WINDOWS, IS_MACOS
/usr/local/lib/python3.12/dist-packages/torch/testing/_internal/common_utils.py:59: in <module>
    import expecttest
E   ModuleNotFoundError: No module named 'expecttest'
- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_fused_optimizer.xml -
=========================== short test summary info ============================
ERROR tests/pytorch/test_fused_optimizer.py
!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
=============================== 1 error in 1.36s ===============================
+ test_fail test_fused_optimizer.py
+ RET=1
+ FAILED_CASES=' test_numerics.py test_fused_optimizer.py'
+ echo 'Error: sub-test failed: test_fused_optimizer.py'
Error: sub-test failed: test_fused_optimizer.py
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_multi_tensor.xml /root/TransformerEngine/tests/pytorch/test_multi_tensor.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 1872 items

tests/pytorch/test_multi_tensor.py ..................................... [  1%]
........................................................................ [  5%]
........................................................................ [  9%]
...........................................................sssssssssssss [ 13%]
sssssssssssssssssssssssssssssssssss..................................... [ 17%]
...........ssssssssssssssssssssssssssssssssssssssssssssssss............. [ 21%]
........................................................................ [ 25%]
...........sssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 28%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 32%]
sssssssssss................................................sssssssssssss [ 36%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 40%]
sssssssssssssssssssssssssssssssssssssssssssssssssssssssssss............. [ 44%]
........................................................................ [ 48%]
........................................................................ [ 51%]
........................................................................ [ 55%]
........................................................................ [ 59%]
........................................................................ [ 63%]
........................................................................ [ 67%]
........................................................................ [ 71%]
........................................................................ [ 75%]
........................................................................ [ 78%]
........................................................................ [ 82%]
........................................................................ [ 86%]
........................................................................ [ 90%]
........................................................................ [ 94%]
........................................................................ [ 98%]
...................................                                      [100%]

- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_multi_tensor.xml -
====================== 1488 passed, 384 skipped in 15.63s ======================
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_fusible_ops.xml /root/TransformerEngine/tests/pytorch/test_fusible_ops.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 3617 items

tests/pytorch/test_fusible_ops.py ...................................... [  1%]
..ssssssss.....................................ssssssss..........sss...s [  3%]
ss...sssss.sss.......................................................... [  5%]
....................sssssssssssssssssssssssssssssssssss.ssssssss.sssssss [  7%]
s.sssssssssssssssssssssssssssssssssss.ssssssss.ssssssss.ssssssssssssssss [  9%]
sssssssssssssssssss.ssssssss.ssssssss.ssssssssssssssssssssssssssssssssss [ 11%]
sssssssssssssssssss.......................................ssss.sssss.sss [ 12%]
ss.sssss.sssss.sssss.sssss.sssss.sssss.sssssssssssssssss.s.............. [ 14%]
....ssss.sssss.sssss.sssss.sssss.sssss.sssss.sssss.sssss.sssssssssssssss [ 16%]
ss.sssssss....s....s....s....s....s....s....ssssssssssssssssssssssssssss [ 18%]
sssssssssssss....s....s....s....s....s....s....s....ssssssssssssssssssss [ 20%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 22%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 24%]
ssssssssssssssssssssssssssssssssssss..ssssssssss......ssss......ssss.... [ 26%]
..ss..ssssssssss......ssss......ssss......ss..ssssssssss......ssss...... [ 28%]
ssss......ss..ssssssssss......ssss......ssss......ss.................... [ 30%]
....sss.sss.sss.sss.sss.sss.sss.sss.sss.sss.sss.sss.sss.sss.sss.sss.sss. [ 32%]
sss.sssssssssss.sssssssssss..........................sss.sss.sss.sss.sss [ 34%]
.sss.sss.sss.sss.sss.sss.sss.sss.sss.sss.sss.sss.sss.sssssssssss.sssssss [ 36%]
ssss............................ssssss......ssssss......ssssssssss..ssss [ 38%]
ss..................ssssss......ssssss......ssssssssss..ssssss.......... [ 40%]
........................................................................ [ 42%]
........ssssssssssssssssssss..........ssssssssssssssssssss..........ssss [ 44%]
ssssssssssssssss..........ssssssssssssssssssss..........ssssssssssssssss [ 46%]
ssss..........ssssssssssssssssssss..........ssssssssssssssssssss........ [ 48%]
..ssssssssssssssssssss..........ssssssssssssssssssss..........ssssssssss [ 50%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss.. [ 52%]
........................................................................ [ 54%]
..........................ssssssssssssssssssss..........ssssssssssssssss [ 56%]
ssss..........ssssssssssssssssssss..........ssssssssssssssssssss........ [ 58%]
..ssssssssssssssssssss..........ssssssssssssssssssss..........ssssssssss [ 60%]
ssssssssss..........ssssssssssssssssssss..........ssssssssssssssssssss.. [ 62%]
........ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 64%]
ssssssssssssssss......................ss.sss.........ss.sss.........ss.s [ 66%]
ss.........ss................sss............sss............sss.......... [ 68%]
........................................................................ [ 70%]
........ssssss......ssssss..................ssssss......ssssss.......... [ 72%]
........ssssss......ssssss............ssssss............ssssssssss.sssss [ 74%]
.sssssssssss.sssss.sssssssssss.sssss.sssssssssssssssss.sssssss.......... [ 76%]
..ssssssssss.sssss.sssssssssss.sssss.sssssssssss.sssss.sssssssssssssssss [ 78%]
.s......ss....ss....ss....ssss..............ssss........ssss........ssss [ 80%]
........ssssssss........................................................ [ 82%]
....................ssssssssssss.................................s..s..s [ 84%]
..ss.............ssss........ssss........ssss........ssssssss........s.. [ 86%]
..s........ssssssssssssssssssssssss........ssssssssssssssssssssssss..... [ 88%]
...ssssssssssssssssssssssssssssssss........................ssssssss..... [ 90%]
...................ssssssss........................ssssssss............. [ 92%]
...........ssssssss........................ssssssss..................... [ 94%]
...ssssssss........................ssssssss........................sssss [ 96%]
sss........................sssssssssssssssssssssssssssssssssssssssssssss [ 98%]
sssssssssssssssssssssssssss........................                      [100%]

=============================== warnings summary ===============================
tests/pytorch/test_fusible_ops.py: 138 warnings
  /root/TransformerEngine/transformer_engine/pytorch/quantized_tensor.py:120: UserWarning: Quantizer is being updated, this may affect model behavior
    warnings.warn("Quantizer is being updated, this may affect model behavior")

tests/pytorch/test_fusible_ops.py: 301 warnings
  /root/TransformerEngine/transformer_engine/pytorch/tensor/float8_tensor.py:785: UserWarning: A function call(aten._to_copy.default) in <class 'transformer_engine.pytorch.tensor.float8_tensor.Float8Tensor'> may not return <class 'transformer_engine.pytorch.tensor.float8_tensor.Float8Tensor'> tensor as an output. It might cause an error in torch FSDP2!
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_fusible_ops.xml -
============== 1700 passed, 1917 skipped, 439 warnings in 13.77s ===============
+ NVTE_KEEP_BACKWARD_UNQUANTIZED=1
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_keep_backward_unquantized.xml /root/TransformerEngine/tests/pytorch/test_keep_backward_unquantized.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 176 items

tests/pytorch/test_keep_backward_unquantized.py ....ssssssss.sss........ [ 13%]
..s...........s...........s...........s.ssssssss.sss..........s......... [ 54%]
..s...........s...........s....s...s...s...s.sss.sss.sss.sss..s...s...s. [ 95%]
..s.....                                                                 [100%]

- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_keep_backward_unquantized.xml -
======================= 126 passed, 50 skipped in 7.40s ========================
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_permutation.xml /root/TransformerEngine/tests/pytorch/test_permutation.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 316 items

tests/pytorch/test_permutation.py ...................................... [ 12%]
........................................................................ [ 34%]
........................................................................ [ 57%]
........................................................................ [ 80%]
..............................................................           [100%]

- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_permutation.xml -
======================== 316 passed in 76.63s (0:01:16) ========================
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_parallel_cross_entropy.xml /root/TransformerEngine/tests/pytorch/test_parallel_cross_entropy.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 6 items

tests/pytorch/test_parallel_cross_entropy.py ......                      [100%]

- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_parallel_cross_entropy.xml -
============================== 6 passed in 9.15s ===============================
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_cpu_offloading.xml /root/TransformerEngine/tests/pytorch/test_cpu_offloading.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 758 items

tests/pytorch/test_cpu_offloading.py ................................... [  4%]
................................ss...................................... [ 14%]
........ss..............................................ss.............. [ 23%]
.........................................s.....s........................ [ 33%]
.................s.....s..ssssssssssssssssssssssssssssssssssssssssssssss [ 42%]
ss.......................................s.....s..ssssssssssssssssssssss [ 52%]
ssssssssssssssssssssssssss.......................................s.....s [ 61%]
..ssssssssssssssssssssssssssssssssssssssssssssssss...................... [ 71%]
.................s.....s.........................................s.....s [ 80%]
.........................................s.....s..ssssssssssssssssssssss [ 90%]
ssssssssssssssssssssssssss.......................................s.....s [ 99%]
...                                                                      [100%]

=============================== warnings summary ===============================
tests/pytorch/test_cpu_offloading.py:18
  /root/TransformerEngine/tests/pytorch/test_cpu_offloading.py:18: DeprecationWarning: Using deprecated internal API from Transformer Engine. transformer_engine.pytorch.fp8 will be removed in a future release.
    from transformer_engine.pytorch.fp8 import FP8GlobalStateManager

tests/pytorch/test_cpu_offloading.py: 78 warnings
  /root/TransformerEngine/transformer_engine/pytorch/tensor/float8_tensor.py:785: UserWarning: A function call(aten.empty_like.default) in <class 'transformer_engine.pytorch.tensor.float8_tensor.Float8Tensor'> may not return <class 'transformer_engine.pytorch.tensor.float8_tensor.Float8Tensor'> tensor as an output. It might cause an error in torch FSDP2!
    warnings.warn(

tests/pytorch/test_cpu_offloading.py: 6574 warnings
  /usr/lib/python3.12/contextlib.py:137: DeprecationWarning: fp8_autocast is deprecated and will be removed in a future release. Use autocast(enabled=..., calibrating=..., recipe=..., group=..., _graph=...) instead.
    return next(self.gen)

tests/pytorch/test_cpu_offloading.py: 414 warnings
  /root/TransformerEngine/transformer_engine/pytorch/cpu_offload.py:787: UserWarning: Offloading num_layers == model_layers - 1 is not recommended, it prevents overlapping of computation and offload/reload.
    warnings.warn(

tests/pytorch/test_cpu_offloading.py: 37 warnings
  /root/TransformerEngine/transformer_engine/pytorch/cpu_offload.py:416: UserWarning: Tried to offload non-contiguous tensor, which is not supported. Offload of this tensor will be skipped.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_cpu_offloading.xml -
=============== 544 passed, 214 skipped, 7104 warnings in 27.17s ===============
+ NVTE_FLASH_ATTN=0
+ NVTE_CPU_OFFLOAD_V1=1
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_cpu_offloading_v1.xml /root/TransformerEngine/tests/pytorch/test_cpu_offloading_v1.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 21 items

tests/pytorch/test_cpu_offloading_v1.py .....................            [100%]

- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_cpu_offloading_v1.xml -
============================= 21 passed in 19.27s ==============================
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_attention.xml /root/TransformerEngine/tests/pytorch/attention/test_attention.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 2573 items

tests/pytorch/attention/test_attention.py .............................. [  1%]
......................FFFFFFFFFFFFssssssssssss.....ss.ss.ss.ss.ss....... [  3%]
...............................s.....s........ssss........ssss.ss.ss.... [  6%]
......................ss..sss...ss..sss....s...s.....s...s.....s...s.... [  9%]
ss..sss...ss..sss....s...s.....s...s.....s...s...ss.ss.sssss.ss.ss.ss.ss [ 12%]
.sssss.ss.ss.....s...sss..s........s...sss..s........s...sss..s.......ss [ 15%]
..................ss..............ssssss..ssss....sssss.s............... [ 17%]
........................................................................ [ 20%]
........................................................................ [ 23%]
........................................................................ [ 26%]
........................................................................ [ 29%]
........................................................................ [ 31%]
........................................................................ [ 34%]
........................................................................ [ 37%]
........................................................................ [ 40%]
........................................................................ [ 43%]
........................................................................ [ 45%]
.......................................sssssssssssssssssssssssssssssssss [ 48%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 51%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 54%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 57%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 59%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 62%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 65%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 68%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 71%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 73%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 76%]
sssssssssssssss..ssss..ssss..ssss..ssss................................. [ 79%]
.................ssss..ssss..ssss..ssss................................. [ 82%]
.................ssss..ssss..ssss..ssss................................. [ 85%]
.................ssss..ssss..ssss..ssss................................. [ 87%]
...............sssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 90%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 93%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 96%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 99%]
sssssssssssssss........                                                  [100%]

=================================== FAILURES ===================================
_____ test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_1-model_configs0-dtype0] _____

dtype = torch.float16
model_configs = {'max_logit_1': <utils.ModelConfig object at 0x13ab36818380>, 'max_logit_2': <utils.ModelConfig object at 0x13a92924b3...git_3': <utils.ModelConfig object at 0x13a92924b440>, 'max_logit_4': <utils.ModelConfig object at 0x13a92924b590>, ...}
model = 'max_logit_1', qkv_layout = 'sbhd_sbhd_sbhd'

    @pytest.mark.skipif(get_cudnn_version() < (8, 9, 1), reason="cuDNN 8.9.1+ is required.")
    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model_configs", [model_configs_max_logit])
    @pytest.mark.parametrize("model", model_configs_max_logit.keys())
    @pytest.mark.parametrize("qkv_layout", ["sbhd_sbhd_sbhd", "thd_thd_thd"])
    def test_dpa_max_logit(dtype, model_configs, model, qkv_layout):
        """Test DotProductAttention module with checkpointing"""
        config = model_configs[model]
        config.return_max_logit = True
>       test_dot_product_attention(dtype, model_configs, model, False, True, qkv_layout, False, False)

tests/pytorch/attention/test_attention.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/attention/test_attention.py:209: in test_dot_product_attention
    fused_attn_fwd, fused_max_logit, fused_attn_bwd = _run_dot_product_attention(
tests/pytorch/attention/test_attention.py:1168: in _run_dot_product_attention
    out = block(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1493: in forward
    return self.fused_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1916: in forward
    output = FusedAttnFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1267: in forward
    out_, aux_ctx_tensors, *max_logit = fused_attn_fwd(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

is_training = True, max_seqlen_q = 2048, max_seqlen_kv = 4096
cu_seqlens_q = tensor([   0, 2048], device='cuda:0', dtype=torch.int32)
cu_seqlens_kv = tensor([   0, 4096], device='cuda:0', dtype=torch.int32)
q = tensor([[[[-1.6162e-01,  5.6824e-02, -5.1025e-02,  ..., -1.3501e-01,
            2.7100e-01, -1.6800e-02],
          [... -1.7065e-01,
            1.0840e-01,  1.0126e-01]]]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
k = tensor([[[[-0.0131,  0.0287, -0.0152,  ..., -0.0745, -0.0908,  0.0562],
          [-0.1882, -0.0041, -0.0862,  ...,  0...-0.0090, -0.0726,  ...,  0.0657,  0.0109, -0.2178]]]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
v = tensor([[[[-9.3384e-02, -1.1407e-01, -1.2335e-01,  ...,  9.1431e-02,
           -1.1133e-01, -6.7932e-02],
          [... -1.0480e-01,
            1.4587e-01, -8.2458e-02]]]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
fake_dtype = torch.float16
fused_attention_backend = <NVTE_Fused_Attn_Backend.NVTE_F16_arbitrary_seqlen: 1>
attn_bias = None
cu_seqlens_q_padded = tensor([   0, 2048], device='cuda:0', dtype=torch.int32)
cu_seqlens_kv_padded = tensor([   0, 4096], device='cuda:0', dtype=torch.int32)
page_table_k = None, page_table_v = None, s_quantizer = None, o_quantizer = None
attn_scale = 0.08838834764831843, dropout = 0.0, fast_zero_fill = True
qkv_layout = 'sbhd_sbhd_sbhd', attn_bias_type = 'no_bias'
attn_mask_type = 'no_mask', softmax_type = 'vanilla', window_size = (-1, -1)
rng_gen = None, softmax_offset = None, return_max_logit = True
cuda_graph = False

    def fused_attn_fwd(
        is_training: bool,
        max_seqlen_q: int,
        max_seqlen_kv: int,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_kv: torch.Tensor,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        fake_dtype: torch.dtype,
        fused_attention_backend: tex.NVTE_Fused_Attn_Backend,
        attn_bias: torch.Tensor = None,
        cu_seqlens_q_padded: torch.Tensor = None,
        cu_seqlens_kv_padded: torch.Tensor = None,
        page_table_k: torch.Tensor = None,
        page_table_v: torch.Tensor = None,
        s_quantizer: Quantizer = None,
        o_quantizer: Quantizer = None,
        attn_scale: float = None,
        dropout: float = 0.0,
        fast_zero_fill: bool = True,
        qkv_layout: str = "sbh3d",
        attn_bias_type: str = "no_bias",
        attn_mask_type: str = "padding",
        softmax_type: str = "vanilla",
        window_size: Tuple[int, int] = (-1, -1),
        rng_gen: torch.Generator = None,
        softmax_offset: torch.Tensor = None,
        return_max_logit: bool = False,
        cuda_graph: bool = False,
    ) -> Tuple[Union[torch.Tensor, None], ...]:
        """Fused Attention FWD for separate QKV input.
    
        Parameters
        ----------
        is_training : bool
                    if True, runs training and produces auxiliary tensors aux_ctx_tensors
                    for the backward; if False, runs inference and doesn't produce aux_ctx_tensors
        max_seqlen_q : int
                    max sequence length for Q, used for padding;
                    may be larger than max(seqlens_q),
                    seqlens_q = cu_seqlens_q[1:] - cu_seqlens_q[:-1]
        max_seqlen_kv : int
                    max sequence length for K and V, used for padding;
                    may be larger than max(seqlens_kv),
                    seqlens_kv = cu_seqlens_kv[1:] - cu_seqlens_kv[:-1]
        cu_seqlens_q : torch.Tensor
                    cumulative sequence lengths for Q; shape [batch_size + 1]
        cu_seqlens_kv : torch.Tensor
                    cumulative sequence lengths for K and V; shape [batch_size + 1]
        q : torch.Tensor
                    input tensor Q; shape sbhd, bshd or thd (see `qkv_layout` for details)
        k : torch.Tensor
                    input tensor K; shape sbhd, bshd or thd (see `qkv_layout` for details)
        v : torch.Tensor
                    input tensor V; shape sbhd, bshd or thd (see `qkv_layout` for details)
        fake_dtype : tex.DType
                    data type of Q, K and V - in case of high precision, fake dtype in case of FP8;
                    in torch.dtype
        fused_attention_backend : tex.NVTE_Fused_Attn_Backend
                    please see FusedAttention module for details on supported backends.
        attn_bias : torch.Tensor, default = None
                    input tensor Bias when attn_bias_type is "pre_scale_bias" or "post_scale_bias";
                    shape [1, num_heads, max_seqlen_q, max_seqlen_kv], same data type as q, k and v
        cu_seqlens_q_padded : torch.Tensor, default = None
                    cumulative sequence offsets for Q; shape [batch_size + 1]
        cu_seqlens_kv_padded : torch.Tensor, default = None
                    cumulative sequence offsets for KV; shape [batch_size + 1]
        page_table_k : torch.Tensor, default = None
                    page table for K cache; shape [batch_size, max_pages_per_seq_k]
        page_table_v : torch.Tensor, default = None
                    page table for V cache; shape [batch_size, max_pages_per_seq_v]
        s_quantizer : Quantizer, default = None
                    Quantizer object for the intermediate value S.
        o_quantizer : Quantizer, default = None
                    Quantizer object for the output of the attention.
        attn_scale : float, default = None
                    if not None, use attn_scale as the attention scale for Q*K.T BMM;
                    if None, use 1.0/sqrt(head_dim_qk) as the default
        dropout : float, default = 0.0
                    dropout probability, 0.0 means no dropout, 1.0 means no output;
                    dropout must be 0.0 if is_training is False
        fast_zero_fill : bool, default = True
                    if True, initializes the output tensor O to zero using the fast filling method;
                    if False, uses PyTorch's .fill_() method
        qkv_layout : str, default = "sbh3d"
                    layout of Q, K and V;
                    {"sb3hd", "sbh3d", "sbhd_sb2hd", "sbhd_sbh2d", "sbhd_sbhd_sbhd",
                    "bs3hd", "bsh3d", "bshd_bs2hd", "bshd_bsh2d", "bshd_bshd_bshd",
                    "t3hd", "th3d", "thd_t2hd", "thd_th2d", "thd_thd_thd"}
        attn_bias_type : str, default = "no_bias"
                    type of the bias; {"no_bias", "pre_scale_bias", "post_scale_bias", "alibi"}
        attn_mask_type : str, default = "padding"
                    type of the attention mask; {"padding", "causal", "padding_causal", "no_mask"}
        softmax_type : str, default = "vanilla"
                    type of the attention softmax; {"vanilla", "off-by-one", "learnable"}
        window_size : Tuple[int, int], default = (-1, -1)
                    sliding window size for local attention, where query at position i attends to keys
                    in [i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q
                    + window_size[1]] inclusive. Special cases (-1, -1) and (-1, 0) mean no sliding
                    window and causal mask specifically.
        rng_gen : torch.Generator, default = None
                    random number generator;
                    if None, uses the default CUDA generator from PyTorch; otherwise, uses rng_gen
        softmax_offset : torch.Tensor, default = None
                    softmax offset tensor of shape [1, h_q, 1, 1].
                    See softmax_type in DotProductAttention for details.
        return_max_logit : bool, default = False
                          whether to return the maximum attention score
        cuda_graph : bool, default = False
                    whether or not cuda graph capture is enabled.
    
        Returns
        ----------
        o : torch.Tensor
                    output tensor O, of the attention calculation; same data type as Q, K and V;
                    same shape as Q
        aux_ctx_tensors : List[torch.Tensor]
                    auxiliary output tensors used for the backward;
                    if is_training is True, aux_ctx_tensors = [softmax-related tensors, rng_state]
                    if is_training is False, aux_ctx_tensors = None
    
                    softmax-related tensors:
                        1. if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]
                           softmax: torch.Tensor
                               Softmax(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, max_seqlen_kv], dtype float32
                        2. if fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]
                           softmaxStats: torch.Tensor
                               log(sum(e^(x - max(x)))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                        3. if fused_attention_backend == FusedAttnBackend["FP8"]
                           M: torch.Tensor
                               max(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                           ZInv: torch.Tensor
                               1/sum(e^(x - max(x))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                    rng_state: torch.Tensor, optional, if backend is not F16_max512_seqlen
                        state of the random number generator;
                        [seed, offset], dtype uint64
        max_logit : if return_max_logit = True, shape [h] and same data type as O; otherwise None
        """
    
        if attn_scale is None:
            d = q.size(-1)
            attn_scale = 1.0 / math.sqrt(d)
    
        if attn_bias_type not in ["no_bias", "alibi"]:
            assert (
                attn_bias is not None
            ), "attn_bias tensor cannot be None when attn_bias_type is not no_bias or alibi."
            assert attn_bias.dtype == q.dtype, "attn_bias tensor must be in the same dtype as q and kv."
    
        assert (
            fused_attention_backend != FusedAttnBackend["No_Backend"]
        ), "Fused attention does not support this input combination."
    
        # BF16/FP16 fused attention API from fmha_v1 apex
        if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_kv + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
        # BF16/FP16 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]:
            rng_elts_per_thread = BACKEND_F16arb_ELTS_PER_THREADS
        # FP8 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["FP8"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_q + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
    
            assert (
                s_quantizer is not None
            ), "s_quantizer is required as an input for FP8 fused attention."
            assert (
                o_quantizer is not None
            ), "o_quantizer is required as an input for FP8 fused attention."
        else:
            raise ValueError(f"Unsupported backend {fused_attention_backend}")
    
        # execute kernel
    
>       output_tensors = tex.fused_attn_fwd(
            max_seqlen_q,
            max_seqlen_kv,
            is_training,
            attn_scale,
            dropout,
            fast_zero_fill,
            QKVLayout[qkv_layout],
            AttnBiasType[attn_bias_type],
            AttnMaskType[attn_mask_type],
            SoftmaxType[softmax_type],
            window_size,
            cu_seqlens_q,
            cu_seqlens_kv,
            q,
            k,
            v,
            fake_dtype,
            cu_seqlens_q_padded,
            cu_seqlens_kv_padded,
            page_table_k,
            page_table_v,
            s_quantizer,
            o_quantizer,
            attn_bias,
            softmax_offset,
            rng_gen,
            rng_elts_per_thread,
            return_max_logit,
            cuda_graph,
        )
E       RuntimeError: /root/TransformerEngine/transformer_engine/common/fused_attn/fused_attn_f16_arbitrary_seqlen.cu:406 in function operator(): cuDNN Error: No valid engine configs for Matmul_MUL_Reduction_SUB_EXP_Reduction_DIV_Matmul_
E       {"engineId":0,"smVersion":1000,"knobChoices":{"CUDNN_KNOB_TYPE_KERNEL_CFG":0}}
E       
E       . For more information, enable cuDNN error logging by setting CUDNN_LOGERR_DBG=1 and CUDNN_LOGDEST_DBG=stderr in the environment.

transformer_engine/pytorch/cpp_extensions/fused_attn.py:297: RuntimeError
_____ test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_1-model_configs0-dtype1] _____

dtype = torch.bfloat16
model_configs = {'max_logit_1': <utils.ModelConfig object at 0x13ab36818380>, 'max_logit_2': <utils.ModelConfig object at 0x13a92924b3...git_3': <utils.ModelConfig object at 0x13a92924b440>, 'max_logit_4': <utils.ModelConfig object at 0x13a92924b590>, ...}
model = 'max_logit_1', qkv_layout = 'sbhd_sbhd_sbhd'

    @pytest.mark.skipif(get_cudnn_version() < (8, 9, 1), reason="cuDNN 8.9.1+ is required.")
    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model_configs", [model_configs_max_logit])
    @pytest.mark.parametrize("model", model_configs_max_logit.keys())
    @pytest.mark.parametrize("qkv_layout", ["sbhd_sbhd_sbhd", "thd_thd_thd"])
    def test_dpa_max_logit(dtype, model_configs, model, qkv_layout):
        """Test DotProductAttention module with checkpointing"""
        config = model_configs[model]
        config.return_max_logit = True
>       test_dot_product_attention(dtype, model_configs, model, False, True, qkv_layout, False, False)

tests/pytorch/attention/test_attention.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/attention/test_attention.py:209: in test_dot_product_attention
    fused_attn_fwd, fused_max_logit, fused_attn_bwd = _run_dot_product_attention(
tests/pytorch/attention/test_attention.py:1168: in _run_dot_product_attention
    out = block(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1493: in forward
    return self.fused_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1916: in forward
    output = FusedAttnFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1267: in forward
    out_, aux_ctx_tensors, *max_logit = fused_attn_fwd(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

is_training = True, max_seqlen_q = 2048, max_seqlen_kv = 4096
cu_seqlens_q = tensor([   0, 2048], device='cuda:0', dtype=torch.int32)
cu_seqlens_kv = tensor([   0, 4096], device='cuda:0', dtype=torch.int32)
q = tensor([[[[-1.6211e-01,  5.7129e-02, -5.1270e-02,  ..., -1.3477e-01,
            2.6953e-01, -1.6846e-02],
          [...-1.6992e-01,
            1.0840e-01,  1.0156e-01]]]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
k = tensor([[[[-0.0131,  0.0287, -0.0153,  ..., -0.0747, -0.0908,  0.0562],
          [-0.1885, -0.0041, -0.0864,  ...,  0...0.0090, -0.0728,  ...,  0.0654,  0.0109, -0.2168]]]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
v = tensor([[[[-9.3262e-02, -1.1426e-01, -1.2354e-01,  ...,  9.1309e-02,
           -1.1182e-01, -6.7871e-02],
          [...-1.0449e-01,
            1.4648e-01, -8.2520e-02]]]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
fake_dtype = torch.bfloat16
fused_attention_backend = <NVTE_Fused_Attn_Backend.NVTE_F16_arbitrary_seqlen: 1>
attn_bias = None
cu_seqlens_q_padded = tensor([   0, 2048], device='cuda:0', dtype=torch.int32)
cu_seqlens_kv_padded = tensor([   0, 4096], device='cuda:0', dtype=torch.int32)
page_table_k = None, page_table_v = None, s_quantizer = None, o_quantizer = None
attn_scale = 0.08838834764831843, dropout = 0.0, fast_zero_fill = True
qkv_layout = 'sbhd_sbhd_sbhd', attn_bias_type = 'no_bias'
attn_mask_type = 'no_mask', softmax_type = 'vanilla', window_size = (-1, -1)
rng_gen = None, softmax_offset = None, return_max_logit = True
cuda_graph = False

    def fused_attn_fwd(
        is_training: bool,
        max_seqlen_q: int,
        max_seqlen_kv: int,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_kv: torch.Tensor,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        fake_dtype: torch.dtype,
        fused_attention_backend: tex.NVTE_Fused_Attn_Backend,
        attn_bias: torch.Tensor = None,
        cu_seqlens_q_padded: torch.Tensor = None,
        cu_seqlens_kv_padded: torch.Tensor = None,
        page_table_k: torch.Tensor = None,
        page_table_v: torch.Tensor = None,
        s_quantizer: Quantizer = None,
        o_quantizer: Quantizer = None,
        attn_scale: float = None,
        dropout: float = 0.0,
        fast_zero_fill: bool = True,
        qkv_layout: str = "sbh3d",
        attn_bias_type: str = "no_bias",
        attn_mask_type: str = "padding",
        softmax_type: str = "vanilla",
        window_size: Tuple[int, int] = (-1, -1),
        rng_gen: torch.Generator = None,
        softmax_offset: torch.Tensor = None,
        return_max_logit: bool = False,
        cuda_graph: bool = False,
    ) -> Tuple[Union[torch.Tensor, None], ...]:
        """Fused Attention FWD for separate QKV input.
    
        Parameters
        ----------
        is_training : bool
                    if True, runs training and produces auxiliary tensors aux_ctx_tensors
                    for the backward; if False, runs inference and doesn't produce aux_ctx_tensors
        max_seqlen_q : int
                    max sequence length for Q, used for padding;
                    may be larger than max(seqlens_q),
                    seqlens_q = cu_seqlens_q[1:] - cu_seqlens_q[:-1]
        max_seqlen_kv : int
                    max sequence length for K and V, used for padding;
                    may be larger than max(seqlens_kv),
                    seqlens_kv = cu_seqlens_kv[1:] - cu_seqlens_kv[:-1]
        cu_seqlens_q : torch.Tensor
                    cumulative sequence lengths for Q; shape [batch_size + 1]
        cu_seqlens_kv : torch.Tensor
                    cumulative sequence lengths for K and V; shape [batch_size + 1]
        q : torch.Tensor
                    input tensor Q; shape sbhd, bshd or thd (see `qkv_layout` for details)
        k : torch.Tensor
                    input tensor K; shape sbhd, bshd or thd (see `qkv_layout` for details)
        v : torch.Tensor
                    input tensor V; shape sbhd, bshd or thd (see `qkv_layout` for details)
        fake_dtype : tex.DType
                    data type of Q, K and V - in case of high precision, fake dtype in case of FP8;
                    in torch.dtype
        fused_attention_backend : tex.NVTE_Fused_Attn_Backend
                    please see FusedAttention module for details on supported backends.
        attn_bias : torch.Tensor, default = None
                    input tensor Bias when attn_bias_type is "pre_scale_bias" or "post_scale_bias";
                    shape [1, num_heads, max_seqlen_q, max_seqlen_kv], same data type as q, k and v
        cu_seqlens_q_padded : torch.Tensor, default = None
                    cumulative sequence offsets for Q; shape [batch_size + 1]
        cu_seqlens_kv_padded : torch.Tensor, default = None
                    cumulative sequence offsets for KV; shape [batch_size + 1]
        page_table_k : torch.Tensor, default = None
                    page table for K cache; shape [batch_size, max_pages_per_seq_k]
        page_table_v : torch.Tensor, default = None
                    page table for V cache; shape [batch_size, max_pages_per_seq_v]
        s_quantizer : Quantizer, default = None
                    Quantizer object for the intermediate value S.
        o_quantizer : Quantizer, default = None
                    Quantizer object for the output of the attention.
        attn_scale : float, default = None
                    if not None, use attn_scale as the attention scale for Q*K.T BMM;
                    if None, use 1.0/sqrt(head_dim_qk) as the default
        dropout : float, default = 0.0
                    dropout probability, 0.0 means no dropout, 1.0 means no output;
                    dropout must be 0.0 if is_training is False
        fast_zero_fill : bool, default = True
                    if True, initializes the output tensor O to zero using the fast filling method;
                    if False, uses PyTorch's .fill_() method
        qkv_layout : str, default = "sbh3d"
                    layout of Q, K and V;
                    {"sb3hd", "sbh3d", "sbhd_sb2hd", "sbhd_sbh2d", "sbhd_sbhd_sbhd",
                    "bs3hd", "bsh3d", "bshd_bs2hd", "bshd_bsh2d", "bshd_bshd_bshd",
                    "t3hd", "th3d", "thd_t2hd", "thd_th2d", "thd_thd_thd"}
        attn_bias_type : str, default = "no_bias"
                    type of the bias; {"no_bias", "pre_scale_bias", "post_scale_bias", "alibi"}
        attn_mask_type : str, default = "padding"
                    type of the attention mask; {"padding", "causal", "padding_causal", "no_mask"}
        softmax_type : str, default = "vanilla"
                    type of the attention softmax; {"vanilla", "off-by-one", "learnable"}
        window_size : Tuple[int, int], default = (-1, -1)
                    sliding window size for local attention, where query at position i attends to keys
                    in [i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q
                    + window_size[1]] inclusive. Special cases (-1, -1) and (-1, 0) mean no sliding
                    window and causal mask specifically.
        rng_gen : torch.Generator, default = None
                    random number generator;
                    if None, uses the default CUDA generator from PyTorch; otherwise, uses rng_gen
        softmax_offset : torch.Tensor, default = None
                    softmax offset tensor of shape [1, h_q, 1, 1].
                    See softmax_type in DotProductAttention for details.
        return_max_logit : bool, default = False
                          whether to return the maximum attention score
        cuda_graph : bool, default = False
                    whether or not cuda graph capture is enabled.
    
        Returns
        ----------
        o : torch.Tensor
                    output tensor O, of the attention calculation; same data type as Q, K and V;
                    same shape as Q
        aux_ctx_tensors : List[torch.Tensor]
                    auxiliary output tensors used for the backward;
                    if is_training is True, aux_ctx_tensors = [softmax-related tensors, rng_state]
                    if is_training is False, aux_ctx_tensors = None
    
                    softmax-related tensors:
                        1. if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]
                           softmax: torch.Tensor
                               Softmax(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, max_seqlen_kv], dtype float32
                        2. if fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]
                           softmaxStats: torch.Tensor
                               log(sum(e^(x - max(x)))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                        3. if fused_attention_backend == FusedAttnBackend["FP8"]
                           M: torch.Tensor
                               max(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                           ZInv: torch.Tensor
                               1/sum(e^(x - max(x))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                    rng_state: torch.Tensor, optional, if backend is not F16_max512_seqlen
                        state of the random number generator;
                        [seed, offset], dtype uint64
        max_logit : if return_max_logit = True, shape [h] and same data type as O; otherwise None
        """
    
        if attn_scale is None:
            d = q.size(-1)
            attn_scale = 1.0 / math.sqrt(d)
    
        if attn_bias_type not in ["no_bias", "alibi"]:
            assert (
                attn_bias is not None
            ), "attn_bias tensor cannot be None when attn_bias_type is not no_bias or alibi."
            assert attn_bias.dtype == q.dtype, "attn_bias tensor must be in the same dtype as q and kv."
    
        assert (
            fused_attention_backend != FusedAttnBackend["No_Backend"]
        ), "Fused attention does not support this input combination."
    
        # BF16/FP16 fused attention API from fmha_v1 apex
        if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_kv + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
        # BF16/FP16 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]:
            rng_elts_per_thread = BACKEND_F16arb_ELTS_PER_THREADS
        # FP8 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["FP8"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_q + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
    
            assert (
                s_quantizer is not None
            ), "s_quantizer is required as an input for FP8 fused attention."
            assert (
                o_quantizer is not None
            ), "o_quantizer is required as an input for FP8 fused attention."
        else:
            raise ValueError(f"Unsupported backend {fused_attention_backend}")
    
        # execute kernel
    
>       output_tensors = tex.fused_attn_fwd(
            max_seqlen_q,
            max_seqlen_kv,
            is_training,
            attn_scale,
            dropout,
            fast_zero_fill,
            QKVLayout[qkv_layout],
            AttnBiasType[attn_bias_type],
            AttnMaskType[attn_mask_type],
            SoftmaxType[softmax_type],
            window_size,
            cu_seqlens_q,
            cu_seqlens_kv,
            q,
            k,
            v,
            fake_dtype,
            cu_seqlens_q_padded,
            cu_seqlens_kv_padded,
            page_table_k,
            page_table_v,
            s_quantizer,
            o_quantizer,
            attn_bias,
            softmax_offset,
            rng_gen,
            rng_elts_per_thread,
            return_max_logit,
            cuda_graph,
        )
E       RuntimeError: /root/TransformerEngine/transformer_engine/common/fused_attn/fused_attn_f16_arbitrary_seqlen.cu:406 in function operator(): cuDNN Error: No valid engine configs for Matmul_MUL_Reduction_SUB_EXP_Reduction_DIV_Matmul_
E       {"engineId":0,"smVersion":1000,"knobChoices":{"CUDNN_KNOB_TYPE_KERNEL_CFG":0}}
E       
E       . For more information, enable cuDNN error logging by setting CUDNN_LOGERR_DBG=1 and CUDNN_LOGDEST_DBG=stderr in the environment.

transformer_engine/pytorch/cpp_extensions/fused_attn.py:297: RuntimeError
_____ test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_2-model_configs0-dtype0] _____

dtype = torch.float16
model_configs = {'max_logit_1': <utils.ModelConfig object at 0x13ab36818380>, 'max_logit_2': <utils.ModelConfig object at 0x13a92924b3...git_3': <utils.ModelConfig object at 0x13a92924b440>, 'max_logit_4': <utils.ModelConfig object at 0x13a92924b590>, ...}
model = 'max_logit_2', qkv_layout = 'sbhd_sbhd_sbhd'

    @pytest.mark.skipif(get_cudnn_version() < (8, 9, 1), reason="cuDNN 8.9.1+ is required.")
    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model_configs", [model_configs_max_logit])
    @pytest.mark.parametrize("model", model_configs_max_logit.keys())
    @pytest.mark.parametrize("qkv_layout", ["sbhd_sbhd_sbhd", "thd_thd_thd"])
    def test_dpa_max_logit(dtype, model_configs, model, qkv_layout):
        """Test DotProductAttention module with checkpointing"""
        config = model_configs[model]
        config.return_max_logit = True
>       test_dot_product_attention(dtype, model_configs, model, False, True, qkv_layout, False, False)

tests/pytorch/attention/test_attention.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/attention/test_attention.py:209: in test_dot_product_attention
    fused_attn_fwd, fused_max_logit, fused_attn_bwd = _run_dot_product_attention(
tests/pytorch/attention/test_attention.py:1168: in _run_dot_product_attention
    out = block(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1493: in forward
    return self.fused_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1916: in forward
    output = FusedAttnFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1267: in forward
    out_, aux_ctx_tensors, *max_logit = fused_attn_fwd(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

is_training = True, max_seqlen_q = 2048, max_seqlen_kv = 2048
cu_seqlens_q = tensor([   0, 2048, 4096], device='cuda:0', dtype=torch.int32)
cu_seqlens_kv = tensor([   0, 2048, 4096], device='cuda:0', dtype=torch.int32)
q = tensor([[[[-1.6162e-01,  5.6824e-02, -5.1025e-02,  ..., -1.3501e-01,
            2.7100e-01, -1.6800e-02],
          [...  9.5367e-03,
            6.5491e-02,  1.6345e-01]]]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
k = tensor([[[[ 9.3628e-02,  1.5259e-02, -8.6746e-03,  ...,  3.9612e-02,
           -1.9629e-01, -1.3049e-01],
          [... -7.5562e-02,
            6.9458e-02, -1.6187e-01]]]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
v = tensor([[[[-2.5604e-02, -5.9906e-02, -8.9783e-02,  ...,  5.4352e-02,
            6.2561e-02, -6.5979e-02],
          [... -8.5876e-02,
            1.4026e-01,  4.4647e-02]]]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
fake_dtype = torch.float16
fused_attention_backend = <NVTE_Fused_Attn_Backend.NVTE_F16_arbitrary_seqlen: 1>
attn_bias = None
cu_seqlens_q_padded = tensor([   0, 2048, 4096], device='cuda:0', dtype=torch.int32)
cu_seqlens_kv_padded = tensor([   0, 2048, 4096], device='cuda:0', dtype=torch.int32)
page_table_k = None, page_table_v = None, s_quantizer = None, o_quantizer = None
attn_scale = 0.08838834764831843, dropout = 0.0, fast_zero_fill = True
qkv_layout = 'sbhd_sbhd_sbhd', attn_bias_type = 'no_bias'
attn_mask_type = 'causal', softmax_type = 'vanilla', window_size = (-1, 0)
rng_gen = None, softmax_offset = None, return_max_logit = True
cuda_graph = False

    def fused_attn_fwd(
        is_training: bool,
        max_seqlen_q: int,
        max_seqlen_kv: int,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_kv: torch.Tensor,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        fake_dtype: torch.dtype,
        fused_attention_backend: tex.NVTE_Fused_Attn_Backend,
        attn_bias: torch.Tensor = None,
        cu_seqlens_q_padded: torch.Tensor = None,
        cu_seqlens_kv_padded: torch.Tensor = None,
        page_table_k: torch.Tensor = None,
        page_table_v: torch.Tensor = None,
        s_quantizer: Quantizer = None,
        o_quantizer: Quantizer = None,
        attn_scale: float = None,
        dropout: float = 0.0,
        fast_zero_fill: bool = True,
        qkv_layout: str = "sbh3d",
        attn_bias_type: str = "no_bias",
        attn_mask_type: str = "padding",
        softmax_type: str = "vanilla",
        window_size: Tuple[int, int] = (-1, -1),
        rng_gen: torch.Generator = None,
        softmax_offset: torch.Tensor = None,
        return_max_logit: bool = False,
        cuda_graph: bool = False,
    ) -> Tuple[Union[torch.Tensor, None], ...]:
        """Fused Attention FWD for separate QKV input.
    
        Parameters
        ----------
        is_training : bool
                    if True, runs training and produces auxiliary tensors aux_ctx_tensors
                    for the backward; if False, runs inference and doesn't produce aux_ctx_tensors
        max_seqlen_q : int
                    max sequence length for Q, used for padding;
                    may be larger than max(seqlens_q),
                    seqlens_q = cu_seqlens_q[1:] - cu_seqlens_q[:-1]
        max_seqlen_kv : int
                    max sequence length for K and V, used for padding;
                    may be larger than max(seqlens_kv),
                    seqlens_kv = cu_seqlens_kv[1:] - cu_seqlens_kv[:-1]
        cu_seqlens_q : torch.Tensor
                    cumulative sequence lengths for Q; shape [batch_size + 1]
        cu_seqlens_kv : torch.Tensor
                    cumulative sequence lengths for K and V; shape [batch_size + 1]
        q : torch.Tensor
                    input tensor Q; shape sbhd, bshd or thd (see `qkv_layout` for details)
        k : torch.Tensor
                    input tensor K; shape sbhd, bshd or thd (see `qkv_layout` for details)
        v : torch.Tensor
                    input tensor V; shape sbhd, bshd or thd (see `qkv_layout` for details)
        fake_dtype : tex.DType
                    data type of Q, K and V - in case of high precision, fake dtype in case of FP8;
                    in torch.dtype
        fused_attention_backend : tex.NVTE_Fused_Attn_Backend
                    please see FusedAttention module for details on supported backends.
        attn_bias : torch.Tensor, default = None
                    input tensor Bias when attn_bias_type is "pre_scale_bias" or "post_scale_bias";
                    shape [1, num_heads, max_seqlen_q, max_seqlen_kv], same data type as q, k and v
        cu_seqlens_q_padded : torch.Tensor, default = None
                    cumulative sequence offsets for Q; shape [batch_size + 1]
        cu_seqlens_kv_padded : torch.Tensor, default = None
                    cumulative sequence offsets for KV; shape [batch_size + 1]
        page_table_k : torch.Tensor, default = None
                    page table for K cache; shape [batch_size, max_pages_per_seq_k]
        page_table_v : torch.Tensor, default = None
                    page table for V cache; shape [batch_size, max_pages_per_seq_v]
        s_quantizer : Quantizer, default = None
                    Quantizer object for the intermediate value S.
        o_quantizer : Quantizer, default = None
                    Quantizer object for the output of the attention.
        attn_scale : float, default = None
                    if not None, use attn_scale as the attention scale for Q*K.T BMM;
                    if None, use 1.0/sqrt(head_dim_qk) as the default
        dropout : float, default = 0.0
                    dropout probability, 0.0 means no dropout, 1.0 means no output;
                    dropout must be 0.0 if is_training is False
        fast_zero_fill : bool, default = True
                    if True, initializes the output tensor O to zero using the fast filling method;
                    if False, uses PyTorch's .fill_() method
        qkv_layout : str, default = "sbh3d"
                    layout of Q, K and V;
                    {"sb3hd", "sbh3d", "sbhd_sb2hd", "sbhd_sbh2d", "sbhd_sbhd_sbhd",
                    "bs3hd", "bsh3d", "bshd_bs2hd", "bshd_bsh2d", "bshd_bshd_bshd",
                    "t3hd", "th3d", "thd_t2hd", "thd_th2d", "thd_thd_thd"}
        attn_bias_type : str, default = "no_bias"
                    type of the bias; {"no_bias", "pre_scale_bias", "post_scale_bias", "alibi"}
        attn_mask_type : str, default = "padding"
                    type of the attention mask; {"padding", "causal", "padding_causal", "no_mask"}
        softmax_type : str, default = "vanilla"
                    type of the attention softmax; {"vanilla", "off-by-one", "learnable"}
        window_size : Tuple[int, int], default = (-1, -1)
                    sliding window size for local attention, where query at position i attends to keys
                    in [i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q
                    + window_size[1]] inclusive. Special cases (-1, -1) and (-1, 0) mean no sliding
                    window and causal mask specifically.
        rng_gen : torch.Generator, default = None
                    random number generator;
                    if None, uses the default CUDA generator from PyTorch; otherwise, uses rng_gen
        softmax_offset : torch.Tensor, default = None
                    softmax offset tensor of shape [1, h_q, 1, 1].
                    See softmax_type in DotProductAttention for details.
        return_max_logit : bool, default = False
                          whether to return the maximum attention score
        cuda_graph : bool, default = False
                    whether or not cuda graph capture is enabled.
    
        Returns
        ----------
        o : torch.Tensor
                    output tensor O, of the attention calculation; same data type as Q, K and V;
                    same shape as Q
        aux_ctx_tensors : List[torch.Tensor]
                    auxiliary output tensors used for the backward;
                    if is_training is True, aux_ctx_tensors = [softmax-related tensors, rng_state]
                    if is_training is False, aux_ctx_tensors = None
    
                    softmax-related tensors:
                        1. if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]
                           softmax: torch.Tensor
                               Softmax(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, max_seqlen_kv], dtype float32
                        2. if fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]
                           softmaxStats: torch.Tensor
                               log(sum(e^(x - max(x)))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                        3. if fused_attention_backend == FusedAttnBackend["FP8"]
                           M: torch.Tensor
                               max(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                           ZInv: torch.Tensor
                               1/sum(e^(x - max(x))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                    rng_state: torch.Tensor, optional, if backend is not F16_max512_seqlen
                        state of the random number generator;
                        [seed, offset], dtype uint64
        max_logit : if return_max_logit = True, shape [h] and same data type as O; otherwise None
        """
    
        if attn_scale is None:
            d = q.size(-1)
            attn_scale = 1.0 / math.sqrt(d)
    
        if attn_bias_type not in ["no_bias", "alibi"]:
            assert (
                attn_bias is not None
            ), "attn_bias tensor cannot be None when attn_bias_type is not no_bias or alibi."
            assert attn_bias.dtype == q.dtype, "attn_bias tensor must be in the same dtype as q and kv."
    
        assert (
            fused_attention_backend != FusedAttnBackend["No_Backend"]
        ), "Fused attention does not support this input combination."
    
        # BF16/FP16 fused attention API from fmha_v1 apex
        if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_kv + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
        # BF16/FP16 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]:
            rng_elts_per_thread = BACKEND_F16arb_ELTS_PER_THREADS
        # FP8 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["FP8"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_q + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
    
            assert (
                s_quantizer is not None
            ), "s_quantizer is required as an input for FP8 fused attention."
            assert (
                o_quantizer is not None
            ), "o_quantizer is required as an input for FP8 fused attention."
        else:
            raise ValueError(f"Unsupported backend {fused_attention_backend}")
    
        # execute kernel
    
>       output_tensors = tex.fused_attn_fwd(
            max_seqlen_q,
            max_seqlen_kv,
            is_training,
            attn_scale,
            dropout,
            fast_zero_fill,
            QKVLayout[qkv_layout],
            AttnBiasType[attn_bias_type],
            AttnMaskType[attn_mask_type],
            SoftmaxType[softmax_type],
            window_size,
            cu_seqlens_q,
            cu_seqlens_kv,
            q,
            k,
            v,
            fake_dtype,
            cu_seqlens_q_padded,
            cu_seqlens_kv_padded,
            page_table_k,
            page_table_v,
            s_quantizer,
            o_quantizer,
            attn_bias,
            softmax_offset,
            rng_gen,
            rng_elts_per_thread,
            return_max_logit,
            cuda_graph,
        )
E       RuntimeError: /root/TransformerEngine/transformer_engine/common/fused_attn/fused_attn_f16_arbitrary_seqlen.cu:406 in function operator(): cuDNN Error: No valid engine configs for Matmul_MUL_GEN_INDEX_GEN_INDEX_CMP_GE_BINARY_SELECT_Reduction_SUB_EXP_Reduction_DIV_Matmul_
E       {"engineId":0,"smVersion":1000,"knobChoices":{"CUDNN_KNOB_TYPE_KERNEL_CFG":0}}
E       
E       . For more information, enable cuDNN error logging by setting CUDNN_LOGERR_DBG=1 and CUDNN_LOGDEST_DBG=stderr in the environment.

transformer_engine/pytorch/cpp_extensions/fused_attn.py:297: RuntimeError
_____ test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_2-model_configs0-dtype1] _____

dtype = torch.bfloat16
model_configs = {'max_logit_1': <utils.ModelConfig object at 0x13ab36818380>, 'max_logit_2': <utils.ModelConfig object at 0x13a92924b3...git_3': <utils.ModelConfig object at 0x13a92924b440>, 'max_logit_4': <utils.ModelConfig object at 0x13a92924b590>, ...}
model = 'max_logit_2', qkv_layout = 'sbhd_sbhd_sbhd'

    @pytest.mark.skipif(get_cudnn_version() < (8, 9, 1), reason="cuDNN 8.9.1+ is required.")
    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model_configs", [model_configs_max_logit])
    @pytest.mark.parametrize("model", model_configs_max_logit.keys())
    @pytest.mark.parametrize("qkv_layout", ["sbhd_sbhd_sbhd", "thd_thd_thd"])
    def test_dpa_max_logit(dtype, model_configs, model, qkv_layout):
        """Test DotProductAttention module with checkpointing"""
        config = model_configs[model]
        config.return_max_logit = True
>       test_dot_product_attention(dtype, model_configs, model, False, True, qkv_layout, False, False)

tests/pytorch/attention/test_attention.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/attention/test_attention.py:209: in test_dot_product_attention
    fused_attn_fwd, fused_max_logit, fused_attn_bwd = _run_dot_product_attention(
tests/pytorch/attention/test_attention.py:1168: in _run_dot_product_attention
    out = block(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1493: in forward
    return self.fused_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1916: in forward
    output = FusedAttnFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1267: in forward
    out_, aux_ctx_tensors, *max_logit = fused_attn_fwd(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

is_training = True, max_seqlen_q = 2048, max_seqlen_kv = 2048
cu_seqlens_q = tensor([   0, 2048, 4096], device='cuda:0', dtype=torch.int32)
cu_seqlens_kv = tensor([   0, 2048, 4096], device='cuda:0', dtype=torch.int32)
q = tensor([[[[-1.6211e-01,  5.7129e-02, -5.1270e-02,  ..., -1.3477e-01,
            2.6953e-01, -1.6846e-02],
          [... 9.5215e-03,
            6.5430e-02,  1.6309e-01]]]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
k = tensor([[[[ 9.3750e-02,  1.5259e-02, -8.6670e-03,  ...,  3.9551e-02,
           -1.9629e-01, -1.3086e-01],
          [...-7.5195e-02,
            6.9336e-02, -1.6211e-01]]]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
v = tensor([[[[-2.5635e-02, -5.9814e-02, -8.9844e-02,  ...,  5.4199e-02,
            6.2500e-02, -6.5918e-02],
          [...-8.5938e-02,
            1.4062e-01,  4.4678e-02]]]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
fake_dtype = torch.bfloat16
fused_attention_backend = <NVTE_Fused_Attn_Backend.NVTE_F16_arbitrary_seqlen: 1>
attn_bias = None
cu_seqlens_q_padded = tensor([   0, 2048, 4096], device='cuda:0', dtype=torch.int32)
cu_seqlens_kv_padded = tensor([   0, 2048, 4096], device='cuda:0', dtype=torch.int32)
page_table_k = None, page_table_v = None, s_quantizer = None, o_quantizer = None
attn_scale = 0.08838834764831843, dropout = 0.0, fast_zero_fill = True
qkv_layout = 'sbhd_sbhd_sbhd', attn_bias_type = 'no_bias'
attn_mask_type = 'causal', softmax_type = 'vanilla', window_size = (-1, 0)
rng_gen = None, softmax_offset = None, return_max_logit = True
cuda_graph = False

    def fused_attn_fwd(
        is_training: bool,
        max_seqlen_q: int,
        max_seqlen_kv: int,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_kv: torch.Tensor,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        fake_dtype: torch.dtype,
        fused_attention_backend: tex.NVTE_Fused_Attn_Backend,
        attn_bias: torch.Tensor = None,
        cu_seqlens_q_padded: torch.Tensor = None,
        cu_seqlens_kv_padded: torch.Tensor = None,
        page_table_k: torch.Tensor = None,
        page_table_v: torch.Tensor = None,
        s_quantizer: Quantizer = None,
        o_quantizer: Quantizer = None,
        attn_scale: float = None,
        dropout: float = 0.0,
        fast_zero_fill: bool = True,
        qkv_layout: str = "sbh3d",
        attn_bias_type: str = "no_bias",
        attn_mask_type: str = "padding",
        softmax_type: str = "vanilla",
        window_size: Tuple[int, int] = (-1, -1),
        rng_gen: torch.Generator = None,
        softmax_offset: torch.Tensor = None,
        return_max_logit: bool = False,
        cuda_graph: bool = False,
    ) -> Tuple[Union[torch.Tensor, None], ...]:
        """Fused Attention FWD for separate QKV input.
    
        Parameters
        ----------
        is_training : bool
                    if True, runs training and produces auxiliary tensors aux_ctx_tensors
                    for the backward; if False, runs inference and doesn't produce aux_ctx_tensors
        max_seqlen_q : int
                    max sequence length for Q, used for padding;
                    may be larger than max(seqlens_q),
                    seqlens_q = cu_seqlens_q[1:] - cu_seqlens_q[:-1]
        max_seqlen_kv : int
                    max sequence length for K and V, used for padding;
                    may be larger than max(seqlens_kv),
                    seqlens_kv = cu_seqlens_kv[1:] - cu_seqlens_kv[:-1]
        cu_seqlens_q : torch.Tensor
                    cumulative sequence lengths for Q; shape [batch_size + 1]
        cu_seqlens_kv : torch.Tensor
                    cumulative sequence lengths for K and V; shape [batch_size + 1]
        q : torch.Tensor
                    input tensor Q; shape sbhd, bshd or thd (see `qkv_layout` for details)
        k : torch.Tensor
                    input tensor K; shape sbhd, bshd or thd (see `qkv_layout` for details)
        v : torch.Tensor
                    input tensor V; shape sbhd, bshd or thd (see `qkv_layout` for details)
        fake_dtype : tex.DType
                    data type of Q, K and V - in case of high precision, fake dtype in case of FP8;
                    in torch.dtype
        fused_attention_backend : tex.NVTE_Fused_Attn_Backend
                    please see FusedAttention module for details on supported backends.
        attn_bias : torch.Tensor, default = None
                    input tensor Bias when attn_bias_type is "pre_scale_bias" or "post_scale_bias";
                    shape [1, num_heads, max_seqlen_q, max_seqlen_kv], same data type as q, k and v
        cu_seqlens_q_padded : torch.Tensor, default = None
                    cumulative sequence offsets for Q; shape [batch_size + 1]
        cu_seqlens_kv_padded : torch.Tensor, default = None
                    cumulative sequence offsets for KV; shape [batch_size + 1]
        page_table_k : torch.Tensor, default = None
                    page table for K cache; shape [batch_size, max_pages_per_seq_k]
        page_table_v : torch.Tensor, default = None
                    page table for V cache; shape [batch_size, max_pages_per_seq_v]
        s_quantizer : Quantizer, default = None
                    Quantizer object for the intermediate value S.
        o_quantizer : Quantizer, default = None
                    Quantizer object for the output of the attention.
        attn_scale : float, default = None
                    if not None, use attn_scale as the attention scale for Q*K.T BMM;
                    if None, use 1.0/sqrt(head_dim_qk) as the default
        dropout : float, default = 0.0
                    dropout probability, 0.0 means no dropout, 1.0 means no output;
                    dropout must be 0.0 if is_training is False
        fast_zero_fill : bool, default = True
                    if True, initializes the output tensor O to zero using the fast filling method;
                    if False, uses PyTorch's .fill_() method
        qkv_layout : str, default = "sbh3d"
                    layout of Q, K and V;
                    {"sb3hd", "sbh3d", "sbhd_sb2hd", "sbhd_sbh2d", "sbhd_sbhd_sbhd",
                    "bs3hd", "bsh3d", "bshd_bs2hd", "bshd_bsh2d", "bshd_bshd_bshd",
                    "t3hd", "th3d", "thd_t2hd", "thd_th2d", "thd_thd_thd"}
        attn_bias_type : str, default = "no_bias"
                    type of the bias; {"no_bias", "pre_scale_bias", "post_scale_bias", "alibi"}
        attn_mask_type : str, default = "padding"
                    type of the attention mask; {"padding", "causal", "padding_causal", "no_mask"}
        softmax_type : str, default = "vanilla"
                    type of the attention softmax; {"vanilla", "off-by-one", "learnable"}
        window_size : Tuple[int, int], default = (-1, -1)
                    sliding window size for local attention, where query at position i attends to keys
                    in [i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q
                    + window_size[1]] inclusive. Special cases (-1, -1) and (-1, 0) mean no sliding
                    window and causal mask specifically.
        rng_gen : torch.Generator, default = None
                    random number generator;
                    if None, uses the default CUDA generator from PyTorch; otherwise, uses rng_gen
        softmax_offset : torch.Tensor, default = None
                    softmax offset tensor of shape [1, h_q, 1, 1].
                    See softmax_type in DotProductAttention for details.
        return_max_logit : bool, default = False
                          whether to return the maximum attention score
        cuda_graph : bool, default = False
                    whether or not cuda graph capture is enabled.
    
        Returns
        ----------
        o : torch.Tensor
                    output tensor O, of the attention calculation; same data type as Q, K and V;
                    same shape as Q
        aux_ctx_tensors : List[torch.Tensor]
                    auxiliary output tensors used for the backward;
                    if is_training is True, aux_ctx_tensors = [softmax-related tensors, rng_state]
                    if is_training is False, aux_ctx_tensors = None
    
                    softmax-related tensors:
                        1. if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]
                           softmax: torch.Tensor
                               Softmax(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, max_seqlen_kv], dtype float32
                        2. if fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]
                           softmaxStats: torch.Tensor
                               log(sum(e^(x - max(x)))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                        3. if fused_attention_backend == FusedAttnBackend["FP8"]
                           M: torch.Tensor
                               max(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                           ZInv: torch.Tensor
                               1/sum(e^(x - max(x))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                    rng_state: torch.Tensor, optional, if backend is not F16_max512_seqlen
                        state of the random number generator;
                        [seed, offset], dtype uint64
        max_logit : if return_max_logit = True, shape [h] and same data type as O; otherwise None
        """
    
        if attn_scale is None:
            d = q.size(-1)
            attn_scale = 1.0 / math.sqrt(d)
    
        if attn_bias_type not in ["no_bias", "alibi"]:
            assert (
                attn_bias is not None
            ), "attn_bias tensor cannot be None when attn_bias_type is not no_bias or alibi."
            assert attn_bias.dtype == q.dtype, "attn_bias tensor must be in the same dtype as q and kv."
    
        assert (
            fused_attention_backend != FusedAttnBackend["No_Backend"]
        ), "Fused attention does not support this input combination."
    
        # BF16/FP16 fused attention API from fmha_v1 apex
        if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_kv + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
        # BF16/FP16 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]:
            rng_elts_per_thread = BACKEND_F16arb_ELTS_PER_THREADS
        # FP8 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["FP8"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_q + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
    
            assert (
                s_quantizer is not None
            ), "s_quantizer is required as an input for FP8 fused attention."
            assert (
                o_quantizer is not None
            ), "o_quantizer is required as an input for FP8 fused attention."
        else:
            raise ValueError(f"Unsupported backend {fused_attention_backend}")
    
        # execute kernel
    
>       output_tensors = tex.fused_attn_fwd(
            max_seqlen_q,
            max_seqlen_kv,
            is_training,
            attn_scale,
            dropout,
            fast_zero_fill,
            QKVLayout[qkv_layout],
            AttnBiasType[attn_bias_type],
            AttnMaskType[attn_mask_type],
            SoftmaxType[softmax_type],
            window_size,
            cu_seqlens_q,
            cu_seqlens_kv,
            q,
            k,
            v,
            fake_dtype,
            cu_seqlens_q_padded,
            cu_seqlens_kv_padded,
            page_table_k,
            page_table_v,
            s_quantizer,
            o_quantizer,
            attn_bias,
            softmax_offset,
            rng_gen,
            rng_elts_per_thread,
            return_max_logit,
            cuda_graph,
        )
E       RuntimeError: /root/TransformerEngine/transformer_engine/common/fused_attn/fused_attn_f16_arbitrary_seqlen.cu:406 in function operator(): cuDNN Error: No valid engine configs for Matmul_MUL_GEN_INDEX_GEN_INDEX_CMP_GE_BINARY_SELECT_Reduction_SUB_EXP_Reduction_DIV_Matmul_
E       {"engineId":0,"smVersion":1000,"knobChoices":{"CUDNN_KNOB_TYPE_KERNEL_CFG":0}}
E       
E       . For more information, enable cuDNN error logging by setting CUDNN_LOGERR_DBG=1 and CUDNN_LOGDEST_DBG=stderr in the environment.

transformer_engine/pytorch/cpp_extensions/fused_attn.py:297: RuntimeError
_____ test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_3-model_configs0-dtype0] _____

dtype = torch.float16
model_configs = {'max_logit_1': <utils.ModelConfig object at 0x13ab36818380>, 'max_logit_2': <utils.ModelConfig object at 0x13a92924b3...git_3': <utils.ModelConfig object at 0x13a92924b440>, 'max_logit_4': <utils.ModelConfig object at 0x13a92924b590>, ...}
model = 'max_logit_3', qkv_layout = 'sbhd_sbhd_sbhd'

    @pytest.mark.skipif(get_cudnn_version() < (8, 9, 1), reason="cuDNN 8.9.1+ is required.")
    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model_configs", [model_configs_max_logit])
    @pytest.mark.parametrize("model", model_configs_max_logit.keys())
    @pytest.mark.parametrize("qkv_layout", ["sbhd_sbhd_sbhd", "thd_thd_thd"])
    def test_dpa_max_logit(dtype, model_configs, model, qkv_layout):
        """Test DotProductAttention module with checkpointing"""
        config = model_configs[model]
        config.return_max_logit = True
>       test_dot_product_attention(dtype, model_configs, model, False, True, qkv_layout, False, False)

tests/pytorch/attention/test_attention.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/attention/test_attention.py:209: in test_dot_product_attention
    fused_attn_fwd, fused_max_logit, fused_attn_bwd = _run_dot_product_attention(
tests/pytorch/attention/test_attention.py:1168: in _run_dot_product_attention
    out = block(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1493: in forward
    return self.fused_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1916: in forward
    output = FusedAttnFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1267: in forward
    out_, aux_ctx_tensors, *max_logit = fused_attn_fwd(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

is_training = True, max_seqlen_q = 1, max_seqlen_kv = 2048
cu_seqlens_q = tensor([0, 1, 2], device='cuda:0', dtype=torch.int32)
cu_seqlens_kv = tensor([   0, 1505, 3330], device='cuda:0', dtype=torch.int32)
q = tensor([[[[ 0.0634,  0.1636, -0.0346,  ..., -0.0270,  0.0003,  0.0067],
          [ 0.0714,  0.0892,  0.2976,  ..., -0...-0.1459,  0.0488,  ...,  0.0606,  0.0105,  0.1205]]]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
k = tensor([[[[-1.6235e-02,  1.8079e-01, -1.0535e-01,  ...,  2.2385e-02,
           -5.7434e-02, -7.4097e-02],
          [... -1.3013e-01,
           -3.7022e-03, -4.8004e-02]]]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
v = tensor([[[[ 1.0077e-01, -1.4539e-01, -1.3428e-01,  ...,  9.3384e-02,
            8.7402e-02, -7.5928e-02],
          [... -3.3356e-02,
           -6.4621e-03,  1.4624e-01]]]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
fake_dtype = torch.float16
fused_attention_backend = <NVTE_Fused_Attn_Backend.NVTE_F16_arbitrary_seqlen: 1>
attn_bias = None
cu_seqlens_q_padded = tensor([0, 1, 2], device='cuda:0', dtype=torch.int32)
cu_seqlens_kv_padded = tensor([   0, 1505, 3330], device='cuda:0', dtype=torch.int32)
page_table_k = None, page_table_v = None, s_quantizer = None, o_quantizer = None
attn_scale = 0.08838834764831843, dropout = 0.0, fast_zero_fill = True
qkv_layout = 'sbhd_sbhd_sbhd', attn_bias_type = 'no_bias'
attn_mask_type = 'padding_causal', softmax_type = 'vanilla'
window_size = (-1, 0), rng_gen = None, softmax_offset = None
return_max_logit = True, cuda_graph = False

    def fused_attn_fwd(
        is_training: bool,
        max_seqlen_q: int,
        max_seqlen_kv: int,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_kv: torch.Tensor,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        fake_dtype: torch.dtype,
        fused_attention_backend: tex.NVTE_Fused_Attn_Backend,
        attn_bias: torch.Tensor = None,
        cu_seqlens_q_padded: torch.Tensor = None,
        cu_seqlens_kv_padded: torch.Tensor = None,
        page_table_k: torch.Tensor = None,
        page_table_v: torch.Tensor = None,
        s_quantizer: Quantizer = None,
        o_quantizer: Quantizer = None,
        attn_scale: float = None,
        dropout: float = 0.0,
        fast_zero_fill: bool = True,
        qkv_layout: str = "sbh3d",
        attn_bias_type: str = "no_bias",
        attn_mask_type: str = "padding",
        softmax_type: str = "vanilla",
        window_size: Tuple[int, int] = (-1, -1),
        rng_gen: torch.Generator = None,
        softmax_offset: torch.Tensor = None,
        return_max_logit: bool = False,
        cuda_graph: bool = False,
    ) -> Tuple[Union[torch.Tensor, None], ...]:
        """Fused Attention FWD for separate QKV input.
    
        Parameters
        ----------
        is_training : bool
                    if True, runs training and produces auxiliary tensors aux_ctx_tensors
                    for the backward; if False, runs inference and doesn't produce aux_ctx_tensors
        max_seqlen_q : int
                    max sequence length for Q, used for padding;
                    may be larger than max(seqlens_q),
                    seqlens_q = cu_seqlens_q[1:] - cu_seqlens_q[:-1]
        max_seqlen_kv : int
                    max sequence length for K and V, used for padding;
                    may be larger than max(seqlens_kv),
                    seqlens_kv = cu_seqlens_kv[1:] - cu_seqlens_kv[:-1]
        cu_seqlens_q : torch.Tensor
                    cumulative sequence lengths for Q; shape [batch_size + 1]
        cu_seqlens_kv : torch.Tensor
                    cumulative sequence lengths for K and V; shape [batch_size + 1]
        q : torch.Tensor
                    input tensor Q; shape sbhd, bshd or thd (see `qkv_layout` for details)
        k : torch.Tensor
                    input tensor K; shape sbhd, bshd or thd (see `qkv_layout` for details)
        v : torch.Tensor
                    input tensor V; shape sbhd, bshd or thd (see `qkv_layout` for details)
        fake_dtype : tex.DType
                    data type of Q, K and V - in case of high precision, fake dtype in case of FP8;
                    in torch.dtype
        fused_attention_backend : tex.NVTE_Fused_Attn_Backend
                    please see FusedAttention module for details on supported backends.
        attn_bias : torch.Tensor, default = None
                    input tensor Bias when attn_bias_type is "pre_scale_bias" or "post_scale_bias";
                    shape [1, num_heads, max_seqlen_q, max_seqlen_kv], same data type as q, k and v
        cu_seqlens_q_padded : torch.Tensor, default = None
                    cumulative sequence offsets for Q; shape [batch_size + 1]
        cu_seqlens_kv_padded : torch.Tensor, default = None
                    cumulative sequence offsets for KV; shape [batch_size + 1]
        page_table_k : torch.Tensor, default = None
                    page table for K cache; shape [batch_size, max_pages_per_seq_k]
        page_table_v : torch.Tensor, default = None
                    page table for V cache; shape [batch_size, max_pages_per_seq_v]
        s_quantizer : Quantizer, default = None
                    Quantizer object for the intermediate value S.
        o_quantizer : Quantizer, default = None
                    Quantizer object for the output of the attention.
        attn_scale : float, default = None
                    if not None, use attn_scale as the attention scale for Q*K.T BMM;
                    if None, use 1.0/sqrt(head_dim_qk) as the default
        dropout : float, default = 0.0
                    dropout probability, 0.0 means no dropout, 1.0 means no output;
                    dropout must be 0.0 if is_training is False
        fast_zero_fill : bool, default = True
                    if True, initializes the output tensor O to zero using the fast filling method;
                    if False, uses PyTorch's .fill_() method
        qkv_layout : str, default = "sbh3d"
                    layout of Q, K and V;
                    {"sb3hd", "sbh3d", "sbhd_sb2hd", "sbhd_sbh2d", "sbhd_sbhd_sbhd",
                    "bs3hd", "bsh3d", "bshd_bs2hd", "bshd_bsh2d", "bshd_bshd_bshd",
                    "t3hd", "th3d", "thd_t2hd", "thd_th2d", "thd_thd_thd"}
        attn_bias_type : str, default = "no_bias"
                    type of the bias; {"no_bias", "pre_scale_bias", "post_scale_bias", "alibi"}
        attn_mask_type : str, default = "padding"
                    type of the attention mask; {"padding", "causal", "padding_causal", "no_mask"}
        softmax_type : str, default = "vanilla"
                    type of the attention softmax; {"vanilla", "off-by-one", "learnable"}
        window_size : Tuple[int, int], default = (-1, -1)
                    sliding window size for local attention, where query at position i attends to keys
                    in [i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q
                    + window_size[1]] inclusive. Special cases (-1, -1) and (-1, 0) mean no sliding
                    window and causal mask specifically.
        rng_gen : torch.Generator, default = None
                    random number generator;
                    if None, uses the default CUDA generator from PyTorch; otherwise, uses rng_gen
        softmax_offset : torch.Tensor, default = None
                    softmax offset tensor of shape [1, h_q, 1, 1].
                    See softmax_type in DotProductAttention for details.
        return_max_logit : bool, default = False
                          whether to return the maximum attention score
        cuda_graph : bool, default = False
                    whether or not cuda graph capture is enabled.
    
        Returns
        ----------
        o : torch.Tensor
                    output tensor O, of the attention calculation; same data type as Q, K and V;
                    same shape as Q
        aux_ctx_tensors : List[torch.Tensor]
                    auxiliary output tensors used for the backward;
                    if is_training is True, aux_ctx_tensors = [softmax-related tensors, rng_state]
                    if is_training is False, aux_ctx_tensors = None
    
                    softmax-related tensors:
                        1. if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]
                           softmax: torch.Tensor
                               Softmax(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, max_seqlen_kv], dtype float32
                        2. if fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]
                           softmaxStats: torch.Tensor
                               log(sum(e^(x - max(x)))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                        3. if fused_attention_backend == FusedAttnBackend["FP8"]
                           M: torch.Tensor
                               max(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                           ZInv: torch.Tensor
                               1/sum(e^(x - max(x))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                    rng_state: torch.Tensor, optional, if backend is not F16_max512_seqlen
                        state of the random number generator;
                        [seed, offset], dtype uint64
        max_logit : if return_max_logit = True, shape [h] and same data type as O; otherwise None
        """
    
        if attn_scale is None:
            d = q.size(-1)
            attn_scale = 1.0 / math.sqrt(d)
    
        if attn_bias_type not in ["no_bias", "alibi"]:
            assert (
                attn_bias is not None
            ), "attn_bias tensor cannot be None when attn_bias_type is not no_bias or alibi."
            assert attn_bias.dtype == q.dtype, "attn_bias tensor must be in the same dtype as q and kv."
    
        assert (
            fused_attention_backend != FusedAttnBackend["No_Backend"]
        ), "Fused attention does not support this input combination."
    
        # BF16/FP16 fused attention API from fmha_v1 apex
        if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_kv + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
        # BF16/FP16 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]:
            rng_elts_per_thread = BACKEND_F16arb_ELTS_PER_THREADS
        # FP8 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["FP8"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_q + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
    
            assert (
                s_quantizer is not None
            ), "s_quantizer is required as an input for FP8 fused attention."
            assert (
                o_quantizer is not None
            ), "o_quantizer is required as an input for FP8 fused attention."
        else:
            raise ValueError(f"Unsupported backend {fused_attention_backend}")
    
        # execute kernel
    
>       output_tensors = tex.fused_attn_fwd(
            max_seqlen_q,
            max_seqlen_kv,
            is_training,
            attn_scale,
            dropout,
            fast_zero_fill,
            QKVLayout[qkv_layout],
            AttnBiasType[attn_bias_type],
            AttnMaskType[attn_mask_type],
            SoftmaxType[softmax_type],
            window_size,
            cu_seqlens_q,
            cu_seqlens_kv,
            q,
            k,
            v,
            fake_dtype,
            cu_seqlens_q_padded,
            cu_seqlens_kv_padded,
            page_table_k,
            page_table_v,
            s_quantizer,
            o_quantizer,
            attn_bias,
            softmax_offset,
            rng_gen,
            rng_elts_per_thread,
            return_max_logit,
            cuda_graph,
        )
E       RuntimeError: /root/TransformerEngine/transformer_engine/common/fused_attn/fused_attn_f16_arbitrary_seqlen.cu:406 in function operator(): cuDNN Error: No valid engine configs for Matmul_MUL_GEN_INDEX_GEN_INDEX_CMP_LT_CMP_LT_LOGICAL_AND_BINARY_SELECT_GEN_INDEX_GEN_INDEX_CMP_GE_BINARY_SELECT_Reduction_SUB_EXP_Reduction_DIV_Matmul_
E       {"engineId":0,"smVersion":1000,"knobChoices":{"CUDNN_KNOB_TYPE_KERNEL_CFG":0}}
E       
E       . For more information, enable cuDNN error logging by setting CUDNN_LOGERR_DBG=1 and CUDNN_LOGDEST_DBG=stderr in the environment.

transformer_engine/pytorch/cpp_extensions/fused_attn.py:297: RuntimeError
_____ test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_3-model_configs0-dtype1] _____

dtype = torch.bfloat16
model_configs = {'max_logit_1': <utils.ModelConfig object at 0x13ab36818380>, 'max_logit_2': <utils.ModelConfig object at 0x13a92924b3...git_3': <utils.ModelConfig object at 0x13a92924b440>, 'max_logit_4': <utils.ModelConfig object at 0x13a92924b590>, ...}
model = 'max_logit_3', qkv_layout = 'sbhd_sbhd_sbhd'

    @pytest.mark.skipif(get_cudnn_version() < (8, 9, 1), reason="cuDNN 8.9.1+ is required.")
    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model_configs", [model_configs_max_logit])
    @pytest.mark.parametrize("model", model_configs_max_logit.keys())
    @pytest.mark.parametrize("qkv_layout", ["sbhd_sbhd_sbhd", "thd_thd_thd"])
    def test_dpa_max_logit(dtype, model_configs, model, qkv_layout):
        """Test DotProductAttention module with checkpointing"""
        config = model_configs[model]
        config.return_max_logit = True
>       test_dot_product_attention(dtype, model_configs, model, False, True, qkv_layout, False, False)

tests/pytorch/attention/test_attention.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/attention/test_attention.py:209: in test_dot_product_attention
    fused_attn_fwd, fused_max_logit, fused_attn_bwd = _run_dot_product_attention(
tests/pytorch/attention/test_attention.py:1168: in _run_dot_product_attention
    out = block(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1493: in forward
    return self.fused_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1916: in forward
    output = FusedAttnFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1267: in forward
    out_, aux_ctx_tensors, *max_logit = fused_attn_fwd(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

is_training = True, max_seqlen_q = 1, max_seqlen_kv = 2048
cu_seqlens_q = tensor([0, 1, 2], device='cuda:0', dtype=torch.int32)
cu_seqlens_kv = tensor([   0, 1505, 3330], device='cuda:0', dtype=torch.int32)
q = tensor([[[[ 0.0635,  0.1631, -0.0347,  ..., -0.0270,  0.0003,  0.0067],
          [ 0.0713,  0.0889,  0.2969,  ..., -0...0.1465,  0.0488,  ...,  0.0605,  0.0105,  0.1201]]]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
k = tensor([[[[-1.6235e-02,  1.8066e-01, -1.0547e-01,  ...,  2.2339e-02,
           -5.7373e-02, -7.4219e-02],
          [...-1.3086e-01,
           -3.7079e-03, -4.8096e-02]]]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
v = tensor([[[[ 1.0059e-01, -1.4551e-01, -1.3477e-01,  ...,  9.3262e-02,
            8.7402e-02, -7.5684e-02],
          [...-3.3447e-02,
           -6.4392e-03,  1.4648e-01]]]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
fake_dtype = torch.bfloat16
fused_attention_backend = <NVTE_Fused_Attn_Backend.NVTE_F16_arbitrary_seqlen: 1>
attn_bias = None
cu_seqlens_q_padded = tensor([0, 1, 2], device='cuda:0', dtype=torch.int32)
cu_seqlens_kv_padded = tensor([   0, 1505, 3330], device='cuda:0', dtype=torch.int32)
page_table_k = None, page_table_v = None, s_quantizer = None, o_quantizer = None
attn_scale = 0.08838834764831843, dropout = 0.0, fast_zero_fill = True
qkv_layout = 'sbhd_sbhd_sbhd', attn_bias_type = 'no_bias'
attn_mask_type = 'padding_causal', softmax_type = 'vanilla'
window_size = (-1, 0), rng_gen = None, softmax_offset = None
return_max_logit = True, cuda_graph = False

    def fused_attn_fwd(
        is_training: bool,
        max_seqlen_q: int,
        max_seqlen_kv: int,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_kv: torch.Tensor,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        fake_dtype: torch.dtype,
        fused_attention_backend: tex.NVTE_Fused_Attn_Backend,
        attn_bias: torch.Tensor = None,
        cu_seqlens_q_padded: torch.Tensor = None,
        cu_seqlens_kv_padded: torch.Tensor = None,
        page_table_k: torch.Tensor = None,
        page_table_v: torch.Tensor = None,
        s_quantizer: Quantizer = None,
        o_quantizer: Quantizer = None,
        attn_scale: float = None,
        dropout: float = 0.0,
        fast_zero_fill: bool = True,
        qkv_layout: str = "sbh3d",
        attn_bias_type: str = "no_bias",
        attn_mask_type: str = "padding",
        softmax_type: str = "vanilla",
        window_size: Tuple[int, int] = (-1, -1),
        rng_gen: torch.Generator = None,
        softmax_offset: torch.Tensor = None,
        return_max_logit: bool = False,
        cuda_graph: bool = False,
    ) -> Tuple[Union[torch.Tensor, None], ...]:
        """Fused Attention FWD for separate QKV input.
    
        Parameters
        ----------
        is_training : bool
                    if True, runs training and produces auxiliary tensors aux_ctx_tensors
                    for the backward; if False, runs inference and doesn't produce aux_ctx_tensors
        max_seqlen_q : int
                    max sequence length for Q, used for padding;
                    may be larger than max(seqlens_q),
                    seqlens_q = cu_seqlens_q[1:] - cu_seqlens_q[:-1]
        max_seqlen_kv : int
                    max sequence length for K and V, used for padding;
                    may be larger than max(seqlens_kv),
                    seqlens_kv = cu_seqlens_kv[1:] - cu_seqlens_kv[:-1]
        cu_seqlens_q : torch.Tensor
                    cumulative sequence lengths for Q; shape [batch_size + 1]
        cu_seqlens_kv : torch.Tensor
                    cumulative sequence lengths for K and V; shape [batch_size + 1]
        q : torch.Tensor
                    input tensor Q; shape sbhd, bshd or thd (see `qkv_layout` for details)
        k : torch.Tensor
                    input tensor K; shape sbhd, bshd or thd (see `qkv_layout` for details)
        v : torch.Tensor
                    input tensor V; shape sbhd, bshd or thd (see `qkv_layout` for details)
        fake_dtype : tex.DType
                    data type of Q, K and V - in case of high precision, fake dtype in case of FP8;
                    in torch.dtype
        fused_attention_backend : tex.NVTE_Fused_Attn_Backend
                    please see FusedAttention module for details on supported backends.
        attn_bias : torch.Tensor, default = None
                    input tensor Bias when attn_bias_type is "pre_scale_bias" or "post_scale_bias";
                    shape [1, num_heads, max_seqlen_q, max_seqlen_kv], same data type as q, k and v
        cu_seqlens_q_padded : torch.Tensor, default = None
                    cumulative sequence offsets for Q; shape [batch_size + 1]
        cu_seqlens_kv_padded : torch.Tensor, default = None
                    cumulative sequence offsets for KV; shape [batch_size + 1]
        page_table_k : torch.Tensor, default = None
                    page table for K cache; shape [batch_size, max_pages_per_seq_k]
        page_table_v : torch.Tensor, default = None
                    page table for V cache; shape [batch_size, max_pages_per_seq_v]
        s_quantizer : Quantizer, default = None
                    Quantizer object for the intermediate value S.
        o_quantizer : Quantizer, default = None
                    Quantizer object for the output of the attention.
        attn_scale : float, default = None
                    if not None, use attn_scale as the attention scale for Q*K.T BMM;
                    if None, use 1.0/sqrt(head_dim_qk) as the default
        dropout : float, default = 0.0
                    dropout probability, 0.0 means no dropout, 1.0 means no output;
                    dropout must be 0.0 if is_training is False
        fast_zero_fill : bool, default = True
                    if True, initializes the output tensor O to zero using the fast filling method;
                    if False, uses PyTorch's .fill_() method
        qkv_layout : str, default = "sbh3d"
                    layout of Q, K and V;
                    {"sb3hd", "sbh3d", "sbhd_sb2hd", "sbhd_sbh2d", "sbhd_sbhd_sbhd",
                    "bs3hd", "bsh3d", "bshd_bs2hd", "bshd_bsh2d", "bshd_bshd_bshd",
                    "t3hd", "th3d", "thd_t2hd", "thd_th2d", "thd_thd_thd"}
        attn_bias_type : str, default = "no_bias"
                    type of the bias; {"no_bias", "pre_scale_bias", "post_scale_bias", "alibi"}
        attn_mask_type : str, default = "padding"
                    type of the attention mask; {"padding", "causal", "padding_causal", "no_mask"}
        softmax_type : str, default = "vanilla"
                    type of the attention softmax; {"vanilla", "off-by-one", "learnable"}
        window_size : Tuple[int, int], default = (-1, -1)
                    sliding window size for local attention, where query at position i attends to keys
                    in [i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q
                    + window_size[1]] inclusive. Special cases (-1, -1) and (-1, 0) mean no sliding
                    window and causal mask specifically.
        rng_gen : torch.Generator, default = None
                    random number generator;
                    if None, uses the default CUDA generator from PyTorch; otherwise, uses rng_gen
        softmax_offset : torch.Tensor, default = None
                    softmax offset tensor of shape [1, h_q, 1, 1].
                    See softmax_type in DotProductAttention for details.
        return_max_logit : bool, default = False
                          whether to return the maximum attention score
        cuda_graph : bool, default = False
                    whether or not cuda graph capture is enabled.
    
        Returns
        ----------
        o : torch.Tensor
                    output tensor O, of the attention calculation; same data type as Q, K and V;
                    same shape as Q
        aux_ctx_tensors : List[torch.Tensor]
                    auxiliary output tensors used for the backward;
                    if is_training is True, aux_ctx_tensors = [softmax-related tensors, rng_state]
                    if is_training is False, aux_ctx_tensors = None
    
                    softmax-related tensors:
                        1. if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]
                           softmax: torch.Tensor
                               Softmax(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, max_seqlen_kv], dtype float32
                        2. if fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]
                           softmaxStats: torch.Tensor
                               log(sum(e^(x - max(x)))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                        3. if fused_attention_backend == FusedAttnBackend["FP8"]
                           M: torch.Tensor
                               max(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                           ZInv: torch.Tensor
                               1/sum(e^(x - max(x))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                    rng_state: torch.Tensor, optional, if backend is not F16_max512_seqlen
                        state of the random number generator;
                        [seed, offset], dtype uint64
        max_logit : if return_max_logit = True, shape [h] and same data type as O; otherwise None
        """
    
        if attn_scale is None:
            d = q.size(-1)
            attn_scale = 1.0 / math.sqrt(d)
    
        if attn_bias_type not in ["no_bias", "alibi"]:
            assert (
                attn_bias is not None
            ), "attn_bias tensor cannot be None when attn_bias_type is not no_bias or alibi."
            assert attn_bias.dtype == q.dtype, "attn_bias tensor must be in the same dtype as q and kv."
    
        assert (
            fused_attention_backend != FusedAttnBackend["No_Backend"]
        ), "Fused attention does not support this input combination."
    
        # BF16/FP16 fused attention API from fmha_v1 apex
        if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_kv + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
        # BF16/FP16 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]:
            rng_elts_per_thread = BACKEND_F16arb_ELTS_PER_THREADS
        # FP8 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["FP8"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_q + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
    
            assert (
                s_quantizer is not None
            ), "s_quantizer is required as an input for FP8 fused attention."
            assert (
                o_quantizer is not None
            ), "o_quantizer is required as an input for FP8 fused attention."
        else:
            raise ValueError(f"Unsupported backend {fused_attention_backend}")
    
        # execute kernel
    
>       output_tensors = tex.fused_attn_fwd(
            max_seqlen_q,
            max_seqlen_kv,
            is_training,
            attn_scale,
            dropout,
            fast_zero_fill,
            QKVLayout[qkv_layout],
            AttnBiasType[attn_bias_type],
            AttnMaskType[attn_mask_type],
            SoftmaxType[softmax_type],
            window_size,
            cu_seqlens_q,
            cu_seqlens_kv,
            q,
            k,
            v,
            fake_dtype,
            cu_seqlens_q_padded,
            cu_seqlens_kv_padded,
            page_table_k,
            page_table_v,
            s_quantizer,
            o_quantizer,
            attn_bias,
            softmax_offset,
            rng_gen,
            rng_elts_per_thread,
            return_max_logit,
            cuda_graph,
        )
E       RuntimeError: /root/TransformerEngine/transformer_engine/common/fused_attn/fused_attn_f16_arbitrary_seqlen.cu:406 in function operator(): cuDNN Error: No valid engine configs for Matmul_MUL_GEN_INDEX_GEN_INDEX_CMP_LT_CMP_LT_LOGICAL_AND_BINARY_SELECT_GEN_INDEX_GEN_INDEX_CMP_GE_BINARY_SELECT_Reduction_SUB_EXP_Reduction_DIV_Matmul_
E       {"engineId":0,"smVersion":1000,"knobChoices":{"CUDNN_KNOB_TYPE_KERNEL_CFG":0}}
E       
E       . For more information, enable cuDNN error logging by setting CUDNN_LOGERR_DBG=1 and CUDNN_LOGDEST_DBG=stderr in the environment.

transformer_engine/pytorch/cpp_extensions/fused_attn.py:297: RuntimeError
_____ test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_4-model_configs0-dtype0] _____

dtype = torch.float16
model_configs = {'max_logit_1': <utils.ModelConfig object at 0x13ab36818380>, 'max_logit_2': <utils.ModelConfig object at 0x13a92924b3...git_3': <utils.ModelConfig object at 0x13a92924b440>, 'max_logit_4': <utils.ModelConfig object at 0x13a92924b590>, ...}
model = 'max_logit_4', qkv_layout = 'sbhd_sbhd_sbhd'

    @pytest.mark.skipif(get_cudnn_version() < (8, 9, 1), reason="cuDNN 8.9.1+ is required.")
    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model_configs", [model_configs_max_logit])
    @pytest.mark.parametrize("model", model_configs_max_logit.keys())
    @pytest.mark.parametrize("qkv_layout", ["sbhd_sbhd_sbhd", "thd_thd_thd"])
    def test_dpa_max_logit(dtype, model_configs, model, qkv_layout):
        """Test DotProductAttention module with checkpointing"""
        config = model_configs[model]
        config.return_max_logit = True
>       test_dot_product_attention(dtype, model_configs, model, False, True, qkv_layout, False, False)

tests/pytorch/attention/test_attention.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/attention/test_attention.py:209: in test_dot_product_attention
    fused_attn_fwd, fused_max_logit, fused_attn_bwd = _run_dot_product_attention(
tests/pytorch/attention/test_attention.py:1168: in _run_dot_product_attention
    out = block(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1493: in forward
    return self.fused_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1916: in forward
    output = FusedAttnFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1267: in forward
    out_, aux_ctx_tensors, *max_logit = fused_attn_fwd(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

is_training = False, max_seqlen_q = 128, max_seqlen_kv = 2048
cu_seqlens_q = tensor([   0,  128,  256,  384,  512,  640,  768,  896, 1024], device='cuda:0',
       dtype=torch.int32)
cu_seqlens_kv = tensor([    0,  2048,  4096,  6144,  8192, 10240, 12288, 14336, 16384],
       device='cuda:0', dtype=torch.int32)
q = tensor([[[[-1.6162e-01,  5.6824e-02, -5.1025e-02,  ...,  1.2482e-02,
            5.2307e-02, -1.6333e-01],
          [... -2.0950e-02,
           -6.1096e-02,  2.0325e-02]]]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
k = tensor([[[[ 6.3416e-02, -1.2244e-01,  2.3003e-03,  ...,  3.5217e-02,
            3.1834e-03,  1.1279e-01],
          [...  1.6077e-01,
            1.9562e-02, -1.0242e-01]]]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
v = tensor([[[[-3.3112e-02,  4.5288e-02, -8.9844e-02,  ..., -2.5436e-02,
           -7.3730e-02, -9.0881e-02],
          [... -9.9060e-02,
           -4.0680e-02,  4.0192e-02]]]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
fake_dtype = torch.float16
fused_attention_backend = <NVTE_Fused_Attn_Backend.NVTE_F16_arbitrary_seqlen: 1>
attn_bias = tensor([[[[-0.3525, -0.1298, -0.3076,  ...,  1.2012,  1.7334,  0.6899],
          [-0.0163, -0.7925,  0.5845,  ..., -0...          [ 0.9971,  0.7461, -0.9580,  ...,  0.3809, -0.7188, -1.3467]]]],
       device='cuda:0', dtype=torch.float16)
cu_seqlens_q_padded = tensor([   0,  128,  256,  384,  512,  640,  768,  896, 1024], device='cuda:0',
       dtype=torch.int32)
cu_seqlens_kv_padded = tensor([    0,  2048,  4096,  6144,  8192, 10240, 12288, 14336, 16384],
       device='cuda:0', dtype=torch.int32)
page_table_k = None, page_table_v = None, s_quantizer = None, o_quantizer = None
attn_scale = 0.07216878364870323, dropout = 0.0, fast_zero_fill = True
qkv_layout = 'sbhd_sbhd_sbhd', attn_bias_type = 'post_scale_bias'
attn_mask_type = 'no_mask', softmax_type = 'vanilla', window_size = (-1, -1)
rng_gen = None, softmax_offset = None, return_max_logit = True
cuda_graph = False

    def fused_attn_fwd(
        is_training: bool,
        max_seqlen_q: int,
        max_seqlen_kv: int,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_kv: torch.Tensor,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        fake_dtype: torch.dtype,
        fused_attention_backend: tex.NVTE_Fused_Attn_Backend,
        attn_bias: torch.Tensor = None,
        cu_seqlens_q_padded: torch.Tensor = None,
        cu_seqlens_kv_padded: torch.Tensor = None,
        page_table_k: torch.Tensor = None,
        page_table_v: torch.Tensor = None,
        s_quantizer: Quantizer = None,
        o_quantizer: Quantizer = None,
        attn_scale: float = None,
        dropout: float = 0.0,
        fast_zero_fill: bool = True,
        qkv_layout: str = "sbh3d",
        attn_bias_type: str = "no_bias",
        attn_mask_type: str = "padding",
        softmax_type: str = "vanilla",
        window_size: Tuple[int, int] = (-1, -1),
        rng_gen: torch.Generator = None,
        softmax_offset: torch.Tensor = None,
        return_max_logit: bool = False,
        cuda_graph: bool = False,
    ) -> Tuple[Union[torch.Tensor, None], ...]:
        """Fused Attention FWD for separate QKV input.
    
        Parameters
        ----------
        is_training : bool
                    if True, runs training and produces auxiliary tensors aux_ctx_tensors
                    for the backward; if False, runs inference and doesn't produce aux_ctx_tensors
        max_seqlen_q : int
                    max sequence length for Q, used for padding;
                    may be larger than max(seqlens_q),
                    seqlens_q = cu_seqlens_q[1:] - cu_seqlens_q[:-1]
        max_seqlen_kv : int
                    max sequence length for K and V, used for padding;
                    may be larger than max(seqlens_kv),
                    seqlens_kv = cu_seqlens_kv[1:] - cu_seqlens_kv[:-1]
        cu_seqlens_q : torch.Tensor
                    cumulative sequence lengths for Q; shape [batch_size + 1]
        cu_seqlens_kv : torch.Tensor
                    cumulative sequence lengths for K and V; shape [batch_size + 1]
        q : torch.Tensor
                    input tensor Q; shape sbhd, bshd or thd (see `qkv_layout` for details)
        k : torch.Tensor
                    input tensor K; shape sbhd, bshd or thd (see `qkv_layout` for details)
        v : torch.Tensor
                    input tensor V; shape sbhd, bshd or thd (see `qkv_layout` for details)
        fake_dtype : tex.DType
                    data type of Q, K and V - in case of high precision, fake dtype in case of FP8;
                    in torch.dtype
        fused_attention_backend : tex.NVTE_Fused_Attn_Backend
                    please see FusedAttention module for details on supported backends.
        attn_bias : torch.Tensor, default = None
                    input tensor Bias when attn_bias_type is "pre_scale_bias" or "post_scale_bias";
                    shape [1, num_heads, max_seqlen_q, max_seqlen_kv], same data type as q, k and v
        cu_seqlens_q_padded : torch.Tensor, default = None
                    cumulative sequence offsets for Q; shape [batch_size + 1]
        cu_seqlens_kv_padded : torch.Tensor, default = None
                    cumulative sequence offsets for KV; shape [batch_size + 1]
        page_table_k : torch.Tensor, default = None
                    page table for K cache; shape [batch_size, max_pages_per_seq_k]
        page_table_v : torch.Tensor, default = None
                    page table for V cache; shape [batch_size, max_pages_per_seq_v]
        s_quantizer : Quantizer, default = None
                    Quantizer object for the intermediate value S.
        o_quantizer : Quantizer, default = None
                    Quantizer object for the output of the attention.
        attn_scale : float, default = None
                    if not None, use attn_scale as the attention scale for Q*K.T BMM;
                    if None, use 1.0/sqrt(head_dim_qk) as the default
        dropout : float, default = 0.0
                    dropout probability, 0.0 means no dropout, 1.0 means no output;
                    dropout must be 0.0 if is_training is False
        fast_zero_fill : bool, default = True
                    if True, initializes the output tensor O to zero using the fast filling method;
                    if False, uses PyTorch's .fill_() method
        qkv_layout : str, default = "sbh3d"
                    layout of Q, K and V;
                    {"sb3hd", "sbh3d", "sbhd_sb2hd", "sbhd_sbh2d", "sbhd_sbhd_sbhd",
                    "bs3hd", "bsh3d", "bshd_bs2hd", "bshd_bsh2d", "bshd_bshd_bshd",
                    "t3hd", "th3d", "thd_t2hd", "thd_th2d", "thd_thd_thd"}
        attn_bias_type : str, default = "no_bias"
                    type of the bias; {"no_bias", "pre_scale_bias", "post_scale_bias", "alibi"}
        attn_mask_type : str, default = "padding"
                    type of the attention mask; {"padding", "causal", "padding_causal", "no_mask"}
        softmax_type : str, default = "vanilla"
                    type of the attention softmax; {"vanilla", "off-by-one", "learnable"}
        window_size : Tuple[int, int], default = (-1, -1)
                    sliding window size for local attention, where query at position i attends to keys
                    in [i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q
                    + window_size[1]] inclusive. Special cases (-1, -1) and (-1, 0) mean no sliding
                    window and causal mask specifically.
        rng_gen : torch.Generator, default = None
                    random number generator;
                    if None, uses the default CUDA generator from PyTorch; otherwise, uses rng_gen
        softmax_offset : torch.Tensor, default = None
                    softmax offset tensor of shape [1, h_q, 1, 1].
                    See softmax_type in DotProductAttention for details.
        return_max_logit : bool, default = False
                          whether to return the maximum attention score
        cuda_graph : bool, default = False
                    whether or not cuda graph capture is enabled.
    
        Returns
        ----------
        o : torch.Tensor
                    output tensor O, of the attention calculation; same data type as Q, K and V;
                    same shape as Q
        aux_ctx_tensors : List[torch.Tensor]
                    auxiliary output tensors used for the backward;
                    if is_training is True, aux_ctx_tensors = [softmax-related tensors, rng_state]
                    if is_training is False, aux_ctx_tensors = None
    
                    softmax-related tensors:
                        1. if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]
                           softmax: torch.Tensor
                               Softmax(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, max_seqlen_kv], dtype float32
                        2. if fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]
                           softmaxStats: torch.Tensor
                               log(sum(e^(x - max(x)))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                        3. if fused_attention_backend == FusedAttnBackend["FP8"]
                           M: torch.Tensor
                               max(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                           ZInv: torch.Tensor
                               1/sum(e^(x - max(x))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                    rng_state: torch.Tensor, optional, if backend is not F16_max512_seqlen
                        state of the random number generator;
                        [seed, offset], dtype uint64
        max_logit : if return_max_logit = True, shape [h] and same data type as O; otherwise None
        """
    
        if attn_scale is None:
            d = q.size(-1)
            attn_scale = 1.0 / math.sqrt(d)
    
        if attn_bias_type not in ["no_bias", "alibi"]:
            assert (
                attn_bias is not None
            ), "attn_bias tensor cannot be None when attn_bias_type is not no_bias or alibi."
            assert attn_bias.dtype == q.dtype, "attn_bias tensor must be in the same dtype as q and kv."
    
        assert (
            fused_attention_backend != FusedAttnBackend["No_Backend"]
        ), "Fused attention does not support this input combination."
    
        # BF16/FP16 fused attention API from fmha_v1 apex
        if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_kv + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
        # BF16/FP16 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]:
            rng_elts_per_thread = BACKEND_F16arb_ELTS_PER_THREADS
        # FP8 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["FP8"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_q + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
    
            assert (
                s_quantizer is not None
            ), "s_quantizer is required as an input for FP8 fused attention."
            assert (
                o_quantizer is not None
            ), "o_quantizer is required as an input for FP8 fused attention."
        else:
            raise ValueError(f"Unsupported backend {fused_attention_backend}")
    
        # execute kernel
    
>       output_tensors = tex.fused_attn_fwd(
            max_seqlen_q,
            max_seqlen_kv,
            is_training,
            attn_scale,
            dropout,
            fast_zero_fill,
            QKVLayout[qkv_layout],
            AttnBiasType[attn_bias_type],
            AttnMaskType[attn_mask_type],
            SoftmaxType[softmax_type],
            window_size,
            cu_seqlens_q,
            cu_seqlens_kv,
            q,
            k,
            v,
            fake_dtype,
            cu_seqlens_q_padded,
            cu_seqlens_kv_padded,
            page_table_k,
            page_table_v,
            s_quantizer,
            o_quantizer,
            attn_bias,
            softmax_offset,
            rng_gen,
            rng_elts_per_thread,
            return_max_logit,
            cuda_graph,
        )
E       RuntimeError: /root/TransformerEngine/transformer_engine/common/fused_attn/fused_attn_f16_arbitrary_seqlen.cu:406 in function operator(): cuDNN Error: No valid engine configs for Matmul_MUL_ADD_Reduction_SUB_EXP_Reduction_DIV_Matmul_
E       {"engineId":0,"smVersion":1000,"knobChoices":{"CUDNN_KNOB_TYPE_KERNEL_CFG":0}}
E       
E       . For more information, enable cuDNN error logging by setting CUDNN_LOGERR_DBG=1 and CUDNN_LOGDEST_DBG=stderr in the environment.

transformer_engine/pytorch/cpp_extensions/fused_attn.py:297: RuntimeError
_____ test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_4-model_configs0-dtype1] _____

dtype = torch.bfloat16
model_configs = {'max_logit_1': <utils.ModelConfig object at 0x13ab36818380>, 'max_logit_2': <utils.ModelConfig object at 0x13a92924b3...git_3': <utils.ModelConfig object at 0x13a92924b440>, 'max_logit_4': <utils.ModelConfig object at 0x13a92924b590>, ...}
model = 'max_logit_4', qkv_layout = 'sbhd_sbhd_sbhd'

    @pytest.mark.skipif(get_cudnn_version() < (8, 9, 1), reason="cuDNN 8.9.1+ is required.")
    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model_configs", [model_configs_max_logit])
    @pytest.mark.parametrize("model", model_configs_max_logit.keys())
    @pytest.mark.parametrize("qkv_layout", ["sbhd_sbhd_sbhd", "thd_thd_thd"])
    def test_dpa_max_logit(dtype, model_configs, model, qkv_layout):
        """Test DotProductAttention module with checkpointing"""
        config = model_configs[model]
        config.return_max_logit = True
>       test_dot_product_attention(dtype, model_configs, model, False, True, qkv_layout, False, False)

tests/pytorch/attention/test_attention.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/attention/test_attention.py:209: in test_dot_product_attention
    fused_attn_fwd, fused_max_logit, fused_attn_bwd = _run_dot_product_attention(
tests/pytorch/attention/test_attention.py:1168: in _run_dot_product_attention
    out = block(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1493: in forward
    return self.fused_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1916: in forward
    output = FusedAttnFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1267: in forward
    out_, aux_ctx_tensors, *max_logit = fused_attn_fwd(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

is_training = False, max_seqlen_q = 128, max_seqlen_kv = 2048
cu_seqlens_q = tensor([   0,  128,  256,  384,  512,  640,  768,  896, 1024], device='cuda:0',
       dtype=torch.int32)
cu_seqlens_kv = tensor([    0,  2048,  4096,  6144,  8192, 10240, 12288, 14336, 16384],
       device='cuda:0', dtype=torch.int32)
q = tensor([[[[-1.6211e-01,  5.7129e-02, -5.1270e-02,  ...,  1.2512e-02,
            5.2246e-02, -1.6309e-01],
          [...-2.0996e-02,
           -6.1035e-02,  2.0264e-02]]]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
k = tensor([[[[ 6.3477e-02, -1.2256e-01,  2.3041e-03,  ...,  3.5156e-02,
            3.1738e-03,  1.1230e-01],
          [... 1.6113e-01,
            1.9531e-02, -1.0254e-01]]]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
v = tensor([[[[-3.3203e-02,  4.5410e-02, -8.9844e-02,  ..., -2.5391e-02,
           -7.3730e-02, -9.0820e-02],
          [...-9.9121e-02,
           -4.0527e-02,  4.0283e-02]]]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
fake_dtype = torch.bfloat16
fused_attention_backend = <NVTE_Fused_Attn_Backend.NVTE_F16_arbitrary_seqlen: 1>
attn_bias = tensor([[[[-0.3516, -0.1299, -0.3066,  ...,  1.2031,  1.7344,  0.6914],
          [-0.0162, -0.7930,  0.5859,  ..., -0...         [ 0.9961,  0.7461, -0.9570,  ...,  0.3809, -0.7188, -1.3438]]]],
       device='cuda:0', dtype=torch.bfloat16)
cu_seqlens_q_padded = tensor([   0,  128,  256,  384,  512,  640,  768,  896, 1024], device='cuda:0',
       dtype=torch.int32)
cu_seqlens_kv_padded = tensor([    0,  2048,  4096,  6144,  8192, 10240, 12288, 14336, 16384],
       device='cuda:0', dtype=torch.int32)
page_table_k = None, page_table_v = None, s_quantizer = None, o_quantizer = None
attn_scale = 0.07216878364870323, dropout = 0.0, fast_zero_fill = True
qkv_layout = 'sbhd_sbhd_sbhd', attn_bias_type = 'post_scale_bias'
attn_mask_type = 'no_mask', softmax_type = 'vanilla', window_size = (-1, -1)
rng_gen = None, softmax_offset = None, return_max_logit = True
cuda_graph = False

    def fused_attn_fwd(
        is_training: bool,
        max_seqlen_q: int,
        max_seqlen_kv: int,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_kv: torch.Tensor,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        fake_dtype: torch.dtype,
        fused_attention_backend: tex.NVTE_Fused_Attn_Backend,
        attn_bias: torch.Tensor = None,
        cu_seqlens_q_padded: torch.Tensor = None,
        cu_seqlens_kv_padded: torch.Tensor = None,
        page_table_k: torch.Tensor = None,
        page_table_v: torch.Tensor = None,
        s_quantizer: Quantizer = None,
        o_quantizer: Quantizer = None,
        attn_scale: float = None,
        dropout: float = 0.0,
        fast_zero_fill: bool = True,
        qkv_layout: str = "sbh3d",
        attn_bias_type: str = "no_bias",
        attn_mask_type: str = "padding",
        softmax_type: str = "vanilla",
        window_size: Tuple[int, int] = (-1, -1),
        rng_gen: torch.Generator = None,
        softmax_offset: torch.Tensor = None,
        return_max_logit: bool = False,
        cuda_graph: bool = False,
    ) -> Tuple[Union[torch.Tensor, None], ...]:
        """Fused Attention FWD for separate QKV input.
    
        Parameters
        ----------
        is_training : bool
                    if True, runs training and produces auxiliary tensors aux_ctx_tensors
                    for the backward; if False, runs inference and doesn't produce aux_ctx_tensors
        max_seqlen_q : int
                    max sequence length for Q, used for padding;
                    may be larger than max(seqlens_q),
                    seqlens_q = cu_seqlens_q[1:] - cu_seqlens_q[:-1]
        max_seqlen_kv : int
                    max sequence length for K and V, used for padding;
                    may be larger than max(seqlens_kv),
                    seqlens_kv = cu_seqlens_kv[1:] - cu_seqlens_kv[:-1]
        cu_seqlens_q : torch.Tensor
                    cumulative sequence lengths for Q; shape [batch_size + 1]
        cu_seqlens_kv : torch.Tensor
                    cumulative sequence lengths for K and V; shape [batch_size + 1]
        q : torch.Tensor
                    input tensor Q; shape sbhd, bshd or thd (see `qkv_layout` for details)
        k : torch.Tensor
                    input tensor K; shape sbhd, bshd or thd (see `qkv_layout` for details)
        v : torch.Tensor
                    input tensor V; shape sbhd, bshd or thd (see `qkv_layout` for details)
        fake_dtype : tex.DType
                    data type of Q, K and V - in case of high precision, fake dtype in case of FP8;
                    in torch.dtype
        fused_attention_backend : tex.NVTE_Fused_Attn_Backend
                    please see FusedAttention module for details on supported backends.
        attn_bias : torch.Tensor, default = None
                    input tensor Bias when attn_bias_type is "pre_scale_bias" or "post_scale_bias";
                    shape [1, num_heads, max_seqlen_q, max_seqlen_kv], same data type as q, k and v
        cu_seqlens_q_padded : torch.Tensor, default = None
                    cumulative sequence offsets for Q; shape [batch_size + 1]
        cu_seqlens_kv_padded : torch.Tensor, default = None
                    cumulative sequence offsets for KV; shape [batch_size + 1]
        page_table_k : torch.Tensor, default = None
                    page table for K cache; shape [batch_size, max_pages_per_seq_k]
        page_table_v : torch.Tensor, default = None
                    page table for V cache; shape [batch_size, max_pages_per_seq_v]
        s_quantizer : Quantizer, default = None
                    Quantizer object for the intermediate value S.
        o_quantizer : Quantizer, default = None
                    Quantizer object for the output of the attention.
        attn_scale : float, default = None
                    if not None, use attn_scale as the attention scale for Q*K.T BMM;
                    if None, use 1.0/sqrt(head_dim_qk) as the default
        dropout : float, default = 0.0
                    dropout probability, 0.0 means no dropout, 1.0 means no output;
                    dropout must be 0.0 if is_training is False
        fast_zero_fill : bool, default = True
                    if True, initializes the output tensor O to zero using the fast filling method;
                    if False, uses PyTorch's .fill_() method
        qkv_layout : str, default = "sbh3d"
                    layout of Q, K and V;
                    {"sb3hd", "sbh3d", "sbhd_sb2hd", "sbhd_sbh2d", "sbhd_sbhd_sbhd",
                    "bs3hd", "bsh3d", "bshd_bs2hd", "bshd_bsh2d", "bshd_bshd_bshd",
                    "t3hd", "th3d", "thd_t2hd", "thd_th2d", "thd_thd_thd"}
        attn_bias_type : str, default = "no_bias"
                    type of the bias; {"no_bias", "pre_scale_bias", "post_scale_bias", "alibi"}
        attn_mask_type : str, default = "padding"
                    type of the attention mask; {"padding", "causal", "padding_causal", "no_mask"}
        softmax_type : str, default = "vanilla"
                    type of the attention softmax; {"vanilla", "off-by-one", "learnable"}
        window_size : Tuple[int, int], default = (-1, -1)
                    sliding window size for local attention, where query at position i attends to keys
                    in [i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q
                    + window_size[1]] inclusive. Special cases (-1, -1) and (-1, 0) mean no sliding
                    window and causal mask specifically.
        rng_gen : torch.Generator, default = None
                    random number generator;
                    if None, uses the default CUDA generator from PyTorch; otherwise, uses rng_gen
        softmax_offset : torch.Tensor, default = None
                    softmax offset tensor of shape [1, h_q, 1, 1].
                    See softmax_type in DotProductAttention for details.
        return_max_logit : bool, default = False
                          whether to return the maximum attention score
        cuda_graph : bool, default = False
                    whether or not cuda graph capture is enabled.
    
        Returns
        ----------
        o : torch.Tensor
                    output tensor O, of the attention calculation; same data type as Q, K and V;
                    same shape as Q
        aux_ctx_tensors : List[torch.Tensor]
                    auxiliary output tensors used for the backward;
                    if is_training is True, aux_ctx_tensors = [softmax-related tensors, rng_state]
                    if is_training is False, aux_ctx_tensors = None
    
                    softmax-related tensors:
                        1. if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]
                           softmax: torch.Tensor
                               Softmax(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, max_seqlen_kv], dtype float32
                        2. if fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]
                           softmaxStats: torch.Tensor
                               log(sum(e^(x - max(x)))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                        3. if fused_attention_backend == FusedAttnBackend["FP8"]
                           M: torch.Tensor
                               max(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                           ZInv: torch.Tensor
                               1/sum(e^(x - max(x))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                    rng_state: torch.Tensor, optional, if backend is not F16_max512_seqlen
                        state of the random number generator;
                        [seed, offset], dtype uint64
        max_logit : if return_max_logit = True, shape [h] and same data type as O; otherwise None
        """
    
        if attn_scale is None:
            d = q.size(-1)
            attn_scale = 1.0 / math.sqrt(d)
    
        if attn_bias_type not in ["no_bias", "alibi"]:
            assert (
                attn_bias is not None
            ), "attn_bias tensor cannot be None when attn_bias_type is not no_bias or alibi."
            assert attn_bias.dtype == q.dtype, "attn_bias tensor must be in the same dtype as q and kv."
    
        assert (
            fused_attention_backend != FusedAttnBackend["No_Backend"]
        ), "Fused attention does not support this input combination."
    
        # BF16/FP16 fused attention API from fmha_v1 apex
        if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_kv + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
        # BF16/FP16 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]:
            rng_elts_per_thread = BACKEND_F16arb_ELTS_PER_THREADS
        # FP8 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["FP8"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_q + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
    
            assert (
                s_quantizer is not None
            ), "s_quantizer is required as an input for FP8 fused attention."
            assert (
                o_quantizer is not None
            ), "o_quantizer is required as an input for FP8 fused attention."
        else:
            raise ValueError(f"Unsupported backend {fused_attention_backend}")
    
        # execute kernel
    
>       output_tensors = tex.fused_attn_fwd(
            max_seqlen_q,
            max_seqlen_kv,
            is_training,
            attn_scale,
            dropout,
            fast_zero_fill,
            QKVLayout[qkv_layout],
            AttnBiasType[attn_bias_type],
            AttnMaskType[attn_mask_type],
            SoftmaxType[softmax_type],
            window_size,
            cu_seqlens_q,
            cu_seqlens_kv,
            q,
            k,
            v,
            fake_dtype,
            cu_seqlens_q_padded,
            cu_seqlens_kv_padded,
            page_table_k,
            page_table_v,
            s_quantizer,
            o_quantizer,
            attn_bias,
            softmax_offset,
            rng_gen,
            rng_elts_per_thread,
            return_max_logit,
            cuda_graph,
        )
E       RuntimeError: /root/TransformerEngine/transformer_engine/common/fused_attn/fused_attn_f16_arbitrary_seqlen.cu:406 in function operator(): cuDNN Error: No valid engine configs for Matmul_MUL_ADD_Reduction_SUB_EXP_Reduction_DIV_Matmul_
E       {"engineId":0,"smVersion":1000,"knobChoices":{"CUDNN_KNOB_TYPE_KERNEL_CFG":0}}
E       
E       . For more information, enable cuDNN error logging by setting CUDNN_LOGERR_DBG=1 and CUDNN_LOGDEST_DBG=stderr in the environment.

transformer_engine/pytorch/cpp_extensions/fused_attn.py:297: RuntimeError
_____ test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_5-model_configs0-dtype0] _____

dtype = torch.float16
model_configs = {'max_logit_1': <utils.ModelConfig object at 0x13ab36818380>, 'max_logit_2': <utils.ModelConfig object at 0x13a92924b3...git_3': <utils.ModelConfig object at 0x13a92924b440>, 'max_logit_4': <utils.ModelConfig object at 0x13a92924b590>, ...}
model = 'max_logit_5', qkv_layout = 'sbhd_sbhd_sbhd'

    @pytest.mark.skipif(get_cudnn_version() < (8, 9, 1), reason="cuDNN 8.9.1+ is required.")
    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model_configs", [model_configs_max_logit])
    @pytest.mark.parametrize("model", model_configs_max_logit.keys())
    @pytest.mark.parametrize("qkv_layout", ["sbhd_sbhd_sbhd", "thd_thd_thd"])
    def test_dpa_max_logit(dtype, model_configs, model, qkv_layout):
        """Test DotProductAttention module with checkpointing"""
        config = model_configs[model]
        config.return_max_logit = True
>       test_dot_product_attention(dtype, model_configs, model, False, True, qkv_layout, False, False)

tests/pytorch/attention/test_attention.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/attention/test_attention.py:209: in test_dot_product_attention
    fused_attn_fwd, fused_max_logit, fused_attn_bwd = _run_dot_product_attention(
tests/pytorch/attention/test_attention.py:1168: in _run_dot_product_attention
    out = block(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1493: in forward
    return self.fused_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1916: in forward
    output = FusedAttnFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1267: in forward
    out_, aux_ctx_tensors, *max_logit = fused_attn_fwd(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

is_training = False, max_seqlen_q = 128, max_seqlen_kv = 2048
cu_seqlens_q = tensor([   0,  128,  256,  384,  512,  640,  768,  896, 1024], device='cuda:0',
       dtype=torch.int32)
cu_seqlens_kv = tensor([    0,  2048,  4096,  6144,  8192, 10240, 12288, 14336, 16384],
       device='cuda:0', dtype=torch.int32)
q = tensor([[[[-1.6162e-01,  5.6824e-02, -5.1025e-02,  ..., -5.9082e-02,
           -4.1428e-03, -6.9519e-02],
          [...  1.3269e-01,
            1.0632e-01,  7.4036e-02]]]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
k = tensor([[[[ 3.4821e-02, -9.7046e-02, -1.4307e-01,  ...,  1.1816e-01,
            7.5745e-02, -2.2888e-02],
          [... -2.8717e-02,
           -1.9791e-02, -2.3758e-02]]]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
v = tensor([[[[ 8.6060e-03,  3.8605e-02, -1.0022e-01,  ..., -3.0838e-02,
           -9.3933e-02,  9.5520e-02],
          [...  1.8646e-02,
           -1.1115e-01, -5.7465e-02]]]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
fake_dtype = torch.float16
fused_attention_backend = <NVTE_Fused_Attn_Backend.NVTE_F16_arbitrary_seqlen: 1>
attn_bias = None
cu_seqlens_q_padded = tensor([   0,  128,  256,  384,  512,  640,  768,  896, 1024], device='cuda:0',
       dtype=torch.int32)
cu_seqlens_kv_padded = tensor([    0,  2048,  4096,  6144,  8192, 10240, 12288, 14336, 16384],
       device='cuda:0', dtype=torch.int32)
page_table_k = None, page_table_v = None, s_quantizer = None, o_quantizer = None
attn_scale = 0.044194173824159216, dropout = 0.0, fast_zero_fill = True
qkv_layout = 'sbhd_sbhd_sbhd', attn_bias_type = 'no_bias'
attn_mask_type = 'causal', softmax_type = 'vanilla', window_size = (20, 0)
rng_gen = None, softmax_offset = None, return_max_logit = True
cuda_graph = False

    def fused_attn_fwd(
        is_training: bool,
        max_seqlen_q: int,
        max_seqlen_kv: int,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_kv: torch.Tensor,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        fake_dtype: torch.dtype,
        fused_attention_backend: tex.NVTE_Fused_Attn_Backend,
        attn_bias: torch.Tensor = None,
        cu_seqlens_q_padded: torch.Tensor = None,
        cu_seqlens_kv_padded: torch.Tensor = None,
        page_table_k: torch.Tensor = None,
        page_table_v: torch.Tensor = None,
        s_quantizer: Quantizer = None,
        o_quantizer: Quantizer = None,
        attn_scale: float = None,
        dropout: float = 0.0,
        fast_zero_fill: bool = True,
        qkv_layout: str = "sbh3d",
        attn_bias_type: str = "no_bias",
        attn_mask_type: str = "padding",
        softmax_type: str = "vanilla",
        window_size: Tuple[int, int] = (-1, -1),
        rng_gen: torch.Generator = None,
        softmax_offset: torch.Tensor = None,
        return_max_logit: bool = False,
        cuda_graph: bool = False,
    ) -> Tuple[Union[torch.Tensor, None], ...]:
        """Fused Attention FWD for separate QKV input.
    
        Parameters
        ----------
        is_training : bool
                    if True, runs training and produces auxiliary tensors aux_ctx_tensors
                    for the backward; if False, runs inference and doesn't produce aux_ctx_tensors
        max_seqlen_q : int
                    max sequence length for Q, used for padding;
                    may be larger than max(seqlens_q),
                    seqlens_q = cu_seqlens_q[1:] - cu_seqlens_q[:-1]
        max_seqlen_kv : int
                    max sequence length for K and V, used for padding;
                    may be larger than max(seqlens_kv),
                    seqlens_kv = cu_seqlens_kv[1:] - cu_seqlens_kv[:-1]
        cu_seqlens_q : torch.Tensor
                    cumulative sequence lengths for Q; shape [batch_size + 1]
        cu_seqlens_kv : torch.Tensor
                    cumulative sequence lengths for K and V; shape [batch_size + 1]
        q : torch.Tensor
                    input tensor Q; shape sbhd, bshd or thd (see `qkv_layout` for details)
        k : torch.Tensor
                    input tensor K; shape sbhd, bshd or thd (see `qkv_layout` for details)
        v : torch.Tensor
                    input tensor V; shape sbhd, bshd or thd (see `qkv_layout` for details)
        fake_dtype : tex.DType
                    data type of Q, K and V - in case of high precision, fake dtype in case of FP8;
                    in torch.dtype
        fused_attention_backend : tex.NVTE_Fused_Attn_Backend
                    please see FusedAttention module for details on supported backends.
        attn_bias : torch.Tensor, default = None
                    input tensor Bias when attn_bias_type is "pre_scale_bias" or "post_scale_bias";
                    shape [1, num_heads, max_seqlen_q, max_seqlen_kv], same data type as q, k and v
        cu_seqlens_q_padded : torch.Tensor, default = None
                    cumulative sequence offsets for Q; shape [batch_size + 1]
        cu_seqlens_kv_padded : torch.Tensor, default = None
                    cumulative sequence offsets for KV; shape [batch_size + 1]
        page_table_k : torch.Tensor, default = None
                    page table for K cache; shape [batch_size, max_pages_per_seq_k]
        page_table_v : torch.Tensor, default = None
                    page table for V cache; shape [batch_size, max_pages_per_seq_v]
        s_quantizer : Quantizer, default = None
                    Quantizer object for the intermediate value S.
        o_quantizer : Quantizer, default = None
                    Quantizer object for the output of the attention.
        attn_scale : float, default = None
                    if not None, use attn_scale as the attention scale for Q*K.T BMM;
                    if None, use 1.0/sqrt(head_dim_qk) as the default
        dropout : float, default = 0.0
                    dropout probability, 0.0 means no dropout, 1.0 means no output;
                    dropout must be 0.0 if is_training is False
        fast_zero_fill : bool, default = True
                    if True, initializes the output tensor O to zero using the fast filling method;
                    if False, uses PyTorch's .fill_() method
        qkv_layout : str, default = "sbh3d"
                    layout of Q, K and V;
                    {"sb3hd", "sbh3d", "sbhd_sb2hd", "sbhd_sbh2d", "sbhd_sbhd_sbhd",
                    "bs3hd", "bsh3d", "bshd_bs2hd", "bshd_bsh2d", "bshd_bshd_bshd",
                    "t3hd", "th3d", "thd_t2hd", "thd_th2d", "thd_thd_thd"}
        attn_bias_type : str, default = "no_bias"
                    type of the bias; {"no_bias", "pre_scale_bias", "post_scale_bias", "alibi"}
        attn_mask_type : str, default = "padding"
                    type of the attention mask; {"padding", "causal", "padding_causal", "no_mask"}
        softmax_type : str, default = "vanilla"
                    type of the attention softmax; {"vanilla", "off-by-one", "learnable"}
        window_size : Tuple[int, int], default = (-1, -1)
                    sliding window size for local attention, where query at position i attends to keys
                    in [i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q
                    + window_size[1]] inclusive. Special cases (-1, -1) and (-1, 0) mean no sliding
                    window and causal mask specifically.
        rng_gen : torch.Generator, default = None
                    random number generator;
                    if None, uses the default CUDA generator from PyTorch; otherwise, uses rng_gen
        softmax_offset : torch.Tensor, default = None
                    softmax offset tensor of shape [1, h_q, 1, 1].
                    See softmax_type in DotProductAttention for details.
        return_max_logit : bool, default = False
                          whether to return the maximum attention score
        cuda_graph : bool, default = False
                    whether or not cuda graph capture is enabled.
    
        Returns
        ----------
        o : torch.Tensor
                    output tensor O, of the attention calculation; same data type as Q, K and V;
                    same shape as Q
        aux_ctx_tensors : List[torch.Tensor]
                    auxiliary output tensors used for the backward;
                    if is_training is True, aux_ctx_tensors = [softmax-related tensors, rng_state]
                    if is_training is False, aux_ctx_tensors = None
    
                    softmax-related tensors:
                        1. if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]
                           softmax: torch.Tensor
                               Softmax(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, max_seqlen_kv], dtype float32
                        2. if fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]
                           softmaxStats: torch.Tensor
                               log(sum(e^(x - max(x)))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                        3. if fused_attention_backend == FusedAttnBackend["FP8"]
                           M: torch.Tensor
                               max(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                           ZInv: torch.Tensor
                               1/sum(e^(x - max(x))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                    rng_state: torch.Tensor, optional, if backend is not F16_max512_seqlen
                        state of the random number generator;
                        [seed, offset], dtype uint64
        max_logit : if return_max_logit = True, shape [h] and same data type as O; otherwise None
        """
    
        if attn_scale is None:
            d = q.size(-1)
            attn_scale = 1.0 / math.sqrt(d)
    
        if attn_bias_type not in ["no_bias", "alibi"]:
            assert (
                attn_bias is not None
            ), "attn_bias tensor cannot be None when attn_bias_type is not no_bias or alibi."
            assert attn_bias.dtype == q.dtype, "attn_bias tensor must be in the same dtype as q and kv."
    
        assert (
            fused_attention_backend != FusedAttnBackend["No_Backend"]
        ), "Fused attention does not support this input combination."
    
        # BF16/FP16 fused attention API from fmha_v1 apex
        if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_kv + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
        # BF16/FP16 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]:
            rng_elts_per_thread = BACKEND_F16arb_ELTS_PER_THREADS
        # FP8 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["FP8"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_q + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
    
            assert (
                s_quantizer is not None
            ), "s_quantizer is required as an input for FP8 fused attention."
            assert (
                o_quantizer is not None
            ), "o_quantizer is required as an input for FP8 fused attention."
        else:
            raise ValueError(f"Unsupported backend {fused_attention_backend}")
    
        # execute kernel
    
>       output_tensors = tex.fused_attn_fwd(
            max_seqlen_q,
            max_seqlen_kv,
            is_training,
            attn_scale,
            dropout,
            fast_zero_fill,
            QKVLayout[qkv_layout],
            AttnBiasType[attn_bias_type],
            AttnMaskType[attn_mask_type],
            SoftmaxType[softmax_type],
            window_size,
            cu_seqlens_q,
            cu_seqlens_kv,
            q,
            k,
            v,
            fake_dtype,
            cu_seqlens_q_padded,
            cu_seqlens_kv_padded,
            page_table_k,
            page_table_v,
            s_quantizer,
            o_quantizer,
            attn_bias,
            softmax_offset,
            rng_gen,
            rng_elts_per_thread,
            return_max_logit,
            cuda_graph,
        )
E       RuntimeError: /root/TransformerEngine/transformer_engine/common/fused_attn/fused_attn_f16_arbitrary_seqlen.cu:406 in function operator(): cuDNN Error: No valid engine configs for Matmul_MUL_GEN_INDEX_GEN_INDEX_CMP_GE_BINARY_SELECT_GEN_INDEX_GEN_INDEX_ADD_CMP_GT_BINARY_SELECT_Reduction_SUB_EXP_Reduction_DIV_Matmul_
E       {"engineId":0,"smVersion":1000,"knobChoices":{"CUDNN_KNOB_TYPE_KERNEL_CFG":0}}
E       
E       . For more information, enable cuDNN error logging by setting CUDNN_LOGERR_DBG=1 and CUDNN_LOGDEST_DBG=stderr in the environment.

transformer_engine/pytorch/cpp_extensions/fused_attn.py:297: RuntimeError
_____ test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_5-model_configs0-dtype1] _____

dtype = torch.bfloat16
model_configs = {'max_logit_1': <utils.ModelConfig object at 0x13ab36818380>, 'max_logit_2': <utils.ModelConfig object at 0x13a92924b3...git_3': <utils.ModelConfig object at 0x13a92924b440>, 'max_logit_4': <utils.ModelConfig object at 0x13a92924b590>, ...}
model = 'max_logit_5', qkv_layout = 'sbhd_sbhd_sbhd'

    @pytest.mark.skipif(get_cudnn_version() < (8, 9, 1), reason="cuDNN 8.9.1+ is required.")
    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model_configs", [model_configs_max_logit])
    @pytest.mark.parametrize("model", model_configs_max_logit.keys())
    @pytest.mark.parametrize("qkv_layout", ["sbhd_sbhd_sbhd", "thd_thd_thd"])
    def test_dpa_max_logit(dtype, model_configs, model, qkv_layout):
        """Test DotProductAttention module with checkpointing"""
        config = model_configs[model]
        config.return_max_logit = True
>       test_dot_product_attention(dtype, model_configs, model, False, True, qkv_layout, False, False)

tests/pytorch/attention/test_attention.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/attention/test_attention.py:209: in test_dot_product_attention
    fused_attn_fwd, fused_max_logit, fused_attn_bwd = _run_dot_product_attention(
tests/pytorch/attention/test_attention.py:1168: in _run_dot_product_attention
    out = block(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1493: in forward
    return self.fused_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1916: in forward
    output = FusedAttnFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1267: in forward
    out_, aux_ctx_tensors, *max_logit = fused_attn_fwd(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

is_training = False, max_seqlen_q = 128, max_seqlen_kv = 2048
cu_seqlens_q = tensor([   0,  128,  256,  384,  512,  640,  768,  896, 1024], device='cuda:0',
       dtype=torch.int32)
cu_seqlens_kv = tensor([    0,  2048,  4096,  6144,  8192, 10240, 12288, 14336, 16384],
       device='cuda:0', dtype=torch.int32)
q = tensor([[[[-1.6211e-01,  5.7129e-02, -5.1270e-02,  ..., -5.9082e-02,
           -4.1504e-03, -6.9336e-02],
          [... 1.3281e-01,
            1.0645e-01,  7.4219e-02]]]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
k = tensor([[[[ 3.4668e-02, -9.6680e-02, -1.4258e-01,  ...,  1.1816e-01,
            7.5684e-02, -2.2827e-02],
          [...-2.8687e-02,
           -1.9775e-02, -2.3682e-02]]]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
v = tensor([[[[ 8.6060e-03,  3.8574e-02, -1.0010e-01,  ..., -3.0884e-02,
           -9.3750e-02,  9.5215e-02],
          [... 1.8677e-02,
           -1.1084e-01, -5.7373e-02]]]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
fake_dtype = torch.bfloat16
fused_attention_backend = <NVTE_Fused_Attn_Backend.NVTE_F16_arbitrary_seqlen: 1>
attn_bias = None
cu_seqlens_q_padded = tensor([   0,  128,  256,  384,  512,  640,  768,  896, 1024], device='cuda:0',
       dtype=torch.int32)
cu_seqlens_kv_padded = tensor([    0,  2048,  4096,  6144,  8192, 10240, 12288, 14336, 16384],
       device='cuda:0', dtype=torch.int32)
page_table_k = None, page_table_v = None, s_quantizer = None, o_quantizer = None
attn_scale = 0.044194173824159216, dropout = 0.0, fast_zero_fill = True
qkv_layout = 'sbhd_sbhd_sbhd', attn_bias_type = 'no_bias'
attn_mask_type = 'causal', softmax_type = 'vanilla', window_size = (20, 0)
rng_gen = None, softmax_offset = None, return_max_logit = True
cuda_graph = False

    def fused_attn_fwd(
        is_training: bool,
        max_seqlen_q: int,
        max_seqlen_kv: int,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_kv: torch.Tensor,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        fake_dtype: torch.dtype,
        fused_attention_backend: tex.NVTE_Fused_Attn_Backend,
        attn_bias: torch.Tensor = None,
        cu_seqlens_q_padded: torch.Tensor = None,
        cu_seqlens_kv_padded: torch.Tensor = None,
        page_table_k: torch.Tensor = None,
        page_table_v: torch.Tensor = None,
        s_quantizer: Quantizer = None,
        o_quantizer: Quantizer = None,
        attn_scale: float = None,
        dropout: float = 0.0,
        fast_zero_fill: bool = True,
        qkv_layout: str = "sbh3d",
        attn_bias_type: str = "no_bias",
        attn_mask_type: str = "padding",
        softmax_type: str = "vanilla",
        window_size: Tuple[int, int] = (-1, -1),
        rng_gen: torch.Generator = None,
        softmax_offset: torch.Tensor = None,
        return_max_logit: bool = False,
        cuda_graph: bool = False,
    ) -> Tuple[Union[torch.Tensor, None], ...]:
        """Fused Attention FWD for separate QKV input.
    
        Parameters
        ----------
        is_training : bool
                    if True, runs training and produces auxiliary tensors aux_ctx_tensors
                    for the backward; if False, runs inference and doesn't produce aux_ctx_tensors
        max_seqlen_q : int
                    max sequence length for Q, used for padding;
                    may be larger than max(seqlens_q),
                    seqlens_q = cu_seqlens_q[1:] - cu_seqlens_q[:-1]
        max_seqlen_kv : int
                    max sequence length for K and V, used for padding;
                    may be larger than max(seqlens_kv),
                    seqlens_kv = cu_seqlens_kv[1:] - cu_seqlens_kv[:-1]
        cu_seqlens_q : torch.Tensor
                    cumulative sequence lengths for Q; shape [batch_size + 1]
        cu_seqlens_kv : torch.Tensor
                    cumulative sequence lengths for K and V; shape [batch_size + 1]
        q : torch.Tensor
                    input tensor Q; shape sbhd, bshd or thd (see `qkv_layout` for details)
        k : torch.Tensor
                    input tensor K; shape sbhd, bshd or thd (see `qkv_layout` for details)
        v : torch.Tensor
                    input tensor V; shape sbhd, bshd or thd (see `qkv_layout` for details)
        fake_dtype : tex.DType
                    data type of Q, K and V - in case of high precision, fake dtype in case of FP8;
                    in torch.dtype
        fused_attention_backend : tex.NVTE_Fused_Attn_Backend
                    please see FusedAttention module for details on supported backends.
        attn_bias : torch.Tensor, default = None
                    input tensor Bias when attn_bias_type is "pre_scale_bias" or "post_scale_bias";
                    shape [1, num_heads, max_seqlen_q, max_seqlen_kv], same data type as q, k and v
        cu_seqlens_q_padded : torch.Tensor, default = None
                    cumulative sequence offsets for Q; shape [batch_size + 1]
        cu_seqlens_kv_padded : torch.Tensor, default = None
                    cumulative sequence offsets for KV; shape [batch_size + 1]
        page_table_k : torch.Tensor, default = None
                    page table for K cache; shape [batch_size, max_pages_per_seq_k]
        page_table_v : torch.Tensor, default = None
                    page table for V cache; shape [batch_size, max_pages_per_seq_v]
        s_quantizer : Quantizer, default = None
                    Quantizer object for the intermediate value S.
        o_quantizer : Quantizer, default = None
                    Quantizer object for the output of the attention.
        attn_scale : float, default = None
                    if not None, use attn_scale as the attention scale for Q*K.T BMM;
                    if None, use 1.0/sqrt(head_dim_qk) as the default
        dropout : float, default = 0.0
                    dropout probability, 0.0 means no dropout, 1.0 means no output;
                    dropout must be 0.0 if is_training is False
        fast_zero_fill : bool, default = True
                    if True, initializes the output tensor O to zero using the fast filling method;
                    if False, uses PyTorch's .fill_() method
        qkv_layout : str, default = "sbh3d"
                    layout of Q, K and V;
                    {"sb3hd", "sbh3d", "sbhd_sb2hd", "sbhd_sbh2d", "sbhd_sbhd_sbhd",
                    "bs3hd", "bsh3d", "bshd_bs2hd", "bshd_bsh2d", "bshd_bshd_bshd",
                    "t3hd", "th3d", "thd_t2hd", "thd_th2d", "thd_thd_thd"}
        attn_bias_type : str, default = "no_bias"
                    type of the bias; {"no_bias", "pre_scale_bias", "post_scale_bias", "alibi"}
        attn_mask_type : str, default = "padding"
                    type of the attention mask; {"padding", "causal", "padding_causal", "no_mask"}
        softmax_type : str, default = "vanilla"
                    type of the attention softmax; {"vanilla", "off-by-one", "learnable"}
        window_size : Tuple[int, int], default = (-1, -1)
                    sliding window size for local attention, where query at position i attends to keys
                    in [i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q
                    + window_size[1]] inclusive. Special cases (-1, -1) and (-1, 0) mean no sliding
                    window and causal mask specifically.
        rng_gen : torch.Generator, default = None
                    random number generator;
                    if None, uses the default CUDA generator from PyTorch; otherwise, uses rng_gen
        softmax_offset : torch.Tensor, default = None
                    softmax offset tensor of shape [1, h_q, 1, 1].
                    See softmax_type in DotProductAttention for details.
        return_max_logit : bool, default = False
                          whether to return the maximum attention score
        cuda_graph : bool, default = False
                    whether or not cuda graph capture is enabled.
    
        Returns
        ----------
        o : torch.Tensor
                    output tensor O, of the attention calculation; same data type as Q, K and V;
                    same shape as Q
        aux_ctx_tensors : List[torch.Tensor]
                    auxiliary output tensors used for the backward;
                    if is_training is True, aux_ctx_tensors = [softmax-related tensors, rng_state]
                    if is_training is False, aux_ctx_tensors = None
    
                    softmax-related tensors:
                        1. if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]
                           softmax: torch.Tensor
                               Softmax(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, max_seqlen_kv], dtype float32
                        2. if fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]
                           softmaxStats: torch.Tensor
                               log(sum(e^(x - max(x)))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                        3. if fused_attention_backend == FusedAttnBackend["FP8"]
                           M: torch.Tensor
                               max(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                           ZInv: torch.Tensor
                               1/sum(e^(x - max(x))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                    rng_state: torch.Tensor, optional, if backend is not F16_max512_seqlen
                        state of the random number generator;
                        [seed, offset], dtype uint64
        max_logit : if return_max_logit = True, shape [h] and same data type as O; otherwise None
        """
    
        if attn_scale is None:
            d = q.size(-1)
            attn_scale = 1.0 / math.sqrt(d)
    
        if attn_bias_type not in ["no_bias", "alibi"]:
            assert (
                attn_bias is not None
            ), "attn_bias tensor cannot be None when attn_bias_type is not no_bias or alibi."
            assert attn_bias.dtype == q.dtype, "attn_bias tensor must be in the same dtype as q and kv."
    
        assert (
            fused_attention_backend != FusedAttnBackend["No_Backend"]
        ), "Fused attention does not support this input combination."
    
        # BF16/FP16 fused attention API from fmha_v1 apex
        if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_kv + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
        # BF16/FP16 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]:
            rng_elts_per_thread = BACKEND_F16arb_ELTS_PER_THREADS
        # FP8 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["FP8"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_q + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
    
            assert (
                s_quantizer is not None
            ), "s_quantizer is required as an input for FP8 fused attention."
            assert (
                o_quantizer is not None
            ), "o_quantizer is required as an input for FP8 fused attention."
        else:
            raise ValueError(f"Unsupported backend {fused_attention_backend}")
    
        # execute kernel
    
>       output_tensors = tex.fused_attn_fwd(
            max_seqlen_q,
            max_seqlen_kv,
            is_training,
            attn_scale,
            dropout,
            fast_zero_fill,
            QKVLayout[qkv_layout],
            AttnBiasType[attn_bias_type],
            AttnMaskType[attn_mask_type],
            SoftmaxType[softmax_type],
            window_size,
            cu_seqlens_q,
            cu_seqlens_kv,
            q,
            k,
            v,
            fake_dtype,
            cu_seqlens_q_padded,
            cu_seqlens_kv_padded,
            page_table_k,
            page_table_v,
            s_quantizer,
            o_quantizer,
            attn_bias,
            softmax_offset,
            rng_gen,
            rng_elts_per_thread,
            return_max_logit,
            cuda_graph,
        )
E       RuntimeError: /root/TransformerEngine/transformer_engine/common/fused_attn/fused_attn_f16_arbitrary_seqlen.cu:406 in function operator(): cuDNN Error: No valid engine configs for Matmul_MUL_GEN_INDEX_GEN_INDEX_CMP_GE_BINARY_SELECT_GEN_INDEX_GEN_INDEX_ADD_CMP_GT_BINARY_SELECT_Reduction_SUB_EXP_Reduction_DIV_Matmul_
E       {"engineId":0,"smVersion":1000,"knobChoices":{"CUDNN_KNOB_TYPE_KERNEL_CFG":0}}
E       
E       . For more information, enable cuDNN error logging by setting CUDNN_LOGERR_DBG=1 and CUDNN_LOGDEST_DBG=stderr in the environment.

transformer_engine/pytorch/cpp_extensions/fused_attn.py:297: RuntimeError
_____ test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_6-model_configs0-dtype0] _____

dtype = torch.float16
model_configs = {'max_logit_1': <utils.ModelConfig object at 0x13ab36818380>, 'max_logit_2': <utils.ModelConfig object at 0x13a92924b3...git_3': <utils.ModelConfig object at 0x13a92924b440>, 'max_logit_4': <utils.ModelConfig object at 0x13a92924b590>, ...}
model = 'max_logit_6', qkv_layout = 'sbhd_sbhd_sbhd'

    @pytest.mark.skipif(get_cudnn_version() < (8, 9, 1), reason="cuDNN 8.9.1+ is required.")
    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model_configs", [model_configs_max_logit])
    @pytest.mark.parametrize("model", model_configs_max_logit.keys())
    @pytest.mark.parametrize("qkv_layout", ["sbhd_sbhd_sbhd", "thd_thd_thd"])
    def test_dpa_max_logit(dtype, model_configs, model, qkv_layout):
        """Test DotProductAttention module with checkpointing"""
        config = model_configs[model]
        config.return_max_logit = True
>       test_dot_product_attention(dtype, model_configs, model, False, True, qkv_layout, False, False)

tests/pytorch/attention/test_attention.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/attention/test_attention.py:209: in test_dot_product_attention
    fused_attn_fwd, fused_max_logit, fused_attn_bwd = _run_dot_product_attention(
tests/pytorch/attention/test_attention.py:1168: in _run_dot_product_attention
    out = block(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1493: in forward
    return self.fused_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1916: in forward
    output = FusedAttnFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1267: in forward
    out_, aux_ctx_tensors, *max_logit = fused_attn_fwd(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

is_training = False, max_seqlen_q = 1, max_seqlen_kv = 2048
cu_seqlens_q = tensor([0, 1, 2, 3, 4, 5, 6, 7, 8], device='cuda:0', dtype=torch.int32)
cu_seqlens_kv = tensor([    0,  2048,  4096,  6144,  8192, 10240, 12288, 14336, 16384],
       device='cuda:0', dtype=torch.int32)
q = tensor([[[[-0.1616,  0.0568, -0.0510,  ..., -0.0121,  0.0517,  0.2371],
          [ 0.1187,  0.0266, -0.0284,  ..., -0...-0.0327,  0.0656,  ...,  0.0672,  0.0133, -0.1143]]]],
       device='cuda:0', dtype=torch.float16, requires_grad=True)
k = tensor([[[[ 6.3354e-02,  1.6357e-01, -3.4607e-02,  ...,  1.1444e-01,
            4.0710e-02, -4.1138e-02],
          [...  2.5345e-02,
           -2.2339e-02, -2.3819e-02]]]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
v = tensor([[[[-7.5867e-02,  1.1389e-01, -2.7145e-02,  ..., -4.9210e-03,
            1.3037e-01,  4.4983e-02],
          [... -2.3376e-01,
            3.2654e-02,  1.2000e-01]]]], device='cuda:0', dtype=torch.float16,
       requires_grad=True)
fake_dtype = torch.float16
fused_attention_backend = <NVTE_Fused_Attn_Backend.NVTE_F16_arbitrary_seqlen: 1>
attn_bias = None
cu_seqlens_q_padded = tensor([0, 1, 2, 3, 4, 5, 6, 7, 8], device='cuda:0', dtype=torch.int32)
cu_seqlens_kv_padded = tensor([    0,  2048,  4096,  6144,  8192, 10240, 12288, 14336, 16384],
       device='cuda:0', dtype=torch.int32)
page_table_k = None, page_table_v = None, s_quantizer = None, o_quantizer = None
attn_scale = 0.03125, dropout = 0.0, fast_zero_fill = True
qkv_layout = 'sbhd_sbhd_sbhd', attn_bias_type = 'no_bias'
attn_mask_type = 'no_mask', softmax_type = 'vanilla', window_size = (-1, -1)
rng_gen = None, softmax_offset = None, return_max_logit = True
cuda_graph = False

    def fused_attn_fwd(
        is_training: bool,
        max_seqlen_q: int,
        max_seqlen_kv: int,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_kv: torch.Tensor,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        fake_dtype: torch.dtype,
        fused_attention_backend: tex.NVTE_Fused_Attn_Backend,
        attn_bias: torch.Tensor = None,
        cu_seqlens_q_padded: torch.Tensor = None,
        cu_seqlens_kv_padded: torch.Tensor = None,
        page_table_k: torch.Tensor = None,
        page_table_v: torch.Tensor = None,
        s_quantizer: Quantizer = None,
        o_quantizer: Quantizer = None,
        attn_scale: float = None,
        dropout: float = 0.0,
        fast_zero_fill: bool = True,
        qkv_layout: str = "sbh3d",
        attn_bias_type: str = "no_bias",
        attn_mask_type: str = "padding",
        softmax_type: str = "vanilla",
        window_size: Tuple[int, int] = (-1, -1),
        rng_gen: torch.Generator = None,
        softmax_offset: torch.Tensor = None,
        return_max_logit: bool = False,
        cuda_graph: bool = False,
    ) -> Tuple[Union[torch.Tensor, None], ...]:
        """Fused Attention FWD for separate QKV input.
    
        Parameters
        ----------
        is_training : bool
                    if True, runs training and produces auxiliary tensors aux_ctx_tensors
                    for the backward; if False, runs inference and doesn't produce aux_ctx_tensors
        max_seqlen_q : int
                    max sequence length for Q, used for padding;
                    may be larger than max(seqlens_q),
                    seqlens_q = cu_seqlens_q[1:] - cu_seqlens_q[:-1]
        max_seqlen_kv : int
                    max sequence length for K and V, used for padding;
                    may be larger than max(seqlens_kv),
                    seqlens_kv = cu_seqlens_kv[1:] - cu_seqlens_kv[:-1]
        cu_seqlens_q : torch.Tensor
                    cumulative sequence lengths for Q; shape [batch_size + 1]
        cu_seqlens_kv : torch.Tensor
                    cumulative sequence lengths for K and V; shape [batch_size + 1]
        q : torch.Tensor
                    input tensor Q; shape sbhd, bshd or thd (see `qkv_layout` for details)
        k : torch.Tensor
                    input tensor K; shape sbhd, bshd or thd (see `qkv_layout` for details)
        v : torch.Tensor
                    input tensor V; shape sbhd, bshd or thd (see `qkv_layout` for details)
        fake_dtype : tex.DType
                    data type of Q, K and V - in case of high precision, fake dtype in case of FP8;
                    in torch.dtype
        fused_attention_backend : tex.NVTE_Fused_Attn_Backend
                    please see FusedAttention module for details on supported backends.
        attn_bias : torch.Tensor, default = None
                    input tensor Bias when attn_bias_type is "pre_scale_bias" or "post_scale_bias";
                    shape [1, num_heads, max_seqlen_q, max_seqlen_kv], same data type as q, k and v
        cu_seqlens_q_padded : torch.Tensor, default = None
                    cumulative sequence offsets for Q; shape [batch_size + 1]
        cu_seqlens_kv_padded : torch.Tensor, default = None
                    cumulative sequence offsets for KV; shape [batch_size + 1]
        page_table_k : torch.Tensor, default = None
                    page table for K cache; shape [batch_size, max_pages_per_seq_k]
        page_table_v : torch.Tensor, default = None
                    page table for V cache; shape [batch_size, max_pages_per_seq_v]
        s_quantizer : Quantizer, default = None
                    Quantizer object for the intermediate value S.
        o_quantizer : Quantizer, default = None
                    Quantizer object for the output of the attention.
        attn_scale : float, default = None
                    if not None, use attn_scale as the attention scale for Q*K.T BMM;
                    if None, use 1.0/sqrt(head_dim_qk) as the default
        dropout : float, default = 0.0
                    dropout probability, 0.0 means no dropout, 1.0 means no output;
                    dropout must be 0.0 if is_training is False
        fast_zero_fill : bool, default = True
                    if True, initializes the output tensor O to zero using the fast filling method;
                    if False, uses PyTorch's .fill_() method
        qkv_layout : str, default = "sbh3d"
                    layout of Q, K and V;
                    {"sb3hd", "sbh3d", "sbhd_sb2hd", "sbhd_sbh2d", "sbhd_sbhd_sbhd",
                    "bs3hd", "bsh3d", "bshd_bs2hd", "bshd_bsh2d", "bshd_bshd_bshd",
                    "t3hd", "th3d", "thd_t2hd", "thd_th2d", "thd_thd_thd"}
        attn_bias_type : str, default = "no_bias"
                    type of the bias; {"no_bias", "pre_scale_bias", "post_scale_bias", "alibi"}
        attn_mask_type : str, default = "padding"
                    type of the attention mask; {"padding", "causal", "padding_causal", "no_mask"}
        softmax_type : str, default = "vanilla"
                    type of the attention softmax; {"vanilla", "off-by-one", "learnable"}
        window_size : Tuple[int, int], default = (-1, -1)
                    sliding window size for local attention, where query at position i attends to keys
                    in [i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q
                    + window_size[1]] inclusive. Special cases (-1, -1) and (-1, 0) mean no sliding
                    window and causal mask specifically.
        rng_gen : torch.Generator, default = None
                    random number generator;
                    if None, uses the default CUDA generator from PyTorch; otherwise, uses rng_gen
        softmax_offset : torch.Tensor, default = None
                    softmax offset tensor of shape [1, h_q, 1, 1].
                    See softmax_type in DotProductAttention for details.
        return_max_logit : bool, default = False
                          whether to return the maximum attention score
        cuda_graph : bool, default = False
                    whether or not cuda graph capture is enabled.
    
        Returns
        ----------
        o : torch.Tensor
                    output tensor O, of the attention calculation; same data type as Q, K and V;
                    same shape as Q
        aux_ctx_tensors : List[torch.Tensor]
                    auxiliary output tensors used for the backward;
                    if is_training is True, aux_ctx_tensors = [softmax-related tensors, rng_state]
                    if is_training is False, aux_ctx_tensors = None
    
                    softmax-related tensors:
                        1. if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]
                           softmax: torch.Tensor
                               Softmax(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, max_seqlen_kv], dtype float32
                        2. if fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]
                           softmaxStats: torch.Tensor
                               log(sum(e^(x - max(x)))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                        3. if fused_attention_backend == FusedAttnBackend["FP8"]
                           M: torch.Tensor
                               max(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                           ZInv: torch.Tensor
                               1/sum(e^(x - max(x))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                    rng_state: torch.Tensor, optional, if backend is not F16_max512_seqlen
                        state of the random number generator;
                        [seed, offset], dtype uint64
        max_logit : if return_max_logit = True, shape [h] and same data type as O; otherwise None
        """
    
        if attn_scale is None:
            d = q.size(-1)
            attn_scale = 1.0 / math.sqrt(d)
    
        if attn_bias_type not in ["no_bias", "alibi"]:
            assert (
                attn_bias is not None
            ), "attn_bias tensor cannot be None when attn_bias_type is not no_bias or alibi."
            assert attn_bias.dtype == q.dtype, "attn_bias tensor must be in the same dtype as q and kv."
    
        assert (
            fused_attention_backend != FusedAttnBackend["No_Backend"]
        ), "Fused attention does not support this input combination."
    
        # BF16/FP16 fused attention API from fmha_v1 apex
        if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_kv + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
        # BF16/FP16 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]:
            rng_elts_per_thread = BACKEND_F16arb_ELTS_PER_THREADS
        # FP8 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["FP8"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_q + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
    
            assert (
                s_quantizer is not None
            ), "s_quantizer is required as an input for FP8 fused attention."
            assert (
                o_quantizer is not None
            ), "o_quantizer is required as an input for FP8 fused attention."
        else:
            raise ValueError(f"Unsupported backend {fused_attention_backend}")
    
        # execute kernel
    
>       output_tensors = tex.fused_attn_fwd(
            max_seqlen_q,
            max_seqlen_kv,
            is_training,
            attn_scale,
            dropout,
            fast_zero_fill,
            QKVLayout[qkv_layout],
            AttnBiasType[attn_bias_type],
            AttnMaskType[attn_mask_type],
            SoftmaxType[softmax_type],
            window_size,
            cu_seqlens_q,
            cu_seqlens_kv,
            q,
            k,
            v,
            fake_dtype,
            cu_seqlens_q_padded,
            cu_seqlens_kv_padded,
            page_table_k,
            page_table_v,
            s_quantizer,
            o_quantizer,
            attn_bias,
            softmax_offset,
            rng_gen,
            rng_elts_per_thread,
            return_max_logit,
            cuda_graph,
        )
E       RuntimeError: /root/TransformerEngine/transformer_engine/common/fused_attn/fused_attn_f16_arbitrary_seqlen.cu:406 in function operator(): cuDNN Error: No valid engine configs for Matmul_MUL_Reduction_SUB_EXP_Reduction_DIV_Matmul_
E       {"engineId":0,"smVersion":1000,"knobChoices":{"CUDNN_KNOB_TYPE_KERNEL_CFG":0}}
E       
E       . For more information, enable cuDNN error logging by setting CUDNN_LOGERR_DBG=1 and CUDNN_LOGDEST_DBG=stderr in the environment.

transformer_engine/pytorch/cpp_extensions/fused_attn.py:297: RuntimeError
_____ test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_6-model_configs0-dtype1] _____

dtype = torch.bfloat16
model_configs = {'max_logit_1': <utils.ModelConfig object at 0x13ab36818380>, 'max_logit_2': <utils.ModelConfig object at 0x13a92924b3...git_3': <utils.ModelConfig object at 0x13a92924b440>, 'max_logit_4': <utils.ModelConfig object at 0x13a92924b590>, ...}
model = 'max_logit_6', qkv_layout = 'sbhd_sbhd_sbhd'

    @pytest.mark.skipif(get_cudnn_version() < (8, 9, 1), reason="cuDNN 8.9.1+ is required.")
    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model_configs", [model_configs_max_logit])
    @pytest.mark.parametrize("model", model_configs_max_logit.keys())
    @pytest.mark.parametrize("qkv_layout", ["sbhd_sbhd_sbhd", "thd_thd_thd"])
    def test_dpa_max_logit(dtype, model_configs, model, qkv_layout):
        """Test DotProductAttention module with checkpointing"""
        config = model_configs[model]
        config.return_max_logit = True
>       test_dot_product_attention(dtype, model_configs, model, False, True, qkv_layout, False, False)

tests/pytorch/attention/test_attention.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/pytorch/attention/test_attention.py:209: in test_dot_product_attention
    fused_attn_fwd, fused_max_logit, fused_attn_bwd = _run_dot_product_attention(
tests/pytorch/attention/test_attention.py:1168: in _run_dot_product_attention
    out = block(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1493: in forward
    return self.fused_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1916: in forward
    output = FusedAttnFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
transformer_engine/pytorch/attention/dot_product_attention/backends.py:1267: in forward
    out_, aux_ctx_tensors, *max_logit = fused_attn_fwd(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

is_training = False, max_seqlen_q = 1, max_seqlen_kv = 2048
cu_seqlens_q = tensor([0, 1, 2, 3, 4, 5, 6, 7, 8], device='cuda:0', dtype=torch.int32)
cu_seqlens_kv = tensor([    0,  2048,  4096,  6144,  8192, 10240, 12288, 14336, 16384],
       device='cuda:0', dtype=torch.int32)
q = tensor([[[[-0.1621,  0.0571, -0.0513,  ..., -0.0121,  0.0515,  0.2373],
          [ 0.1187,  0.0266, -0.0286,  ..., -0...0.0327,  0.0654,  ...,  0.0674,  0.0133, -0.1143]]]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
k = tensor([[[[ 6.3477e-02,  1.6309e-01, -3.4668e-02,  ...,  1.1475e-01,
            4.0527e-02, -4.1260e-02],
          [... 2.5391e-02,
           -2.2339e-02, -2.3804e-02]]]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
v = tensor([[[[-7.5684e-02,  1.1426e-01, -2.7100e-02,  ..., -4.9438e-03,
            1.3086e-01,  4.4922e-02],
          [...-2.3438e-01,
            3.2715e-02,  1.2012e-01]]]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True)
fake_dtype = torch.bfloat16
fused_attention_backend = <NVTE_Fused_Attn_Backend.NVTE_F16_arbitrary_seqlen: 1>
attn_bias = None
cu_seqlens_q_padded = tensor([0, 1, 2, 3, 4, 5, 6, 7, 8], device='cuda:0', dtype=torch.int32)
cu_seqlens_kv_padded = tensor([    0,  2048,  4096,  6144,  8192, 10240, 12288, 14336, 16384],
       device='cuda:0', dtype=torch.int32)
page_table_k = None, page_table_v = None, s_quantizer = None, o_quantizer = None
attn_scale = 0.03125, dropout = 0.0, fast_zero_fill = True
qkv_layout = 'sbhd_sbhd_sbhd', attn_bias_type = 'no_bias'
attn_mask_type = 'no_mask', softmax_type = 'vanilla', window_size = (-1, -1)
rng_gen = None, softmax_offset = None, return_max_logit = True
cuda_graph = False

    def fused_attn_fwd(
        is_training: bool,
        max_seqlen_q: int,
        max_seqlen_kv: int,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_kv: torch.Tensor,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        fake_dtype: torch.dtype,
        fused_attention_backend: tex.NVTE_Fused_Attn_Backend,
        attn_bias: torch.Tensor = None,
        cu_seqlens_q_padded: torch.Tensor = None,
        cu_seqlens_kv_padded: torch.Tensor = None,
        page_table_k: torch.Tensor = None,
        page_table_v: torch.Tensor = None,
        s_quantizer: Quantizer = None,
        o_quantizer: Quantizer = None,
        attn_scale: float = None,
        dropout: float = 0.0,
        fast_zero_fill: bool = True,
        qkv_layout: str = "sbh3d",
        attn_bias_type: str = "no_bias",
        attn_mask_type: str = "padding",
        softmax_type: str = "vanilla",
        window_size: Tuple[int, int] = (-1, -1),
        rng_gen: torch.Generator = None,
        softmax_offset: torch.Tensor = None,
        return_max_logit: bool = False,
        cuda_graph: bool = False,
    ) -> Tuple[Union[torch.Tensor, None], ...]:
        """Fused Attention FWD for separate QKV input.
    
        Parameters
        ----------
        is_training : bool
                    if True, runs training and produces auxiliary tensors aux_ctx_tensors
                    for the backward; if False, runs inference and doesn't produce aux_ctx_tensors
        max_seqlen_q : int
                    max sequence length for Q, used for padding;
                    may be larger than max(seqlens_q),
                    seqlens_q = cu_seqlens_q[1:] - cu_seqlens_q[:-1]
        max_seqlen_kv : int
                    max sequence length for K and V, used for padding;
                    may be larger than max(seqlens_kv),
                    seqlens_kv = cu_seqlens_kv[1:] - cu_seqlens_kv[:-1]
        cu_seqlens_q : torch.Tensor
                    cumulative sequence lengths for Q; shape [batch_size + 1]
        cu_seqlens_kv : torch.Tensor
                    cumulative sequence lengths for K and V; shape [batch_size + 1]
        q : torch.Tensor
                    input tensor Q; shape sbhd, bshd or thd (see `qkv_layout` for details)
        k : torch.Tensor
                    input tensor K; shape sbhd, bshd or thd (see `qkv_layout` for details)
        v : torch.Tensor
                    input tensor V; shape sbhd, bshd or thd (see `qkv_layout` for details)
        fake_dtype : tex.DType
                    data type of Q, K and V - in case of high precision, fake dtype in case of FP8;
                    in torch.dtype
        fused_attention_backend : tex.NVTE_Fused_Attn_Backend
                    please see FusedAttention module for details on supported backends.
        attn_bias : torch.Tensor, default = None
                    input tensor Bias when attn_bias_type is "pre_scale_bias" or "post_scale_bias";
                    shape [1, num_heads, max_seqlen_q, max_seqlen_kv], same data type as q, k and v
        cu_seqlens_q_padded : torch.Tensor, default = None
                    cumulative sequence offsets for Q; shape [batch_size + 1]
        cu_seqlens_kv_padded : torch.Tensor, default = None
                    cumulative sequence offsets for KV; shape [batch_size + 1]
        page_table_k : torch.Tensor, default = None
                    page table for K cache; shape [batch_size, max_pages_per_seq_k]
        page_table_v : torch.Tensor, default = None
                    page table for V cache; shape [batch_size, max_pages_per_seq_v]
        s_quantizer : Quantizer, default = None
                    Quantizer object for the intermediate value S.
        o_quantizer : Quantizer, default = None
                    Quantizer object for the output of the attention.
        attn_scale : float, default = None
                    if not None, use attn_scale as the attention scale for Q*K.T BMM;
                    if None, use 1.0/sqrt(head_dim_qk) as the default
        dropout : float, default = 0.0
                    dropout probability, 0.0 means no dropout, 1.0 means no output;
                    dropout must be 0.0 if is_training is False
        fast_zero_fill : bool, default = True
                    if True, initializes the output tensor O to zero using the fast filling method;
                    if False, uses PyTorch's .fill_() method
        qkv_layout : str, default = "sbh3d"
                    layout of Q, K and V;
                    {"sb3hd", "sbh3d", "sbhd_sb2hd", "sbhd_sbh2d", "sbhd_sbhd_sbhd",
                    "bs3hd", "bsh3d", "bshd_bs2hd", "bshd_bsh2d", "bshd_bshd_bshd",
                    "t3hd", "th3d", "thd_t2hd", "thd_th2d", "thd_thd_thd"}
        attn_bias_type : str, default = "no_bias"
                    type of the bias; {"no_bias", "pre_scale_bias", "post_scale_bias", "alibi"}
        attn_mask_type : str, default = "padding"
                    type of the attention mask; {"padding", "causal", "padding_causal", "no_mask"}
        softmax_type : str, default = "vanilla"
                    type of the attention softmax; {"vanilla", "off-by-one", "learnable"}
        window_size : Tuple[int, int], default = (-1, -1)
                    sliding window size for local attention, where query at position i attends to keys
                    in [i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q
                    + window_size[1]] inclusive. Special cases (-1, -1) and (-1, 0) mean no sliding
                    window and causal mask specifically.
        rng_gen : torch.Generator, default = None
                    random number generator;
                    if None, uses the default CUDA generator from PyTorch; otherwise, uses rng_gen
        softmax_offset : torch.Tensor, default = None
                    softmax offset tensor of shape [1, h_q, 1, 1].
                    See softmax_type in DotProductAttention for details.
        return_max_logit : bool, default = False
                          whether to return the maximum attention score
        cuda_graph : bool, default = False
                    whether or not cuda graph capture is enabled.
    
        Returns
        ----------
        o : torch.Tensor
                    output tensor O, of the attention calculation; same data type as Q, K and V;
                    same shape as Q
        aux_ctx_tensors : List[torch.Tensor]
                    auxiliary output tensors used for the backward;
                    if is_training is True, aux_ctx_tensors = [softmax-related tensors, rng_state]
                    if is_training is False, aux_ctx_tensors = None
    
                    softmax-related tensors:
                        1. if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]
                           softmax: torch.Tensor
                               Softmax(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, max_seqlen_kv], dtype float32
                        2. if fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]
                           softmaxStats: torch.Tensor
                               log(sum(e^(x - max(x)))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                        3. if fused_attention_backend == FusedAttnBackend["FP8"]
                           M: torch.Tensor
                               max(Q*K.T)
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                           ZInv: torch.Tensor
                               1/sum(e^(x - max(x))), where x=Q*K.T
                               shape [batch_size, num_heads, max_seqlen_q, 1], dtype float32
                    rng_state: torch.Tensor, optional, if backend is not F16_max512_seqlen
                        state of the random number generator;
                        [seed, offset], dtype uint64
        max_logit : if return_max_logit = True, shape [h] and same data type as O; otherwise None
        """
    
        if attn_scale is None:
            d = q.size(-1)
            attn_scale = 1.0 / math.sqrt(d)
    
        if attn_bias_type not in ["no_bias", "alibi"]:
            assert (
                attn_bias is not None
            ), "attn_bias tensor cannot be None when attn_bias_type is not no_bias or alibi."
            assert attn_bias.dtype == q.dtype, "attn_bias tensor must be in the same dtype as q and kv."
    
        assert (
            fused_attention_backend != FusedAttnBackend["No_Backend"]
        ), "Fused attention does not support this input combination."
    
        # BF16/FP16 fused attention API from fmha_v1 apex
        if fused_attention_backend == FusedAttnBackend["F16_max512_seqlen"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_kv + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
        # BF16/FP16 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["F16_arbitrary_seqlen"]:
            rng_elts_per_thread = BACKEND_F16arb_ELTS_PER_THREADS
        # FP8 fused attention API from fmha_v2
        elif fused_attention_backend == FusedAttnBackend["FP8"]:
            rng_elts_per_thread = (
                max_seqlen_q * max_seqlen_q + BACKEND_F16m512_FP8_THREADS_PER_CTA - 1
            ) // BACKEND_F16m512_FP8_THREADS_PER_CTA
    
            assert (
                s_quantizer is not None
            ), "s_quantizer is required as an input for FP8 fused attention."
            assert (
                o_quantizer is not None
            ), "o_quantizer is required as an input for FP8 fused attention."
        else:
            raise ValueError(f"Unsupported backend {fused_attention_backend}")
    
        # execute kernel
    
>       output_tensors = tex.fused_attn_fwd(
            max_seqlen_q,
            max_seqlen_kv,
            is_training,
            attn_scale,
            dropout,
            fast_zero_fill,
            QKVLayout[qkv_layout],
            AttnBiasType[attn_bias_type],
            AttnMaskType[attn_mask_type],
            SoftmaxType[softmax_type],
            window_size,
            cu_seqlens_q,
            cu_seqlens_kv,
            q,
            k,
            v,
            fake_dtype,
            cu_seqlens_q_padded,
            cu_seqlens_kv_padded,
            page_table_k,
            page_table_v,
            s_quantizer,
            o_quantizer,
            attn_bias,
            softmax_offset,
            rng_gen,
            rng_elts_per_thread,
            return_max_logit,
            cuda_graph,
        )
E       RuntimeError: /root/TransformerEngine/transformer_engine/common/fused_attn/fused_attn_f16_arbitrary_seqlen.cu:406 in function operator(): cuDNN Error: No valid engine configs for Matmul_MUL_Reduction_SUB_EXP_Reduction_DIV_Matmul_
E       {"engineId":0,"smVersion":1000,"knobChoices":{"CUDNN_KNOB_TYPE_KERNEL_CFG":0}}
E       
E       . For more information, enable cuDNN error logging by setting CUDNN_LOGERR_DBG=1 and CUDNN_LOGDEST_DBG=stderr in the environment.

transformer_engine/pytorch/cpp_extensions/fused_attn.py:297: RuntimeError
=============================== warnings summary ===============================
transformer_engine/pytorch/attention/dot_product_attention/utils.py:2058
  /root/TransformerEngine/transformer_engine/pytorch/attention/dot_product_attention/utils.py:2058: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
    warnings.warn(

transformer_engine/pytorch/attention/dot_product_attention/utils.py:2058
  /root/TransformerEngine/transformer_engine/pytorch/attention/dot_product_attention/utils.py:2058: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=padding_causal
    warnings.warn(

transformer_engine/pytorch/attention/dot_product_attention/utils.py:2058
  /root/TransformerEngine/transformer_engine/pytorch/attention/dot_product_attention/utils.py:2058: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal_bottom_right
    warnings.warn(

transformer_engine/pytorch/attention/dot_product_attention/utils.py:2058
  /root/TransformerEngine/transformer_engine/pytorch/attention/dot_product_attention/utils.py:2058: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=padding_causal_bottom_right
    warnings.warn(

tests/pytorch/attention/test_attention.py::test_dot_product_attention[False-False-None-True-False-base_1_0-model_configs0-dtype0]
  /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

tests/pytorch/attention/test_attention.py: 262 warnings
  /root/TransformerEngine/transformer_engine/pytorch/attention/dot_product_attention/utils.py:2070: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask
    warnings.warn(

tests/pytorch/attention/test_attention.py: 261 warnings
  /root/TransformerEngine/transformer_engine/pytorch/attention/dot_product_attention/utils.py:2070: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=padding
    warnings.warn(

tests/pytorch/attention/test_attention.py::test_dpa_fp8_extra_state[dtype0-large]
tests/pytorch/attention/test_attention.py::test_dpa_fp8_extra_state[dtype1-large]
  /root/TransformerEngine/transformer_engine/pytorch/attention/dot_product_attention/backends.py:1708: UserWarning: fused_attention._extra_state is not loaded from checkpoint. Please map FusedAttention's _extra_state to DotProductAttention's _extra_state.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_attention.xml -
=========================== short test summary info ============================
FAILED tests/pytorch/attention/test_attention.py::test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_1-model_configs0-dtype0]
FAILED tests/pytorch/attention/test_attention.py::test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_1-model_configs0-dtype1]
FAILED tests/pytorch/attention/test_attention.py::test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_2-model_configs0-dtype0]
FAILED tests/pytorch/attention/test_attention.py::test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_2-model_configs0-dtype1]
FAILED tests/pytorch/attention/test_attention.py::test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_3-model_configs0-dtype0]
FAILED tests/pytorch/attention/test_attention.py::test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_3-model_configs0-dtype1]
FAILED tests/pytorch/attention/test_attention.py::test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_4-model_configs0-dtype0]
FAILED tests/pytorch/attention/test_attention.py::test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_4-model_configs0-dtype1]
FAILED tests/pytorch/attention/test_attention.py::test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_5-model_configs0-dtype0]
FAILED tests/pytorch/attention/test_attention.py::test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_5-model_configs0-dtype1]
FAILED tests/pytorch/attention/test_attention.py::test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_6-model_configs0-dtype0]
FAILED tests/pytorch/attention/test_attention.py::test_dpa_max_logit[sbhd_sbhd_sbhd-max_logit_6-model_configs0-dtype1]
========= 12 failed, 1312 passed, 1249 skipped, 530 warnings in 54.65s =========
+ test_fail test_attention.py
+ RET=1
+ FAILED_CASES=' test_numerics.py test_fused_optimizer.py test_attention.py'
+ echo 'Error: sub-test failed: test_attention.py'
Error: sub-test failed: test_attention.py
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_kv_cache.xml /root/TransformerEngine/tests/pytorch/attention/test_kv_cache.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 576 items

tests/pytorch/attention/test_kv_cache.py ........................FFFFFFF [  5%]
FFFFFssssssssssss................................................FFFFFFF [ 17%]
FFFFFssssssssssss................................................FFFFFFF [ 30%]
FFFFFssssssssssssssssssssssssssssssssssss........................FFFFFFF [ 42%]
FFFFFsssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 55%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 67%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 80%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 92%]
sssssssssssssssssssssssssssssssssssssssss                                [100%]

=================================== FAILURES ===================================
_ test_kv_cache[False-False-TransformerLayer-FlashAttention-False-bshd-infer_0-dtype0] _

dtype = torch.float16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'bshd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[-1.1367,  0.2063, -0.2793,  ...,  0.5215, -0.2111,  0.9126],
         [ 1.4609, -1.8701,  1.5518,  ..., -1.5...,
         [-0.4971, -1.0195, -1.6553,  ..., -1.4707, -0.5957, -0.3569]]],
       device='cuda:0', dtype=torch.float16)
k = tensor([[[[ 0.8755, -0.8584,  1.2988,  ...,  0.0806,  1.4824,  0.6655],
          [-3.0469,  1.0127,  1.0898,  ...,  0...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
v = tensor([[[[ 0.2512, -0.2554, -1.6045,  ..., -0.2654, -0.3354, -0.1606],
          [-0.7661,  0.1204, -1.5088,  ...,  1...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
cu_seqlens_q = tensor([0, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-TransformerLayer-FlashAttention-False-bshd-infer_0-dtype1] _

dtype = torch.bfloat16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'bshd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[-1.1328,  0.2090, -0.2754,  ...,  0.5195, -0.2139,  0.9102],
         [ 1.4688, -1.8750,  1.5547,  ..., -1.5...
         [-0.4941, -1.0234, -1.6562,  ..., -1.4688, -0.5938, -0.3555]]],
       device='cuda:0', dtype=torch.bfloat16)
k = tensor([[[[ 0.8711, -0.8594,  1.2969,  ...,  0.0859,  1.4766,  0.6719],
          [-3.0469,  1.0156,  1.0938,  ...,  0...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
v = tensor([[[[ 0.2539, -0.2559, -1.6016,  ..., -0.2617, -0.3340, -0.1631],
          [-0.7617,  0.1172, -1.5000,  ...,  1...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
cu_seqlens_q = tensor([0, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-TransformerLayer-FlashAttention-False-bshd-infer_1-dtype0] _

dtype = torch.float16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'bshd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[ 3.9414, -0.9683,  0.4854,  ...,  1.9502,  2.2227,  1.3428],
         [-2.3711,  0.2400,  0.0643,  ..., -0.1...,
         [ 0.8315, -1.1680, -0.1442,  ..., -1.4375, -2.3164, -1.6641]]],
       device='cuda:0', dtype=torch.float16)
k = tensor([[[[ 2.9199,  0.4048,  1.8818,  ..., -0.2634, -0.0704,  2.0547],
          [ 2.0488,  0.3687, -1.3613,  ..., -0...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
v = tensor([[[[-1.0439, -1.2734, -0.5435,  ...,  0.0572,  1.9053,  3.0312],
          [-2.4570,  0.4792, -3.9551,  ...,  1...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
cu_seqlens_q = tensor([0, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-TransformerLayer-FlashAttention-False-bshd-infer_1-dtype1] _

dtype = torch.bfloat16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'bshd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[ 3.9375, -0.9688,  0.4863,  ...,  1.9531,  2.2188,  1.3516],
         [-2.3594,  0.2393,  0.0579,  ..., -0.1...
         [ 0.8281, -1.1719, -0.1416,  ..., -1.4375, -2.3125, -1.6719]]],
       device='cuda:0', dtype=torch.bfloat16)
k = tensor([[[[ 2.9219,  0.4043,  1.8828,  ..., -0.2695, -0.0776,  2.0625],
          [ 2.0469,  0.3633, -1.3672,  ..., -0...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
v = tensor([[[[-1.0469, -1.2656, -0.5430,  ...,  0.0581,  1.8984,  3.0312],
          [-2.4531,  0.4844, -3.9531,  ...,  2...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
cu_seqlens_q = tensor([0, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-TransformerLayer-FlashAttention-False-sbhd-infer_0-dtype0] _

dtype = torch.float16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'sbhd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[-1.1367,  0.2063, -0.2793,  ...,  0.5215, -0.2111,  0.9126],
         [ 1.4609, -1.8701,  1.5518,  ..., -1.5...,
         [-0.4971, -1.0195, -1.6553,  ..., -1.4707, -0.5957, -0.3569]]],
       device='cuda:0', dtype=torch.float16)
k = tensor([[[[ 0.8755, -0.8584,  1.2988,  ...,  0.0806,  1.4824,  0.6655],
          [-3.0469,  1.0127,  1.0898,  ...,  0...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
v = tensor([[[[ 0.2512, -0.2554, -1.6045,  ..., -0.2654, -0.3354, -0.1606],
          [-0.7661,  0.1204, -1.5088,  ...,  1...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
cu_seqlens_q = tensor([0, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-TransformerLayer-FlashAttention-False-sbhd-infer_0-dtype1] _

dtype = torch.bfloat16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'sbhd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[-1.1328,  0.2090, -0.2754,  ...,  0.5195, -0.2139,  0.9102],
         [ 1.4688, -1.8750,  1.5547,  ..., -1.5...
         [-0.4941, -1.0234, -1.6562,  ..., -1.4688, -0.5938, -0.3555]]],
       device='cuda:0', dtype=torch.bfloat16)
k = tensor([[[[ 0.8711, -0.8594,  1.2969,  ...,  0.0859,  1.4766,  0.6719],
          [-3.0469,  1.0156,  1.0938,  ...,  0...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
v = tensor([[[[ 0.2539, -0.2559, -1.6016,  ..., -0.2617, -0.3340, -0.1631],
          [-0.7617,  0.1172, -1.5000,  ...,  1...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
cu_seqlens_q = tensor([0, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-TransformerLayer-FlashAttention-False-sbhd-infer_1-dtype0] _

dtype = torch.float16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'sbhd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[ 3.9414, -0.9683,  0.4854,  ...,  1.9502,  2.2227,  1.3428],
         [-2.3711,  0.2400,  0.0643,  ..., -0.1...,
         [ 0.8315, -1.1680, -0.1442,  ..., -1.4375, -2.3164, -1.6641]]],
       device='cuda:0', dtype=torch.float16)
k = tensor([[[[ 2.9199,  0.4048,  1.8818,  ..., -0.2634, -0.0704,  2.0547],
          [ 2.0488,  0.3687, -1.3613,  ..., -0...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
v = tensor([[[[-1.0439, -1.2734, -0.5435,  ...,  0.0572,  1.9053,  3.0312],
          [-2.4570,  0.4792, -3.9551,  ...,  1...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
cu_seqlens_q = tensor([0, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-TransformerLayer-FlashAttention-False-sbhd-infer_1-dtype1] _

dtype = torch.bfloat16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'sbhd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[ 3.9375, -0.9688,  0.4863,  ...,  1.9531,  2.2188,  1.3516],
         [-2.3594,  0.2393,  0.0579,  ..., -0.1...
         [ 0.8281, -1.1719, -0.1416,  ..., -1.4375, -2.3125, -1.6719]]],
       device='cuda:0', dtype=torch.bfloat16)
k = tensor([[[[ 2.9219,  0.4043,  1.8828,  ..., -0.2695, -0.0776,  2.0625],
          [ 2.0469,  0.3633, -1.3672,  ..., -0...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
v = tensor([[[[-1.0469, -1.2656, -0.5430,  ...,  0.0581,  1.8984,  3.0312],
          [-2.4531,  0.4844, -3.9531,  ...,  2...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
cu_seqlens_q = tensor([0, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-TransformerLayer-FlashAttention-False-thd-infer_0-dtype0] _

dtype = torch.float16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'thd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[-1.1367,  0.2063, -0.2793,  ...,  0.5215, -0.2111,  0.9126],
         [ 1.4609, -1.8701,  1.5518,  ..., -1.5...195, -1.6553,  ..., -1.4707, -0.5957, -0.3569]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ViewBackward0>)
k = tensor([[[[ 0.8755, -0.8584,  1.2988,  ...,  0.0806,  1.4824,  0.6655],
          [-3.0469,  1.0127,  1.0898,  ...,  0...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
v = tensor([[[[ 0.2512, -0.2554, -1.6045,  ..., -0.2654, -0.3354, -0.1606],
          [-0.7661,  0.1204, -1.5088,  ...,  1...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
cu_seqlens_q = tensor([0, 1, 1, 1, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1, 1, 1, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0],
        [1],
        [2],
        [3]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-TransformerLayer-FlashAttention-False-thd-infer_0-dtype1] _

dtype = torch.bfloat16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'thd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[-1.1328,  0.2090, -0.2754,  ...,  0.5195, -0.2139,  0.9102],
         [ 1.4688, -1.8750,  1.5547,  ..., -1.5...34, -1.6562,  ..., -1.4688, -0.5938, -0.3555]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ViewBackward0>)
k = tensor([[[[ 0.8711, -0.8594,  1.2969,  ...,  0.0859,  1.4766,  0.6719],
          [-3.0469,  1.0156,  1.0938,  ...,  0...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
v = tensor([[[[ 0.2539, -0.2559, -1.6016,  ..., -0.2617, -0.3340, -0.1631],
          [-0.7617,  0.1172, -1.5000,  ...,  1...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
cu_seqlens_q = tensor([0, 1, 1, 1, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1, 1, 1, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0],
        [1],
        [2],
        [3]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-TransformerLayer-FlashAttention-False-thd-infer_1-dtype0] _

dtype = torch.float16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'thd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[ 3.9414, -0.9683,  0.4854,  ...,  1.9502,  2.2227,  1.3428],
         [-2.3711,  0.2400,  0.0643,  ..., -0.1...680, -0.1442,  ..., -1.4375, -2.3164, -1.6641]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ViewBackward0>)
k = tensor([[[[ 2.9199,  0.4048,  1.8818,  ..., -0.2634, -0.0704,  2.0547],
          [ 2.0488,  0.3687, -1.3613,  ..., -0...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
v = tensor([[[[-1.0439, -1.2734, -0.5435,  ...,  0.0572,  1.9053,  3.0312],
          [-2.4570,  0.4792, -3.9551,  ...,  1...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
cu_seqlens_q = tensor([0, 1, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0],
        [1]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-TransformerLayer-FlashAttention-False-thd-infer_1-dtype1] _

dtype = torch.bfloat16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'thd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[ 3.9375, -0.9688,  0.4863,  ...,  1.9531,  2.2188,  1.3516],
         [-2.3594,  0.2393,  0.0579,  ..., -0.1...19, -0.1416,  ..., -1.4375, -2.3125, -1.6719]]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ViewBackward0>)
k = tensor([[[[ 2.9219,  0.4043,  1.8828,  ..., -0.2695, -0.0776,  2.0625],
          [ 2.0469,  0.3633, -1.3672,  ..., -0...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
v = tensor([[[[-1.0469, -1.2656, -0.5430,  ...,  0.0581,  1.8984,  3.0312],
          [-2.4531,  0.4844, -3.9531,  ...,  2...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
cu_seqlens_q = tensor([0, 1, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0],
        [1]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-DotProductAttention-FlashAttention-False-bshd-infer_0-dtype0] _

dtype = torch.float16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'bshd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[-0.1616,  0.0568, -0.0510,  ..., -0.1350,  0.2710, -0.0168],
         [-0.0323,  0.1426,  0.0125,  ..., -0.1...,
         [-0.0057,  0.0192,  0.0258,  ..., -0.1832,  0.1425,  0.1538]]],
       device='cuda:0', dtype=torch.float16)
k = tensor([[[[ 0.0634,  0.1636, -0.0346,  ..., -0.0270,  0.0003,  0.0067],
          [ 0.0714,  0.0892,  0.2976,  ..., -0...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
v = tensor([[[[-0.0162,  0.1808, -0.1053,  ...,  0.0224, -0.0574, -0.0741],
          [ 0.0243,  0.1534, -0.0161,  ..., -0...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
cu_seqlens_q = tensor([0, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-DotProductAttention-FlashAttention-False-bshd-infer_0-dtype1] _

dtype = torch.bfloat16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'bshd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[-0.1621,  0.0571, -0.0513,  ..., -0.1348,  0.2695, -0.0168],
         [-0.0322,  0.1426,  0.0125,  ..., -0.1...
         [-0.0057,  0.0193,  0.0258,  ..., -0.1836,  0.1426,  0.1543]]],
       device='cuda:0', dtype=torch.bfloat16)
k = tensor([[[[ 0.0635,  0.1631, -0.0347,  ..., -0.0270,  0.0003,  0.0067],
          [ 0.0713,  0.0889,  0.2969,  ..., -0...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
v = tensor([[[[-0.0162,  0.1807, -0.1055,  ...,  0.0223, -0.0574, -0.0742],
          [ 0.0243,  0.1533, -0.0160,  ..., -0...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
cu_seqlens_q = tensor([0, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-DotProductAttention-FlashAttention-False-bshd-infer_1-dtype0] _

dtype = torch.float16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'bshd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[-0.1616,  0.0568, -0.0510,  ..., -0.1727,  0.0854, -0.1760],
         [ 0.0534,  0.0396, -0.0638,  ..., -0.0...,
         [-0.0459,  0.0656, -0.0966,  ..., -0.0159,  0.0961, -0.0082]]],
       device='cuda:0', dtype=torch.float16)
k = tensor([[[[ 6.3416e-02, -1.2244e-01,  2.3003e-03,  ...,  2.2125e-02,
            2.9007e-02, -1.7120e-02],
          [...000e+00,  0.0000e+00,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00]]]], device='cuda:0', dtype=torch.float16)
v = tensor([[[[-0.1105, -0.2067, -0.0229,  ...,  0.0551, -0.1759, -0.0003],
          [-0.0806, -0.0573,  0.0111,  ..., -0...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
cu_seqlens_q = tensor([0, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-DotProductAttention-FlashAttention-False-bshd-infer_1-dtype1] _

dtype = torch.bfloat16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'bshd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[-0.1621,  0.0571, -0.0513,  ..., -0.1729,  0.0854, -0.1758],
         [ 0.0535,  0.0396, -0.0640,  ..., -0.0...
         [-0.0459,  0.0654, -0.0967,  ..., -0.0159,  0.0962, -0.0082]]],
       device='cuda:0', dtype=torch.bfloat16)
k = tensor([[[[ 6.3477e-02, -1.2256e-01,  2.3041e-03,  ...,  2.2095e-02,
            2.8931e-02, -1.7090e-02],
          [...00e+00,  0.0000e+00,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00]]]], device='cuda:0', dtype=torch.bfloat16)
v = tensor([[[[-0.1104, -0.2061, -0.0229,  ...,  0.0552, -0.1758, -0.0003],
          [-0.0806, -0.0574,  0.0111,  ..., -0...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
cu_seqlens_q = tensor([0, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-DotProductAttention-FlashAttention-False-sbhd-infer_0-dtype0] _

dtype = torch.float16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'sbhd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[-0.1616,  0.0568, -0.0510,  ..., -0.1350,  0.2710, -0.0168],
         [-0.0323,  0.1426,  0.0125,  ..., -0.1...,
         [-0.0057,  0.0192,  0.0258,  ..., -0.1832,  0.1425,  0.1538]]],
       device='cuda:0', dtype=torch.float16)
k = tensor([[[[ 0.0634,  0.1636, -0.0346,  ..., -0.0270,  0.0003,  0.0067],
          [ 0.0714,  0.0892,  0.2976,  ..., -0...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
v = tensor([[[[-0.0162,  0.1808, -0.1053,  ...,  0.0224, -0.0574, -0.0741],
          [ 0.0243,  0.1534, -0.0161,  ..., -0...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
cu_seqlens_q = tensor([0, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-DotProductAttention-FlashAttention-False-sbhd-infer_0-dtype1] _

dtype = torch.bfloat16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'sbhd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[-0.1621,  0.0571, -0.0513,  ..., -0.1348,  0.2695, -0.0168],
         [-0.0322,  0.1426,  0.0125,  ..., -0.1...
         [-0.0057,  0.0193,  0.0258,  ..., -0.1836,  0.1426,  0.1543]]],
       device='cuda:0', dtype=torch.bfloat16)
k = tensor([[[[ 0.0635,  0.1631, -0.0347,  ..., -0.0270,  0.0003,  0.0067],
          [ 0.0713,  0.0889,  0.2969,  ..., -0...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
v = tensor([[[[-0.0162,  0.1807, -0.1055,  ...,  0.0223, -0.0574, -0.0742],
          [ 0.0243,  0.1533, -0.0160,  ..., -0...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
cu_seqlens_q = tensor([0, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-DotProductAttention-FlashAttention-False-sbhd-infer_1-dtype0] _

dtype = torch.float16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'sbhd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[-0.1616,  0.0568, -0.0510,  ..., -0.1727,  0.0854, -0.1760],
         [ 0.0534,  0.0396, -0.0638,  ..., -0.0...,
         [-0.0459,  0.0656, -0.0966,  ..., -0.0159,  0.0961, -0.0082]]],
       device='cuda:0', dtype=torch.float16)
k = tensor([[[[ 6.3416e-02, -1.2244e-01,  2.3003e-03,  ...,  2.2125e-02,
            2.9007e-02, -1.7120e-02],
          [...000e+00,  0.0000e+00,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00]]]], device='cuda:0', dtype=torch.float16)
v = tensor([[[[-0.1105, -0.2067, -0.0229,  ...,  0.0551, -0.1759, -0.0003],
          [-0.0806, -0.0573,  0.0111,  ..., -0...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
cu_seqlens_q = tensor([0, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-DotProductAttention-FlashAttention-False-sbhd-infer_1-dtype1] _

dtype = torch.bfloat16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'sbhd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[-0.1621,  0.0571, -0.0513,  ..., -0.1729,  0.0854, -0.1758],
         [ 0.0535,  0.0396, -0.0640,  ..., -0.0...
         [-0.0459,  0.0654, -0.0967,  ..., -0.0159,  0.0962, -0.0082]]],
       device='cuda:0', dtype=torch.bfloat16)
k = tensor([[[[ 6.3477e-02, -1.2256e-01,  2.3041e-03,  ...,  2.2095e-02,
            2.8931e-02, -1.7090e-02],
          [...00e+00,  0.0000e+00,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00]]]], device='cuda:0', dtype=torch.bfloat16)
v = tensor([[[[-0.1104, -0.2061, -0.0229,  ...,  0.0552, -0.1758, -0.0003],
          [-0.0806, -0.0574,  0.0111,  ..., -0...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
cu_seqlens_q = tensor([0, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-DotProductAttention-FlashAttention-False-thd-infer_0-dtype0] _

dtype = torch.float16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'thd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[-0.1616,  0.0568, -0.0510,  ..., -0.1350,  0.2710, -0.0168],
         [-0.0323,  0.1426,  0.0125,  ..., -0.1...,
         [-0.0057,  0.0192,  0.0258,  ..., -0.1832,  0.1425,  0.1538]]],
       device='cuda:0', dtype=torch.float16)
k = tensor([[[[ 0.0634,  0.1636, -0.0346,  ..., -0.0270,  0.0003,  0.0067],
          [ 0.0714,  0.0892,  0.2976,  ..., -0...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
v = tensor([[[[-0.0162,  0.1808, -0.1053,  ...,  0.0224, -0.0574, -0.0741],
          [ 0.0243,  0.1534, -0.0161,  ..., -0...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
cu_seqlens_q = tensor([0, 1, 1, 1, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1, 1, 1, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0],
        [1],
        [2],
        [3]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-DotProductAttention-FlashAttention-False-thd-infer_0-dtype1] _

dtype = torch.bfloat16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'thd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[-0.1621,  0.0571, -0.0513,  ..., -0.1348,  0.2695, -0.0168],
         [-0.0322,  0.1426,  0.0125,  ..., -0.1...
         [-0.0057,  0.0193,  0.0258,  ..., -0.1836,  0.1426,  0.1543]]],
       device='cuda:0', dtype=torch.bfloat16)
k = tensor([[[[ 0.0635,  0.1631, -0.0347,  ..., -0.0270,  0.0003,  0.0067],
          [ 0.0713,  0.0889,  0.2969,  ..., -0...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
v = tensor([[[[-0.0162,  0.1807, -0.1055,  ...,  0.0223, -0.0574, -0.0742],
          [ 0.0243,  0.1533, -0.0160,  ..., -0...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
cu_seqlens_q = tensor([0, 1, 1, 1, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1, 1, 1, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0],
        [1],
        [2],
        [3]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-DotProductAttention-FlashAttention-False-thd-infer_1-dtype0] _

dtype = torch.float16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'thd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[-0.1616,  0.0568, -0.0510,  ..., -0.1727,  0.0854, -0.1760],
         [ 0.0534,  0.0396, -0.0638,  ..., -0.0...,
         [-0.0459,  0.0656, -0.0966,  ..., -0.0159,  0.0961, -0.0082]]],
       device='cuda:0', dtype=torch.float16)
k = tensor([[[[ 6.3416e-02, -1.2244e-01,  2.3003e-03,  ...,  2.2125e-02,
            2.9007e-02, -1.7120e-02],
          [...000e+00,  0.0000e+00,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00]]]], device='cuda:0', dtype=torch.float16)
v = tensor([[[[-0.1105, -0.2067, -0.0229,  ...,  0.0551, -0.1759, -0.0003],
          [-0.0806, -0.0573,  0.0111,  ..., -0...          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.float16)
cu_seqlens_q = tensor([0, 1, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0],
        [1]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-False-DotProductAttention-FlashAttention-False-thd-infer_1-dtype1] _

dtype = torch.bfloat16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'thd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = False, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
                make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]
    
            sim.reset()
            inference_params.reset()
            step_dict = OrderedDict()
    
        # simulate step by step
        # t-1: ...
        #      compute for seq_ids = [0, 1, 2], ctx_lens = [5, 2, 3], gen_lens = [2, 9, 4],
        #              batch_size = 3, step_lens = [1, 1, 1]
        #      increase counter for gen_lens = [3, 10, 5]
        # t:   detect seq 1 is finished since expected_gen_lens = [12, 10, 15]
        #      add two new seqs 3 and 4, with ctx lens 10 and 11
        #      compute for seq_ids = [0, 2, 3, 4], ctx_lens = [5, 3, 10, 11], gen_lens = [3, 5, 0, 0],
        #              batch_size = 4, step_lens = [1, 1, 10, 11]
        #      increase counter for gen_lens = [3, 5, 1, 1]
        max_tokens = config.batch_size * config.max_ctx_len
        while True:
            # prepare batch for the current step
            dynamic_fill = True  # inference_params.is_paged
            sim.step(dynamic_fill=dynamic_fill)
            sim.print_step(logger)
    
            if sim.t_batch_size == 0:
                # all sequences are finished
                if sim.t > sim.last_arrival:
                    sim.serving_times = sim.arrival_times + sim.request_delays
                    sim.complete_times = sim.serving_times + sim.gen_lens
                    break
                # not finished; run next iteration
                else:
                    sim.t += 1
                    continue
    
            # create incremental input
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            max_seqlen_q = sim.max_ctx_len if is_cuda_graph else max(sim.step_lens).item()
            num_tensors = len(full_inputs)
            if qkv_format == "thd":
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.Tensor().to(dtype=dtype, device="cuda")
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp = torch.cat([inc_inp, inp[seq, start:end]], dim=0)
                    if is_cuda_graph:
                        inc_inp = torch.cat(
                            [
                                inc_inp,
                                torch.zeros(
                                    max_tokens - sum(sim.step_lens),
                                    *inp.shape[2:],
                                    dtype=dtype,
                                    device=inc_inp.device,
                                ),
                            ],
                            dim=0,
                        )
                    incremental_inputs.append(inc_inp)
            else:
                incremental_inputs = []
                for i in range(num_tensors):
                    inp = full_inputs[i]
                    inc_inp = torch.zeros(
                        batch_size,
                        max_seqlen_q,
                        *inp.shape[2:],
                        dtype=dtype,
                        device="cuda",
                    )
                    for i, seq in enumerate(sim.t_seq_ids):
                        start = (sim.t_total_lens[i] - sim.step_lens[i]).item()
                        end = sim.t_total_lens[i].item()
                        inc_inp[i, : sim.step_lens[i], :] = inp[seq, start:end]
                    if qkv_format == "sbhd":
                        inc_inp = inc_inp.transpose(0, 1).contiguous()
                    incremental_inputs.append(inc_inp)
    
            # run step
            batch_size = max_batch_size if is_cuda_graph else sim.t_batch_size
            cu_seqlens_q = torch.zeros(batch_size + 1, dtype=torch.int32, device="cuda")
            cu_seqlens_q[1 : sim.t_batch_size + 1] = torch.cumsum(sim.step_lens, dim=0)
            cu_seqlens_kv = cu_seqlens_q.clone()
            step_dict = OrderedDict(zip(sim.t_seq_ids.tolist(), sim.step_lens.tolist()))
            inference_params.pre_step(step_dict)
            if inference_params.is_paged:
                inference_params.cache_manager.print_cache()
            incremental_output = incremental_inputs
            with autocast(enabled=is_fp8, recipe=fp8_recipe):
                for m in model:
>                   incremental_output = m(
                        *incremental_output,
                        cu_seqlens_q=cu_seqlens_q,
                        cu_seqlens_kv=cu_seqlens_kv,
                        inference_params=inference_params,
                        max_seqlen_q=max_seqlen_q,
                        max_seqlen_kv=config.max_seqlen_kv,
                    )

tests/pytorch/attention/test_kv_cache.py:661: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786: in _call_impl
    return forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[-0.1621,  0.0571, -0.0513,  ..., -0.1729,  0.0854, -0.1758],
         [ 0.0535,  0.0396, -0.0640,  ..., -0.0...
         [-0.0459,  0.0654, -0.0967,  ..., -0.0159,  0.0962, -0.0082]]],
       device='cuda:0', dtype=torch.bfloat16)
k = tensor([[[[ 6.3477e-02, -1.2256e-01,  2.3041e-03,  ...,  2.2095e-02,
            2.8931e-02, -1.7090e-02],
          [...00e+00,  0.0000e+00,  ...,  0.0000e+00,
            0.0000e+00,  0.0000e+00]]]], device='cuda:0', dtype=torch.bfloat16)
v = tensor([[[[-0.1104, -0.2061, -0.0229,  ...,  0.0552, -0.1758, -0.0003],
          [-0.0806, -0.0574,  0.0111,  ..., -0...         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],
       device='cuda:0', dtype=torch.bfloat16)
cu_seqlens_q = tensor([0, 1, 1], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([0, 1, 1], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 1, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0],
        [1]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-TransformerLayer-FlashAttention-False-bshd-infer_0-dtype0] _

dtype = torch.float16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'bshd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., ...[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.float16)
k = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
v = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
cu_seqlens_q = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0],
        [1],
        [2],
        [3]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-TransformerLayer-FlashAttention-False-bshd-infer_0-dtype1] _

dtype = torch.bfloat16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'bshd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., ...0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
k = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
v = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
cu_seqlens_q = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0],
        [1],
        [2],
        [3]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-TransformerLayer-FlashAttention-False-bshd-infer_1-dtype0] _

dtype = torch.float16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'bshd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., ...[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.float16)
k = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
v = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
cu_seqlens_q = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0],
        [1]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-TransformerLayer-FlashAttention-False-bshd-infer_1-dtype1] _

dtype = torch.bfloat16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'bshd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., ...0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
k = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
v = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
cu_seqlens_q = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0],
        [1]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-TransformerLayer-FlashAttention-False-sbhd-infer_0-dtype0] _

dtype = torch.float16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'sbhd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., ...[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.float16)
k = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
v = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
cu_seqlens_q = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0],
        [1],
        [2],
        [3]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-TransformerLayer-FlashAttention-False-sbhd-infer_0-dtype1] _

dtype = torch.bfloat16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'sbhd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., ...0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
k = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
v = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
cu_seqlens_q = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0],
        [1],
        [2],
        [3]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-TransformerLayer-FlashAttention-False-sbhd-infer_1-dtype0] _

dtype = torch.float16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'sbhd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., ...[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.float16)
k = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
v = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
cu_seqlens_q = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0],
        [1]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-TransformerLayer-FlashAttention-False-sbhd-infer_1-dtype1] _

dtype = torch.bfloat16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'sbhd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., ...0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16)
k = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
v = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
cu_seqlens_q = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0],
        [1]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-TransformerLayer-FlashAttention-False-thd-infer_0-dtype0] _

dtype = torch.float16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'thd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., ... 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CloneBackward0>)
k = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
v = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
cu_seqlens_q = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0],
        [1],
        [2],
        [3]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-TransformerLayer-FlashAttention-False-thd-infer_0-dtype1] _

dtype = torch.bfloat16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'thd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., ...0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<CloneBackward0>)
k = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
v = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
cu_seqlens_q = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0],
        [1],
        [2],
        [3]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-TransformerLayer-FlashAttention-False-thd-infer_1-dtype0] _

dtype = torch.float16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'thd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., ... 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CloneBackward0>)
k = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
v = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
cu_seqlens_q = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0],
        [1]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-TransformerLayer-FlashAttention-False-thd-infer_1-dtype1] _

dtype = torch.bfloat16
model = [TransformerLayer(
  (self_attention): MultiheadAttention(
    (layernorm_qkv): LayerNormLinear()
    (core_attention)...ion_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (proj): Linear()
  )
  (layernorm_mlp): LayerNormMLP()
)]
qkv_format = 'thd', is_paged = False, backend = 'FlashAttention'
module = 'TransformerLayer', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/transformer.py:779: in forward
    self_attention_outputs = self.self_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/multi_head_attention.py:993: in forward
    context_layer = self.core_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., ...0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       dtype=torch.bfloat16, grad_fn=<CloneBackward0>)
k = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
v = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
cu_seqlens_q = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0],
        [1]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-DotProductAttention-FlashAttention-False-bshd-infer_0-dtype0] _

dtype = torch.float16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'bshd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., ...[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0',
       dtype=torch.float16)
k = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
v = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
cu_seqlens_q = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0],
        [1],
        [2],
        [3]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-DotProductAttention-FlashAttention-False-bshd-infer_0-dtype1] _

dtype = torch.bfloat16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'bshd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., ...1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0',
       dtype=torch.bfloat16)
k = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
v = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
cu_seqlens_q = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0],
        [1],
        [2],
        [3]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-DotProductAttention-FlashAttention-False-bshd-infer_1-dtype0] _

dtype = torch.float16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'bshd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., ...[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0',
       dtype=torch.float16)
k = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
v = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
cu_seqlens_q = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0],
        [1]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-DotProductAttention-FlashAttention-False-bshd-infer_1-dtype1] _

dtype = torch.bfloat16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'bshd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., ...1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0',
       dtype=torch.bfloat16)
k = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
v = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
cu_seqlens_q = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0],
        [1]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-DotProductAttention-FlashAttention-False-sbhd-infer_0-dtype0] _

dtype = torch.float16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'sbhd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., ...[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0',
       dtype=torch.float16)
k = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
v = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
cu_seqlens_q = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0],
        [1],
        [2],
        [3]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-DotProductAttention-FlashAttention-False-sbhd-infer_0-dtype1] _

dtype = torch.bfloat16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'sbhd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., ...1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0',
       dtype=torch.bfloat16)
k = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
v = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
cu_seqlens_q = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0],
        [1],
        [2],
        [3]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-DotProductAttention-FlashAttention-False-sbhd-infer_1-dtype0] _

dtype = torch.float16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'sbhd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., ...[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0',
       dtype=torch.float16)
k = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
v = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
cu_seqlens_q = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0],
        [1]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-DotProductAttention-FlashAttention-False-sbhd-infer_1-dtype1] _

dtype = torch.bfloat16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'sbhd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., ...1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0',
       dtype=torch.bfloat16)
k = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
v = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
cu_seqlens_q = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0],
        [1]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-DotProductAttention-FlashAttention-False-thd-infer_0-dtype0] _

dtype = torch.float16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'thd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., ...[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0',
       dtype=torch.float16)
k = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
v = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
cu_seqlens_q = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0],
        [1],
        [2],
        [3]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-DotProductAttention-FlashAttention-False-thd-infer_0-dtype1] _

dtype = torch.bfloat16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'thd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., ...1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0',
       dtype=torch.bfloat16)
k = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
v = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
cu_seqlens_q = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32, 48, 64], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 64, dropout_p = 0.0
softmax_scale = 0.08838834764831843, causal = True, window_size_left = -1
window_size_right = 0, softcap = 0.0, alibi_slopes = None
return_softmax = False
block_table = tensor([[0],
        [1],
        [2],
        [3]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-DotProductAttention-FlashAttention-False-thd-infer_1-dtype0] _

dtype = torch.float16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'thd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., ...[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0',
       dtype=torch.float16)
k = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
v = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1...., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.float16)
cu_seqlens_q = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0],
        [1]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
_ test_kv_cache[False-True-DotProductAttention-FlashAttention-False-thd-infer_1-dtype1] _

dtype = torch.bfloat16
model = [DotProductAttention(
  (flash_attention): FlashAttention()
  (fused_attention): FusedAttention()
  (unfused_attention...ention(
    (scale_mask_softmax): FusedScaleMaskSoftmax()
    (attention_dropout): Dropout(p=0.0, inplace=False)
  )
)]
qkv_format = 'thd', is_paged = False, backend = 'FlashAttention'
module = 'DotProductAttention', is_cuda_graph = True, is_fp8 = False

    @pytest.mark.parametrize("dtype", param_types)
    @pytest.mark.parametrize("model", model_configs_infer.keys())
    @pytest.mark.parametrize("qkv_format", qkv_formats)
    @pytest.mark.parametrize("is_paged", [False, True])
    @pytest.mark.parametrize("backend", ["FusedAttention", "FlashAttention", "UnfusedAttention"])
    @pytest.mark.parametrize("module", ["TransformerLayer", "DotProductAttention"])
    @pytest.mark.parametrize("is_cuda_graph", [False, True])
    @pytest.mark.parametrize("is_fp8", [False, True])
    def test_kv_cache(dtype, model, qkv_format, is_paged, backend, module, is_cuda_graph, is_fp8):
        reset_rng_states()
        logger = logging.getLogger("test_kv_cache")
        fp8_recipe = recipe.DelayedScaling(
            margin=0,
            fp8_format=recipe.Format.HYBRID,
            amax_history_len=1,
            amax_compute_algo="most_recent",
            fp8_dpa=is_fp8,
            fp8_mha=False,
        )
        fp8_meta = {}
        fp8_meta["recipe"] = fp8_recipe
    
        config = model_configs_infer[model]
        num_layers = 2 if module == "TransformerLayer" else 1
        # flash-attn v2 requires page_size >= 256
        if backend == "FlashAttention" and not fa_utils.v3_is_installed:
            config_max_seqlen_q = config.max_seqlen_q
            config_max_seqlen_kv = config.max_seqlen_kv
            config.max_seqlen_q = 256
            config.max_seqlen_kv = 256
    
        # create a real-life simulation
        max_batch_size = config.batch_size
        page_size = None
        total_num_pages = None
        if is_paged:
            page_size = 256 if backend == "FlashAttention" and not fa_utils.v3_is_installed else 1
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, page_size)
            total_num_pages = int(max_batch_size * config.max_seqlen_kv / page_size)
        else:
            config.max_seqlen_kv = round_up(config.max_seqlen_kv, 64)
        sim = Simulation(
            total_requests=config.total_requests,
            max_seq_len=config.max_seqlen_kv,
            max_ctx_len=config.max_ctx_len,
            max_batch_size=max_batch_size,
            poisson_rate=2,
        )
        sim.print_setup(logger)
    
        # initialize inference_params
        inference_params = InferenceParams(
            max_batch_size=max_batch_size,
            max_sequence_length=config.max_seqlen_kv,
            num_heads_kv=config.num_gqa_groups,
            head_dim_k=config.head_dim_qk,
            head_dim_v=config.head_dim_v,
            dtype=dtype,
            is_paged=is_paged,
            page_size=page_size,
            total_num_pages=total_num_pages,
            max_ctx_len=config.max_ctx_len,
            qkv_format=qkv_format,
        )
        if module == "DotProductAttention":
            for layer_number in range(1, num_layers + 1):
                inference_params.allocate_memory(layer_number)
    
        # figure out supported backends
        inference_params_qkv_format = "bshd"
        qkv_layout = qkv_format + "_" + "_".join([inference_params_qkv_format] * 2)
        if is_paged:
            qkv_layout = "paged_kv_" + qkv_layout
        available_backends, _, fused_attn_backends = get_available_attention_backends(
            config,
            qkv_dtype=dtype,
            qkv_layout=qkv_layout,
            pad_between_seqs=False,
            is_training=False,
            fp8=is_fp8,
            fp8_meta=fp8_meta,
            inference_params=inference_params,
        )
        flash_attn_supported, fused_attn_supported, unfused_attn_supported = available_backends
        if backend == "FlashAttention" and not flash_attn_supported:
            pytest.skip("FlashAttention backend is not supported")
        if backend == "FusedAttention" and not fused_attn_supported:
            pytest.skip("FusedAttention backend is not supported")
        if backend == "UnfusedAttention" and not unfused_attn_supported:
            pytest.skip("UnfusedAttention backend is not supported")
        os.environ["NVTE_FLASH_ATTN"] = str(int(backend == "FlashAttention"))
        os.environ["NVTE_FUSED_ATTN"] = str(int(backend == "FusedAttention"))
        os.environ["NVTE_UNFUSED_ATTN"] = str(int(backend == "UnfusedAttention"))
        if backend == "UnfusedAttention" and is_cuda_graph:
            pytest.skip("CUDA graph is not supported for UnfusedAttention backend")
        # TransformerLayer FP8 TN Gemm currently requires %8=0
        if is_fp8 and not (qkv_format == "thd" and module == "DotProductAttention"):
            pytest.skip("BSHD/SBHD <-> THD conversions for FP8 are not supported")
    
        # create full model
        logger.info("=== Generating all tokens at once ===")
        model = get_model(module, config, dtype, backend, qkv_format, num_layers, mode="reference")
    
        # generate data for all requests
        full_inputs = generate_args(module, config, dtype, qkv_format="bshd", mode="full_inputs")
    
        # generate reference results
        if module == "DotProductAttention":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    *full_output if isinstance(full_output, List) else full_output,
                )
        if module == "TransformerLayer":
            full_output = full_inputs
            for m in model:
                full_output = m(
                    full_output[0] if isinstance(full_output, List) else full_output,
                )
    
        # create inference model
        logger.info("=== Generating one token at a time ===")
        model = get_model(
            module,
            config,
            dtype,
            backend,
            qkv_format,
            num_layers,
            mode="inference",
            is_fp8=is_fp8,
        )
    
        # graph the model if necessary
        if is_cuda_graph:
            t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")
            step_lens = config.max_ctx_len * torch.ones(max_batch_size, dtype=torch.int32, device="cpu")
            step_dict = OrderedDict(zip(t_seq_ids.tolist(), step_lens.tolist()))
            inference_params.pre_step(step_dict)
    
            sample_args = generate_args(
                module, config, dtype, qkv_format=qkv_format, mode="sample_args"
            )
            sample_kwargs = {}
            sample_kwargs["cu_seqlens_q"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["cu_seqlens_kv"] = torch.linspace(
                0,
                config.batch_size * config.max_ctx_len,
                steps=config.batch_size + 1,
                device="cuda",
                dtype=torch.int32,
            )
            sample_kwargs["inference_params"] = inference_params
            sample_kwargs["max_seqlen_q"] = config.max_ctx_len
            sample_kwargs["max_seqlen_kv"] = config.max_seqlen_kv
    
            model = [
>               make_graphed_callables(
                    model[i],
                    sample_args,
                    num_warmup_iters=10,
                    enabled=is_fp8,
                    sample_kwargs=sample_kwargs,
                    recipe=fp8_recipe,
                )
                for i in range(num_layers)
            ]

tests/pytorch/attention/test_kv_cache.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transformer_engine/pytorch/graph.py:1142: in make_graphed_callables
    graphed_callables = _make_graphed_callables(
transformer_engine/pytorch/graph.py:410: in _make_graphed_callables
    outputs, _ = _tree_flatten(func(*args, **kwargs))
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
transformer_engine/pytorch/graph.py:1116: in call_func
    outputs = old_call_funcs[block_cls](self, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py:196: in nonrecursive_disable_wrapper
    return fn(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/dot_product_attention.py:1419: in forward
    return self.flash_attention(
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1881: in _call_impl
    return inner()
/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1829: in inner
    result = forward_call(*args, **kwargs)
transformer_engine/pytorch/attention/dot_product_attention/backends.py:948: in forward
    output = func(
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:1448: in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py:581: in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:930: in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:1255: in __call__
    return self._op(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:111: in autograd_impl
    result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))
/usr/local/lib/python3.12/dist-packages/torch/_library/autograd.py:40: in forward_no_grad
    result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_ops.py:848: in redispatch
    return self._handle.redispatch_boxed(keyset, *args, **kwargs)  # type: ignore[return-value]
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:343: in backend_impl
    result = self._backend_fns[device_type](*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_compile.py:53: in inner
    return disable_fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: in _fn
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/dist-packages/torch/_library/custom_ops.py:376: in wrapped_fn
    return fn(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

q = tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., ...1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0',
       dtype=torch.bfloat16)
k = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
v = tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1., 1.],
          [1., 1., 1.,  ..., 1., 1..., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',
       dtype=torch.bfloat16)
cu_seqlens_q = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
cu_seqlens_k = tensor([ 0, 16, 32], device='cuda:0', dtype=torch.int32)
max_seqlen_q = 16, max_seqlen_k = 128, dropout_p = 0.0, softmax_scale = 0.0625
causal = True, window_size_left = -1, window_size_right = 0, softcap = 0.0
alibi_slopes = None, return_softmax = False
block_table = tensor([[0],
        [1]], device='cuda:0', dtype=torch.int32)
leftpad_k = None, seqused_k = None, zero_tensors = False

    @_torch_custom_op_wrapper("flash_attn::_flash_attn_varlen_forward", mutates_args=(), device_types="cuda")
    def _flash_attn_varlen_forward(
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
        dropout_p: float,
        softmax_scale: float,
        causal: bool,
        window_size_left: int = -1,
        window_size_right: int = -1,
        softcap: float = 0.0,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax: bool = False,
        block_table: Optional[torch.Tensor] = None,
        leftpad_k: Optional[torch.Tensor] = None,
        seqused_k: Optional[torch.Tensor] = None,
        zero_tensors: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = [maybe_contiguous(x) for x in (q, k, v)]
>       out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(
            q,
            k,
            v,
            None,
            cu_seqlens_q,
            cu_seqlens_k,
            seqused_k,
            leftpad_k,
            block_table,
            alibi_slopes,
            max_seqlen_q,
            max_seqlen_k,
            dropout_p,
            softmax_scale,
            zero_tensors,
            causal,
            window_size_left,
            window_size_right,
            softcap,
            return_softmax,
            None,
        )
E       RuntimeError: Paged KV cache block size must be divisible by 256

/usr/local/lib/python3.12/dist-packages/flash_attn/flash_attn_interface.py:170: RuntimeError
=============================== warnings summary ===============================
tests/pytorch/attention/test_kv_cache.py: 576 warnings
  /root/TransformerEngine/tests/pytorch/attention/test_kv_cache.py:86: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).
    self.seq_ids = torch.range(0, total_requests - 1, dtype=torch.int32, device="cpu")

tests/pytorch/attention/test_kv_cache.py: 288 warnings
  /root/TransformerEngine/transformer_engine/pytorch/attention/inference.py:435: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).
    self.batch_indices_post_step = torch.range(

tests/pytorch/attention/test_kv_cache.py: 72 warnings
  /root/TransformerEngine/tests/pytorch/attention/test_kv_cache.py:532: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).
    t_seq_ids = torch.range(0, max_batch_size, dtype=torch.int32, device="cpu")

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_kv_cache.xml -
=========================== short test summary info ============================
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-TransformerLayer-FlashAttention-False-bshd-infer_0-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-TransformerLayer-FlashAttention-False-bshd-infer_0-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-TransformerLayer-FlashAttention-False-bshd-infer_1-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-TransformerLayer-FlashAttention-False-bshd-infer_1-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-TransformerLayer-FlashAttention-False-sbhd-infer_0-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-TransformerLayer-FlashAttention-False-sbhd-infer_0-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-TransformerLayer-FlashAttention-False-sbhd-infer_1-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-TransformerLayer-FlashAttention-False-sbhd-infer_1-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-TransformerLayer-FlashAttention-False-thd-infer_0-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-TransformerLayer-FlashAttention-False-thd-infer_0-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-TransformerLayer-FlashAttention-False-thd-infer_1-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-TransformerLayer-FlashAttention-False-thd-infer_1-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-DotProductAttention-FlashAttention-False-bshd-infer_0-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-DotProductAttention-FlashAttention-False-bshd-infer_0-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-DotProductAttention-FlashAttention-False-bshd-infer_1-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-DotProductAttention-FlashAttention-False-bshd-infer_1-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-DotProductAttention-FlashAttention-False-sbhd-infer_0-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-DotProductAttention-FlashAttention-False-sbhd-infer_0-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-DotProductAttention-FlashAttention-False-sbhd-infer_1-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-DotProductAttention-FlashAttention-False-sbhd-infer_1-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-DotProductAttention-FlashAttention-False-thd-infer_0-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-DotProductAttention-FlashAttention-False-thd-infer_0-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-DotProductAttention-FlashAttention-False-thd-infer_1-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-False-DotProductAttention-FlashAttention-False-thd-infer_1-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-TransformerLayer-FlashAttention-False-bshd-infer_0-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-TransformerLayer-FlashAttention-False-bshd-infer_0-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-TransformerLayer-FlashAttention-False-bshd-infer_1-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-TransformerLayer-FlashAttention-False-bshd-infer_1-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-TransformerLayer-FlashAttention-False-sbhd-infer_0-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-TransformerLayer-FlashAttention-False-sbhd-infer_0-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-TransformerLayer-FlashAttention-False-sbhd-infer_1-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-TransformerLayer-FlashAttention-False-sbhd-infer_1-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-TransformerLayer-FlashAttention-False-thd-infer_0-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-TransformerLayer-FlashAttention-False-thd-infer_0-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-TransformerLayer-FlashAttention-False-thd-infer_1-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-TransformerLayer-FlashAttention-False-thd-infer_1-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-DotProductAttention-FlashAttention-False-bshd-infer_0-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-DotProductAttention-FlashAttention-False-bshd-infer_0-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-DotProductAttention-FlashAttention-False-bshd-infer_1-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-DotProductAttention-FlashAttention-False-bshd-infer_1-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-DotProductAttention-FlashAttention-False-sbhd-infer_0-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-DotProductAttention-FlashAttention-False-sbhd-infer_0-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-DotProductAttention-FlashAttention-False-sbhd-infer_1-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-DotProductAttention-FlashAttention-False-sbhd-infer_1-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-DotProductAttention-FlashAttention-False-thd-infer_0-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-DotProductAttention-FlashAttention-False-thd-infer_0-dtype1]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-DotProductAttention-FlashAttention-False-thd-infer_1-dtype0]
FAILED tests/pytorch/attention/test_kv_cache.py::test_kv_cache[False-True-DotProductAttention-FlashAttention-False-thd-infer_1-dtype1]
========== 48 failed, 144 passed, 384 skipped, 936 warnings in 50.67s ==========
+ test_fail test_kv_cache.py
+ RET=1
+ FAILED_CASES=' test_numerics.py test_fused_optimizer.py test_attention.py test_kv_cache.py'
+ echo 'Error: sub-test failed: test_kv_cache.py'
Error: sub-test failed: test_kv_cache.py
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_hf_integration.xml /root/TransformerEngine/tests/pytorch/test_hf_integration.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 2 items

tests/pytorch/test_hf_integration.py .x                                  [100%]

=============================== warnings summary ===============================
<frozen importlib._bootstrap>:488
  <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute

<frozen importlib._bootstrap>:488
  <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_hf_integration.xml -
=================== 1 passed, 1 xfailed, 2 warnings in 7.96s ===================
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
+ NVTE_TEST_CHECKPOINT_ARTIFACT_PATH=/root/TransformerEngine/artifacts/tests/pytorch/test_checkpoint
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_checkpoint.xml /root/TransformerEngine/tests/pytorch/test_checkpoint.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 11 items

tests/pytorch/test_checkpoint.py FFFFFFFFFFF                             [100%]

=================================== FAILURES ===================================
____________________ TestLoadCheckpoint.test_module[linear] ____________________

self = <test_checkpoint.TestLoadCheckpoint object at 0xf8f6cb62030>
name = 'linear'

    @pytest.mark.parametrize("name", _TestLoadCheckpoint_name_list)
    def test_module(self, name: str) -> None:
        """Test for loading a module's checkpoint file"""
    
        # Skip if quantization is not supported
        quantization = None
        if "." in name:
            quantization = name.split(".")[1]
        if quantization == "fp8" and not fp8_available:
            pytest.skip(reason_for_no_fp8)
        if quantization == "mxfp8" and not mxfp8_available:
            pytest.skip(reason_for_no_mxfp8)
    
        # Construct module
        module = self._make_module(name)
    
        # Load checkpoint from file
        checkpoint_file = self._checkpoint_dir() / f"{name}.pt"
        if not checkpoint_file.is_file():
>           raise FileNotFoundError(f"Could not find checkpoint file at {checkpoint_file}")
E           FileNotFoundError: Could not find checkpoint file at /root/TransformerEngine/artifacts/tests/pytorch/test_checkpoint/linear.pt

tests/pytorch/test_checkpoint.py:131: FileNotFoundError
_______________ TestLoadCheckpoint.test_module[layernorm_linear] _______________

self = <test_checkpoint.TestLoadCheckpoint object at 0xf8f768c0cb0>
name = 'layernorm_linear'

    @pytest.mark.parametrize("name", _TestLoadCheckpoint_name_list)
    def test_module(self, name: str) -> None:
        """Test for loading a module's checkpoint file"""
    
        # Skip if quantization is not supported
        quantization = None
        if "." in name:
            quantization = name.split(".")[1]
        if quantization == "fp8" and not fp8_available:
            pytest.skip(reason_for_no_fp8)
        if quantization == "mxfp8" and not mxfp8_available:
            pytest.skip(reason_for_no_mxfp8)
    
        # Construct module
        module = self._make_module(name)
    
        # Load checkpoint from file
        checkpoint_file = self._checkpoint_dir() / f"{name}.pt"
        if not checkpoint_file.is_file():
>           raise FileNotFoundError(f"Could not find checkpoint file at {checkpoint_file}")
E           FileNotFoundError: Could not find checkpoint file at /root/TransformerEngine/artifacts/tests/pytorch/test_checkpoint/layernorm_linear.pt

tests/pytorch/test_checkpoint.py:131: FileNotFoundError
________________ TestLoadCheckpoint.test_module[layernorm_mlp] _________________

self = <test_checkpoint.TestLoadCheckpoint object at 0xf8f6ca6d8e0>
name = 'layernorm_mlp'

    @pytest.mark.parametrize("name", _TestLoadCheckpoint_name_list)
    def test_module(self, name: str) -> None:
        """Test for loading a module's checkpoint file"""
    
        # Skip if quantization is not supported
        quantization = None
        if "." in name:
            quantization = name.split(".")[1]
        if quantization == "fp8" and not fp8_available:
            pytest.skip(reason_for_no_fp8)
        if quantization == "mxfp8" and not mxfp8_available:
            pytest.skip(reason_for_no_mxfp8)
    
        # Construct module
        module = self._make_module(name)
    
        # Load checkpoint from file
        checkpoint_file = self._checkpoint_dir() / f"{name}.pt"
        if not checkpoint_file.is_file():
>           raise FileNotFoundError(f"Could not find checkpoint file at {checkpoint_file}")
E           FileNotFoundError: Could not find checkpoint file at /root/TransformerEngine/artifacts/tests/pytorch/test_checkpoint/layernorm_mlp.pt

tests/pytorch/test_checkpoint.py:131: FileNotFoundError
__________________ TestLoadCheckpoint.test_module[layernorm] ___________________

self = <test_checkpoint.TestLoadCheckpoint object at 0xf8f6cb36750>
name = 'layernorm'

    @pytest.mark.parametrize("name", _TestLoadCheckpoint_name_list)
    def test_module(self, name: str) -> None:
        """Test for loading a module's checkpoint file"""
    
        # Skip if quantization is not supported
        quantization = None
        if "." in name:
            quantization = name.split(".")[1]
        if quantization == "fp8" and not fp8_available:
            pytest.skip(reason_for_no_fp8)
        if quantization == "mxfp8" and not mxfp8_available:
            pytest.skip(reason_for_no_mxfp8)
    
        # Construct module
        module = self._make_module(name)
    
        # Load checkpoint from file
        checkpoint_file = self._checkpoint_dir() / f"{name}.pt"
        if not checkpoint_file.is_file():
>           raise FileNotFoundError(f"Could not find checkpoint file at {checkpoint_file}")
E           FileNotFoundError: Could not find checkpoint file at /root/TransformerEngine/artifacts/tests/pytorch/test_checkpoint/layernorm.pt

tests/pytorch/test_checkpoint.py:131: FileNotFoundError
___________________ TestLoadCheckpoint.test_module[rmsnorm] ____________________

self = <test_checkpoint.TestLoadCheckpoint object at 0xf8f6cb37410>
name = 'rmsnorm'

    @pytest.mark.parametrize("name", _TestLoadCheckpoint_name_list)
    def test_module(self, name: str) -> None:
        """Test for loading a module's checkpoint file"""
    
        # Skip if quantization is not supported
        quantization = None
        if "." in name:
            quantization = name.split(".")[1]
        if quantization == "fp8" and not fp8_available:
            pytest.skip(reason_for_no_fp8)
        if quantization == "mxfp8" and not mxfp8_available:
            pytest.skip(reason_for_no_mxfp8)
    
        # Construct module
        module = self._make_module(name)
    
        # Load checkpoint from file
        checkpoint_file = self._checkpoint_dir() / f"{name}.pt"
        if not checkpoint_file.is_file():
>           raise FileNotFoundError(f"Could not find checkpoint file at {checkpoint_file}")
E           FileNotFoundError: Could not find checkpoint file at /root/TransformerEngine/artifacts/tests/pytorch/test_checkpoint/rmsnorm.pt

tests/pytorch/test_checkpoint.py:131: FileNotFoundError
______________ TestLoadCheckpoint.test_module[transformer_layer] _______________

self = <test_checkpoint.TestLoadCheckpoint object at 0xf8f6cb374a0>
name = 'transformer_layer'

    @pytest.mark.parametrize("name", _TestLoadCheckpoint_name_list)
    def test_module(self, name: str) -> None:
        """Test for loading a module's checkpoint file"""
    
        # Skip if quantization is not supported
        quantization = None
        if "." in name:
            quantization = name.split(".")[1]
        if quantization == "fp8" and not fp8_available:
            pytest.skip(reason_for_no_fp8)
        if quantization == "mxfp8" and not mxfp8_available:
            pytest.skip(reason_for_no_mxfp8)
    
        # Construct module
        module = self._make_module(name)
    
        # Load checkpoint from file
        checkpoint_file = self._checkpoint_dir() / f"{name}.pt"
        if not checkpoint_file.is_file():
>           raise FileNotFoundError(f"Could not find checkpoint file at {checkpoint_file}")
E           FileNotFoundError: Could not find checkpoint file at /root/TransformerEngine/artifacts/tests/pytorch/test_checkpoint/transformer_layer.pt

tests/pytorch/test_checkpoint.py:131: FileNotFoundError
__________________ TestLoadCheckpoint.test_module[ops_linear] __________________

self = <test_checkpoint.TestLoadCheckpoint object at 0xf8f6cb37530>
name = 'ops_linear'

    @pytest.mark.parametrize("name", _TestLoadCheckpoint_name_list)
    def test_module(self, name: str) -> None:
        """Test for loading a module's checkpoint file"""
    
        # Skip if quantization is not supported
        quantization = None
        if "." in name:
            quantization = name.split(".")[1]
        if quantization == "fp8" and not fp8_available:
            pytest.skip(reason_for_no_fp8)
        if quantization == "mxfp8" and not mxfp8_available:
            pytest.skip(reason_for_no_mxfp8)
    
        # Construct module
        module = self._make_module(name)
    
        # Load checkpoint from file
        checkpoint_file = self._checkpoint_dir() / f"{name}.pt"
        if not checkpoint_file.is_file():
>           raise FileNotFoundError(f"Could not find checkpoint file at {checkpoint_file}")
E           FileNotFoundError: Could not find checkpoint file at /root/TransformerEngine/artifacts/tests/pytorch/test_checkpoint/ops_linear.pt

tests/pytorch/test_checkpoint.py:131: FileNotFoundError
__________________ TestLoadCheckpoint.test_module[linear.fp8] __________________

self = <test_checkpoint.TestLoadCheckpoint object at 0xf8f6cb375c0>
name = 'linear.fp8'

    @pytest.mark.parametrize("name", _TestLoadCheckpoint_name_list)
    def test_module(self, name: str) -> None:
        """Test for loading a module's checkpoint file"""
    
        # Skip if quantization is not supported
        quantization = None
        if "." in name:
            quantization = name.split(".")[1]
        if quantization == "fp8" and not fp8_available:
            pytest.skip(reason_for_no_fp8)
        if quantization == "mxfp8" and not mxfp8_available:
            pytest.skip(reason_for_no_mxfp8)
    
        # Construct module
        module = self._make_module(name)
    
        # Load checkpoint from file
        checkpoint_file = self._checkpoint_dir() / f"{name}.pt"
        if not checkpoint_file.is_file():
>           raise FileNotFoundError(f"Could not find checkpoint file at {checkpoint_file}")
E           FileNotFoundError: Could not find checkpoint file at /root/TransformerEngine/artifacts/tests/pytorch/test_checkpoint/linear.fp8.pt

tests/pytorch/test_checkpoint.py:131: FileNotFoundError
________________ TestLoadCheckpoint.test_module[ops_linear.fp8] ________________

self = <test_checkpoint.TestLoadCheckpoint object at 0xf8f6cb37650>
name = 'ops_linear.fp8'

    @pytest.mark.parametrize("name", _TestLoadCheckpoint_name_list)
    def test_module(self, name: str) -> None:
        """Test for loading a module's checkpoint file"""
    
        # Skip if quantization is not supported
        quantization = None
        if "." in name:
            quantization = name.split(".")[1]
        if quantization == "fp8" and not fp8_available:
            pytest.skip(reason_for_no_fp8)
        if quantization == "mxfp8" and not mxfp8_available:
            pytest.skip(reason_for_no_mxfp8)
    
        # Construct module
        module = self._make_module(name)
    
        # Load checkpoint from file
        checkpoint_file = self._checkpoint_dir() / f"{name}.pt"
        if not checkpoint_file.is_file():
>           raise FileNotFoundError(f"Could not find checkpoint file at {checkpoint_file}")
E           FileNotFoundError: Could not find checkpoint file at /root/TransformerEngine/artifacts/tests/pytorch/test_checkpoint/ops_linear.fp8.pt

tests/pytorch/test_checkpoint.py:131: FileNotFoundError
_________________ TestLoadCheckpoint.test_module[linear.mxfp8] _________________

self = <test_checkpoint.TestLoadCheckpoint object at 0xf8f6cb376e0>
name = 'linear.mxfp8'

    @pytest.mark.parametrize("name", _TestLoadCheckpoint_name_list)
    def test_module(self, name: str) -> None:
        """Test for loading a module's checkpoint file"""
    
        # Skip if quantization is not supported
        quantization = None
        if "." in name:
            quantization = name.split(".")[1]
        if quantization == "fp8" and not fp8_available:
            pytest.skip(reason_for_no_fp8)
        if quantization == "mxfp8" and not mxfp8_available:
            pytest.skip(reason_for_no_mxfp8)
    
        # Construct module
        module = self._make_module(name)
    
        # Load checkpoint from file
        checkpoint_file = self._checkpoint_dir() / f"{name}.pt"
        if not checkpoint_file.is_file():
>           raise FileNotFoundError(f"Could not find checkpoint file at {checkpoint_file}")
E           FileNotFoundError: Could not find checkpoint file at /root/TransformerEngine/artifacts/tests/pytorch/test_checkpoint/linear.mxfp8.pt

tests/pytorch/test_checkpoint.py:131: FileNotFoundError
_______________ TestLoadCheckpoint.test_module[ops_linear.mxfp8] _______________

self = <test_checkpoint.TestLoadCheckpoint object at 0xf8f6cb37770>
name = 'ops_linear.mxfp8'

    @pytest.mark.parametrize("name", _TestLoadCheckpoint_name_list)
    def test_module(self, name: str) -> None:
        """Test for loading a module's checkpoint file"""
    
        # Skip if quantization is not supported
        quantization = None
        if "." in name:
            quantization = name.split(".")[1]
        if quantization == "fp8" and not fp8_available:
            pytest.skip(reason_for_no_fp8)
        if quantization == "mxfp8" and not mxfp8_available:
            pytest.skip(reason_for_no_mxfp8)
    
        # Construct module
        module = self._make_module(name)
    
        # Load checkpoint from file
        checkpoint_file = self._checkpoint_dir() / f"{name}.pt"
        if not checkpoint_file.is_file():
>           raise FileNotFoundError(f"Could not find checkpoint file at {checkpoint_file}")
E           FileNotFoundError: Could not find checkpoint file at /root/TransformerEngine/artifacts/tests/pytorch/test_checkpoint/ops_linear.mxfp8.pt

tests/pytorch/test_checkpoint.py:131: FileNotFoundError
- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_checkpoint.xml -
=========================== short test summary info ============================
FAILED tests/pytorch/test_checkpoint.py::TestLoadCheckpoint::test_module[linear]
FAILED tests/pytorch/test_checkpoint.py::TestLoadCheckpoint::test_module[layernorm_linear]
FAILED tests/pytorch/test_checkpoint.py::TestLoadCheckpoint::test_module[layernorm_mlp]
FAILED tests/pytorch/test_checkpoint.py::TestLoadCheckpoint::test_module[layernorm]
FAILED tests/pytorch/test_checkpoint.py::TestLoadCheckpoint::test_module[rmsnorm]
FAILED tests/pytorch/test_checkpoint.py::TestLoadCheckpoint::test_module[transformer_layer]
FAILED tests/pytorch/test_checkpoint.py::TestLoadCheckpoint::test_module[ops_linear]
FAILED tests/pytorch/test_checkpoint.py::TestLoadCheckpoint::test_module[linear.fp8]
FAILED tests/pytorch/test_checkpoint.py::TestLoadCheckpoint::test_module[ops_linear.fp8]
FAILED tests/pytorch/test_checkpoint.py::TestLoadCheckpoint::test_module[linear.mxfp8]
FAILED tests/pytorch/test_checkpoint.py::TestLoadCheckpoint::test_module[ops_linear.mxfp8]
============================== 11 failed in 6.54s ==============================
+ test_fail test_checkpoint.py
+ RET=1
+ FAILED_CASES=' test_numerics.py test_fused_optimizer.py test_attention.py test_kv_cache.py test_checkpoint.py'
+ echo 'Error: sub-test failed: test_checkpoint.py'
Error: sub-test failed: test_checkpoint.py
+ python3 -m pytest --tb=auto --junitxml=/data/home/ziangli/te_pr_logs_v210//pytest_test_fused_router.xml /root/TransformerEngine/tests/pytorch/test_fused_router.py
/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py:247: PytestDeprecationWarning: The configuration option "asyncio_default_fixture_loop_scope" is unset.
The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: "function", "class", "module", "package", "session"

  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.2.1, pluggy-1.6.0
rootdir: /root/TransformerEngine
configfile: pyproject.toml
plugins: anyio-4.12.1, cov-7.0.0, asyncio-1.3.0, logfire-4.20.0, devtools-0.12.2, hydra-core-1.3.2, typeguard-4.4.4
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 237 items

tests/pytorch/test_fused_router.py ..................................... [ 15%]
........................................................................ [ 45%]
........................................................................ [ 76%]
........................................................                 [100%]

- generated xml file: /data/home/ziangli/te_pr_logs_v210/pytest_test_fused_router.xml -
============================= 237 passed in 7.05s ==============================
+ '[' 1 -ne 0 ']'
+ echo 'Error in the following test cases: test_numerics.py test_fused_optimizer.py test_attention.py test_kv_cache.py test_checkpoint.py'
Error in the following test cases: test_numerics.py test_fused_optimizer.py test_attention.py test_kv_cache.py test_checkpoint.py
+ exit 1
